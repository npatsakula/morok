<!DOCTYPE HTML>
<html lang="en" class="rust sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Morok</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon-de23e50b.svg">
        <link rel="shortcut icon" href="favicon-8114d1fc.png">
        <link rel="stylesheet" href="css/variables-8adf115d.css">
        <link rel="stylesheet" href="css/general-2459343d.css">
        <link rel="stylesheet" href="css/chrome-ae938929.css">
        <link rel="stylesheet" href="css/print-9e4910d8.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="fonts/fonts-9644e21d.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="mdbook-highlight-css" href="highlight-493f70e1.css">
        <link rel="stylesheet" id="mdbook-tomorrow-night-css" href="tomorrow-night-4c0ae647.css">
        <link rel="stylesheet" id="mdbook-ayu-highlight-css" href="ayu-highlight-3fdfc3ac.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "";
            const default_light_theme = "rust";
            const default_dark_theme = "ayu";
            window.path_to_searchindex_js = "searchindex-0624f9d3.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc-6f19a375.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>â†</kbd> or <kbd>â†’</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="mdbook-body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('rust')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="mdbook-sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("mdbook-sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="mdbook-sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="mdbook-sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="mdbook-page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="mdbook-menu-bar-hover-placeholder"></div>
                <div id="mdbook-menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="mdbook-sidebar-toggle" class="icon-button" for="mdbook-sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="mdbook-sidebar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"/></svg></span>
                        </label>
                        <button id="mdbook-theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="mdbook-theme-list">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M371.3 367.1c27.3-3.9 51.9-19.4 67.2-42.9L600.2 74.1c12.6-19.5 9.4-45.3-7.6-61.2S549.7-4.4 531.1 9.6L294.4 187.2c-24 18-38.2 46.1-38.4 76.1L371.3 367.1zm-19.6 25.4l-116-104.4C175.9 290.3 128 339.6 128 400c0 3.9 .2 7.8 .6 11.6c1.8 17.5-10.2 36.4-27.8 36.4H96c-17.7 0-32 14.3-32 32s14.3 32 32 32H240c61.9 0 112-50.1 112-112c0-2.5-.1-5-.2-7.5z"/></svg></span>
                        </button>
                        <ul id="mdbook-theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-ayu">Ayu</button></li>
                        </ul>
                        <button id="mdbook-search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="mdbook-searchbar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M416 208c0 45.9-14.9 88.3-40 122.7L502.6 457.4c12.5 12.5 12.5 32.8 0 45.3s-32.8 12.5-45.3 0L330.7 376c-34.4 25.2-76.8 40-122.7 40C93.1 416 0 322.9 0 208S93.1 0 208 0S416 93.1 416 208zM208 352c79.5 0 144-64.5 144-144s-64.5-144-144-144S64 128.5 64 208s64.5 144 144 144z"/></svg></span>
                        </button>
                    </div>

                    <h1 class="menu-title">Morok</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <span class=fa-svg id="print-button"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M128 0C92.7 0 64 28.7 64 64v96h64V64H354.7L384 93.3V160h64V93.3c0-17-6.7-33.3-18.7-45.3L400 18.7C388 6.7 371.7 0 354.7 0H128zM384 352v32 64H128V384 368 352H384zm64 32h32c17.7 0 32-14.3 32-32V256c0-35.3-28.7-64-64-64H64c-35.3 0-64 28.7-64 64v96c0 17.7 14.3 32 32 32H64v64c0 35.3 28.7 64 64 64H384c35.3 0 64-28.7 64-64V384zm-16-88c-13.3 0-24-10.7-24-24s10.7-24 24-24s24 10.7 24 24s-10.7 24-24 24z"/></svg></span>
                        </a>
                        <a href="https://github.com/patsak/morok" title="Git repository" aria-label="Git repository">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span>
                        </a>

                    </div>
                </div>

                <div id="mdbook-search-wrapper" class="hidden">
                    <form id="mdbook-searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="mdbook-searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="mdbook-searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <span class=fa-svg id="fa-spin"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M304 48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zm0 416c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM48 304c26.5 0 48-21.5 48-48s-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48zm464-48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM142.9 437c18.7-18.7 18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zm0-294.2c18.7-18.7 18.7-49.1 0-67.9S93.7 56.2 75 75s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zM369.1 437c18.7 18.7 49.1 18.7 67.9 0s18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9z"/></svg></span>
                            </div>
                        </div>
                    </form>
                    <div id="mdbook-searchresults-outer" class="searchresults-outer hidden">
                        <div id="mdbook-searchresults-header" class="searchresults-header"></div>
                        <ul id="mdbook-searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('mdbook-sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('mdbook-sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#mdbook-sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="mdbook-content" class="content">
                    <main>
                        <h1 id="morok"><a class="header" href="#morok">Morok</a></h1>
<blockquote>
<p>âš ï¸ <strong>Pre-alpha software.</strong> APIs are unstable and may change without notice. Not recommended for production use. ğŸš§ğŸ’€</p>
</blockquote>
<p>Morok is a Rust-based ML compiler inspired by <a href="https://github.com/tinygrad/tinygrad">Tinygrad</a>. It features lazy tensor evaluation with UOp-based IR, pattern-driven optimization, and multi-backend code generation.</p>
<h2 id="highlights"><a class="header" href="#highlights">Highlights</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Feature</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><strong>Declarative Optimization</strong></td><td><code>patterns!</code> DSL for graph rewrites with Z3-verified correctness</td></tr>
<tr><td><strong>Lazy Evaluation</strong></td><td>Tensors build computation graphs, compiled only at <code>realize()</code></td></tr>
<tr><td><strong>CUDA Support</strong></td><td>Unified memory, D2D copy, LRU buffer caching</td></tr>
<tr><td><strong>Provenance Tracking</strong></td><td><code>#[track_caller]</code> traces every UOp to source location</td></tr>
<tr><td><strong>80+ IR Operations</strong></td><td>Arithmetic, memory, control flow, WMMA tensor cores</td></tr>
<tr><td><strong>20+ Optimizations</strong></td><td>Constant folding, tensor cores, vectorization, loop unrolling</td></tr>
</tbody>
</table>
</div>
<h2 id="quick-example"><a class="header" href="#quick-example">Quick Example</a></h2>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use morok_tensor::Tensor;

// Build lazy computation graph
let a = Tensor::from_slice(&amp;[1.0, 2.0, 3.0], &amp;[3])?;
let b = Tensor::from_slice(&amp;[4.0, 5.0, 6.0], &amp;[3])?;
let c = (a + b).sum();

// Compile and execute
let result = c.realize()?;
<span class="boring">}</span></code></pre>
<h2 id="license"><a class="header" href="#license">License</a></h2>
<p>MIT</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="hands-on-from-tensors-to-models"><a class="header" href="#hands-on-from-tensors-to-models">Hands-On: From Tensors to Models</a></h1>
<p>This chapter teaches Morok through progressive examples. Youâ€™ll start with basic tensor operations and build up to a working neural network classifier.</p>
<p><strong>What youâ€™ll learn:</strong></p>
<ul>
<li>Creating and manipulating tensors</li>
<li>Shape operations (reshape, transpose, broadcast)</li>
<li>Matrix multiplication</li>
<li>Building reusable layers</li>
<li>Composing a complete model</li>
</ul>
<p><strong>Prerequisites:</strong></p>
<ul>
<li>Basic Rust knowledge</li>
<li>Add <code>morok_tensor</code> to your <code>Cargo.toml</code></li>
</ul>
<p><strong>Key pattern:</strong> Morok uses <em>lazy evaluation</em>. Operations build a computation graph without executing. Call <code>realize()</code> to compile and run everything at once.</p>
<hr>
<h2 id="example-1-hello-tensor"><a class="header" href="#example-1-hello-tensor">Example 1: Hello Tensor</a></h2>
<p>Letâ€™s create tensors, perform operations, and get results.</p>
<pre class="playground"><code class="language-rust">use morok_tensor::Tensor;

fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {
    // Create tensors from slices
    let a = Tensor::from_slice(&amp;[1.0f32, 2.0, 3.0, 4.0]);
    let b = Tensor::from_slice(&amp;[10.0f32, 20.0, 30.0, 40.0]);

    // Lazy operations (no execution yet)
    let sum = &amp;a + &amp;b;
    let scaled = &amp;sum * &amp;Tensor::from_slice(&amp;[0.1f32]);

    // Execute and get results
    let result = scaled.realize()?;
    let data = result.to_ndarray::&lt;f32&gt;()?;
    println!("Result: {:?}", data);
    // Output: [1.1, 2.2, 3.3, 4.4]

    Ok(())
}</code></pre>
<p><strong>Whatâ€™s happening:</strong></p>
<ol>
<li>
<p><code>Tensor::from_slice()</code> creates a tensor from a Rust slice. The <code>f32</code> suffix tells Rust the element type.</p>
</li>
<li>
<p><code>&amp;a + &amp;b</code> doesnâ€™t compute anything yet. It returns a new <code>Tensor</code> that <em>represents</em> the addition. The <code>&amp;</code> borrows the tensors so we can reuse them.</p>
</li>
<li>
<p><code>realize()</code> is where the magic happens. Morok:</p>
<ul>
<li>Analyzes the computation graph</li>
<li>Fuses operations where possible</li>
<li>Generates optimized code</li>
<li>Executes on the target device</li>
</ul>
</li>
<li>
<p><code>to_ndarray()</code> extracts the result as an <code>ndarray::ArrayD</code> for inspection.</p>
</li>
</ol>
<p><strong>Try this:</strong> Remove the <code>realize()</code> call. The code still runs, but <code>data</code> would be emptyâ€”nothing was computed.</p>
<hr>
<h2 id="example-2-shape-gymnastics"><a class="header" href="#example-2-shape-gymnastics">Example 2: Shape Gymnastics</a></h2>
<p>Neural networks constantly reshape data. Letâ€™s master the basics.</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn shape_example() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {
    // Create a 1D tensor with 6 elements
    let data = Tensor::from_slice(&amp;[1.0f32, 2.0, 3.0, 4.0, 5.0, 6.0]);
    println!("Original shape: {:?}", data.shape());  // [6]

    // Reshape to a 2x3 matrix
    let matrix = data.try_reshape(&amp;[2, 3])?;
    println!("Matrix shape: {:?}", matrix.shape());  // [2, 3]
    // [[1, 2, 3],
    //  [4, 5, 6]]

    // Transpose to 3x2
    let transposed = matrix.try_transpose(0, 1)?;
    println!("Transposed shape: {:?}", transposed.shape());  // [3, 2]
    // [[1, 4],
    //  [2, 5],
    //  [3, 6]]

    // Broadcasting: add a row vector to every row
    // [3, 2] + [1, 2] â†’ [3, 2]
    let bias = Tensor::from_slice(&amp;[100.0f32, 200.0])
        .try_reshape(&amp;[1, 2])?;
    let biased = &amp;transposed + &amp;bias;

    let result = biased.realize()?;
    println!("{:?}", result.to_ndarray::&lt;f32&gt;()?);
    // [[101, 204],
    //  [102, 205],
    //  [103, 206]]

    Ok(())
}
<span class="boring">}</span></code></pre>
<p><strong>Key operations:</strong></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Operation</th><th>What it does</th></tr>
</thead>
<tbody>
<tr><td><code>try_reshape(&amp;[2, 3])</code></td><td>Change shape (same total elements)</td></tr>
<tr><td><code>try_reshape(&amp;[-1, 3])</code></td><td>Infer dimension from total size</td></tr>
<tr><td><code>try_transpose(0, 1)</code></td><td>Swap dimensions 0 and 1</td></tr>
<tr><td><code>try_squeeze(dim)</code></td><td>Remove dimension of size 1</td></tr>
<tr><td><code>try_unsqueeze(dim)</code></td><td>Add dimension of size 1</td></tr>
</tbody>
</table>
</div>
<p><strong>Broadcasting rules</strong> (same as NumPy/PyTorch):</p>
<ul>
<li>Shapes align from the right</li>
<li>Each dimension must match or be 1</li>
<li>Dimensions of size 1 are â€œstretchedâ€ to match</li>
</ul>
<pre><code class="language-text">[3, 2] + [1, 2] â†’ [3, 2]  âœ“ (1 broadcasts to 3)
[3, 2] + [2]    â†’ [3, 2]  âœ“ (implicit [1, 2])
[3, 2] + [3]    â†’ error   âœ— (2 â‰  3)
</code></pre>
<hr>
<h2 id="example-3-matrix-multiply"><a class="header" href="#example-3-matrix-multiply">Example 3: Matrix Multiply</a></h2>
<p>Matrix multiplication is the workhorse of neural networks. Every layer uses it.</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn matmul_example() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {
    // Input: 4 samples, 3 features each â†’ shape [4, 3]
    let input = Tensor::from_slice(&amp;[
        1.0f32, 2.0, 3.0,   // sample 0
        4.0, 5.0, 6.0,      // sample 1
        7.0, 8.0, 9.0,      // sample 2
        10.0, 11.0, 12.0,   // sample 3
    ]).try_reshape(&amp;[4, 3])?;

    // Weights: 3 inputs â†’ 2 outputs â†’ shape [3, 2]
    let weights = Tensor::from_slice(&amp;[
        0.1f32, 0.2,  // feature 0 â†’ outputs
        0.3, 0.4,     // feature 1 â†’ outputs
        0.5, 0.6,     // feature 2 â†’ outputs
    ]).try_reshape(&amp;[3, 2])?;

    // Matrix multiply: [4, 3] @ [3, 2] â†’ [4, 2]
    let output = input.dot(&amp;weights)?;

    let result = output.realize()?;
    println!("Output shape: {:?}", result.shape());  // [4, 2]
    println!("{:?}", result.to_ndarray::&lt;f32&gt;()?);
    // Each row: weighted sum of that sample's features

    Ok(())
}
<span class="boring">}</span></code></pre>
<p><strong>Shape rules for <code>dot()</code>:</strong></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Left</th><th>Right</th><th>Result</th></tr>
</thead>
<tbody>
<tr><td><code>[M, K]</code></td><td><code>[K, N]</code></td><td><code>[M, N]</code></td></tr>
<tr><td><code>[K]</code></td><td><code>[K, N]</code></td><td><code>[N]</code> (vector-matrix)</td></tr>
<tr><td><code>[M, K]</code></td><td><code>[K]</code></td><td><code>[M]</code> (matrix-vector)</td></tr>
<tr><td><code>[B, M, K]</code></td><td><code>[B, K, N]</code></td><td><code>[B, M, N]</code> (batched)</td></tr>
</tbody>
</table>
</div>
<p>The inner dimensions must match (the <code>K</code>). Think of it as: â€œfor each row of left, dot product with each column of right.â€</p>
<hr>
<h2 id="example-4-building-a-linear-layer"><a class="header" href="#example-4-building-a-linear-layer">Example 4: Building a Linear Layer</a></h2>
<p>A linear layer computes <code>y = x @ W.T + b</code>. Letâ€™s build one from scratch.</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use morok_tensor::{Tensor, Error};

struct Linear {
    weight: Tensor,  // shape: [out_features, in_features]
    bias: Tensor,    // shape: [out_features]
}

impl Linear {
    fn new(in_features: usize, out_features: usize) -&gt; Self {
        // Simple initialization (real code would use proper random init)
        let weight_data: Vec&lt;f32&gt; = (0..in_features * out_features)
            .map(|i| (i as f32 * 0.1).sin() * 0.1)
            .collect();
        let bias_data = vec![0.0f32; out_features];

        Self {
            weight: Tensor::from_slice(&amp;weight_data)
                .try_reshape(&amp;[out_features as isize, in_features as isize])
                .expect("reshape failed"),
            bias: Tensor::from_slice(&amp;bias_data),
        }
    }

    fn forward(&amp;self, x: &amp;Tensor) -&gt; Result&lt;Tensor, Error&gt; {
        // y = x @ W.T + b
        let weight_t = self.weight.try_transpose(0, 1)?;
        let out = x.dot(&amp;weight_t)?;
        Ok(&amp;out + &amp;self.bias)
    }
}

fn linear_example() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {
    // Create a layer: 4 inputs â†’ 2 outputs
    let layer = Linear::new(4, 2);

    // Single sample with 4 features
    let input = Tensor::from_slice(&amp;[1.0f32, 2.0, 3.0, 4.0]);

    // Forward pass
    let output = layer.forward(&amp;input)?;

    let result = output.realize()?;
    println!("Output: {:?}", result.to_ndarray::&lt;f32&gt;()?);

    Ok(())
}
<span class="boring">}</span></code></pre>
<p><strong>Why transpose the weights?</strong></p>
<p>PyTorch convention stores weights as <code>[out_features, in_features]</code>. For a layer mapping 4 â†’ 2:</p>
<ul>
<li>Weight shape: <code>[2, 4]</code></li>
<li>Input shape: <code>[4]</code> or <code>[batch, 4]</code></li>
<li>We need: <code>input @ weight.T</code> = <code>[batch, 4] @ [4, 2]</code> = <code>[batch, 2]</code></li>
</ul>
<p>This convention makes it easy to read the weight matrix: row <code>i</code> contains all weights feeding into output <code>i</code>.</p>
<hr>
<h2 id="example-5-mnist-classifier"><a class="header" href="#example-5-mnist-classifier">Example 5: MNIST Classifier</a></h2>
<p>Letâ€™s build a complete neural network that could classify handwritten digits.</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// Two-layer neural network for MNIST
/// Architecture: 784 (28Ã—28 pixels) â†’ 128 (hidden) â†’ 10 (digits)
struct MnistNet {
    fc1: Linear,
    fc2: Linear,
}

impl MnistNet {
    fn new() -&gt; Self {
        Self {
            fc1: Linear::new(784, 128),
            fc2: Linear::new(128, 10),
        }
    }

    fn forward(&amp;self, x: &amp;Tensor) -&gt; Result&lt;Tensor, Error&gt; {
        // Layer 1: linear + ReLU activation
        let x = self.fc1.forward(x)?;
        let x = x.relu()?;

        // Layer 2: linear (no activation â€” raw logits)
        self.fc2.forward(&amp;x)
    }

    fn predict(&amp;self, x: &amp;Tensor) -&gt; Result&lt;Tensor, Error&gt; {
        let logits = self.forward(x)?;
        // Convert logits to probabilities
        logits.softmax(-1)
    }
}

fn mnist_example() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {
    let model = MnistNet::new();

    // Simulate a 28Ã—28 grayscale image (flattened to 784)
    let fake_image: Vec&lt;f32&gt; = (0..784)
        .map(|i| (i as f32) / 784.0)
        .collect();
    let input = Tensor::from_slice(&amp;fake_image)
        .try_reshape(&amp;[1, 784])?;  // batch size 1

    // Forward pass
    let logits = model.forward(&amp;input)?;
    let probs = logits.softmax(-1)?;

    // Get results
    let probs_result = probs.realize()?;
    println!("Probabilities: {:?}", probs_result.to_ndarray::&lt;f32&gt;()?);

    // Get predicted class
    let prediction = logits.argmax(Some(-1))?;
    let pred_result = prediction.realize()?;
    println!("Predicted digit: {:?}", pred_result.to_ndarray::&lt;i32&gt;()?);

    Ok(())
}
<span class="boring">}</span></code></pre>
<p><strong>Key concepts:</strong></p>
<ol>
<li>
<p><strong>ReLU activation:</strong> <code>x.relu()</code> returns <code>max(0, x)</code>. It introduces non-linearityâ€”without it, stacking linear layers would just be one big linear layer.</p>
</li>
<li>
<p><strong>Logits vs probabilities:</strong> The raw output of the last layer (logits) can be any real number. <code>softmax()</code> converts them to probabilities that sum to 1.</p>
</li>
<li>
<p><strong>argmax:</strong> Returns the index of the maximum valueâ€”the predicted class.</p>
</li>
<li>
<p><strong>Batch dimension:</strong> We use shape <code>[1, 784]</code> for a single image. For 32 images, use <code>[32, 784]</code>. The model handles batches automatically.</p>
</li>
</ol>
<hr>
<h2 id="example-6-under-the-hood"><a class="header" href="#example-6-under-the-hood">Example 6: Under the Hood</a></h2>
<p>Want to see what Morok generates? Hereâ€™s how to inspect the IR and generated code.</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn inspect_compilation() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {
    let a = Tensor::from_slice(&amp;[1.0f32, 2.0, 3.0]);
    let b = Tensor::from_slice(&amp;[4.0f32, 5.0, 6.0]);
    let c = &amp;a + &amp;b;

    // Print the computation graph (before compilation)
    println!("=== IR Graph ===");
    println!("{}", c.uop().tree());

    // Compile and execute
    let result = c.realize()?;

    // Inspect generated kernels
    println!("\n=== Generated Kernels ===");
    for (i, kernel) in result.kernels().iter().enumerate() {
        println!("Kernel {}: {}", i, kernel.name);
        println!("Backend: {}", kernel.backend);
        println!("Code:\n{}\n", kernel.code);
    }

    Ok(())
}
<span class="boring">}</span></code></pre>
<p><strong>What youâ€™ll see:</strong></p>
<ol>
<li>
<p><strong>IR Graph:</strong> The UOp tree shows operations like <code>BUFFER</code>, <code>LOAD</code>, <code>ADD</code>, <code>STORE</code>. This is Morokâ€™s intermediate representation before optimization.</p>
</li>
<li>
<p><strong>Generated Code:</strong> The actual LLVM IR or GPU code that runs. Notice how Morok fuses the loads and add into a single kernelâ€”no intermediate buffers needed.</p>
</li>
</ol>
<p><strong>Debugging tip:</strong> If something seems slow or wrong, print the IR tree. Look for:</p>
<ul>
<li>Unexpected operations (redundant reshapes, extra copies)</li>
<li>Missing fusion (separate kernels where one would do)</li>
<li>Shape mismatches (often the root cause of errors)</li>
</ul>
<hr>
<h2 id="summary"><a class="header" href="#summary">Summary</a></h2>
<p>Youâ€™ve learned the core patterns for using Morok:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Task</th><th>Code</th></tr>
</thead>
<tbody>
<tr><td>Create tensor</td><td><code>Tensor::from_slice(&amp;[1.0f32, 2.0])</code></td></tr>
<tr><td>Arithmetic</td><td><code>&amp;a + &amp;b</code>, <code>&amp;a * &amp;b</code>, <code>-&amp;a</code></td></tr>
<tr><td>Reshape</td><td><code>t.try_reshape(&amp;[2, 3])?</code></td></tr>
<tr><td>Transpose</td><td><code>t.try_transpose(0, 1)?</code></td></tr>
<tr><td>Matrix multiply</td><td><code>a.dot(&amp;b)?</code></td></tr>
<tr><td>Activation</td><td><code>t.relu()?</code>, <code>t.softmax(-1)?</code></td></tr>
<tr><td>Execute</td><td><code>t.realize()?</code></td></tr>
<tr><td>Extract data</td><td><code>result.to_ndarray::&lt;f32&gt;()?</code></td></tr>
</tbody>
</table>
</div>
<p><strong>The lazy evaluation pattern:</strong></p>
<ol>
<li>Build your computation graph with operations</li>
<li>Call <code>realize()</code> once at the end</li>
<li>Morok optimizes and executes everything together</li>
</ol>
<p><strong>Next steps:</strong></p>
<ul>
<li><a href="#op-bestiary-a-field-guide-to-uop-operations">Op Bestiary</a> â€” Reference for IR operations</li>
<li><a href="#from-tensor-to-machine-code">Execution Pipeline</a> â€” How compilation works</li>
<li><a href="#pattern-based-optimization">Optimization System</a> â€” Pattern-based rewrites</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="from-tensor-to-machine-code"><a class="header" href="#from-tensor-to-machine-code">From Tensor to Machine Code</a></h1>
<p>In most ML frameworks, computation happens immediately. Write <code>a + b</code> in PyTorch and it runs <em>now</em>â€”the GPU crunches numbers before you can even inspect the result. This eager execution is simple to understand, but it leaves optimization opportunities on the table. How can a compiler optimize a computation it hasnâ€™t seen yet?</p>
<p>Morok takes the opposite approach: <strong>lazy evaluation</strong>. When you write <code>a.try_add(&amp;b)?</code>, nothing computes. Morok builds a graph describing <em>what</em> to compute, not <em>when</em>. The magic happens when you call <code>realize()</code>â€”that single method triggers the entire compilation pipeline, from high-level tensor operations down to JIT-compiled machine code.</p>
<p>This chapter traces that journey.</p>
<pre><code class="language-text">tensor.realize()
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LAZY GRAPH                                             â”‚
â”‚  Tensor ops build UOp DAG (no computation yet)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RANGEIFY                                               â”‚
â”‚  Movement ops â†’ explicit RANGE loops                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  KERNEL SPLITTING                                       â”‚
â”‚  Split at STORE boundaries â†’ multiple KERNELs          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  OPTIMIZATION &amp; CODEGEN                                 â”‚
â”‚  Heuristics/beam â†’ LLVM IR â†’ JIT compile               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  EXECUTION                                              â”‚
â”‚  Parallel kernel launch â†’ result buffer                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<p>Each box is a distinct phase. Letâ€™s walk through them.</p>
<hr>
<h2 id="lazy-evaluation-building-the-graph"><a class="header" href="#lazy-evaluation-building-the-graph">Lazy Evaluation: Building the Graph</a></h2>
<p>A <code>Tensor</code> in Morok is surprisingly lightweight:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct Tensor {
    entry: Arc&lt;TensorEntry&gt;,      // Computation graph
    buffer: Option&lt;Arc&lt;Buffer&gt;&gt;,  // Materialized data (if any)
}
<span class="boring">}</span></code></pre>
<p>The <code>entry</code> holds a <code>TensorEntry</code> containing the UOp graphâ€”the computation this tensor represents. The <code>buffer</code> is optional: lazy tensors donâ€™t have one, only realized tensors do.</p>
<h3 id="three-ways-to-create-tensors"><a class="header" href="#three-ways-to-create-tensors">Three Ways to Create Tensors</a></h3>
<p><strong>1. Input tensors</strong> â€” buffer allocated immediately:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let a = Tensor::from_slice(&amp;[1.0, 2.0, 3.0], &amp;[3])?;
// `a.buffer` = Some(Arc&lt;Buffer&gt;) with actual data
<span class="boring">}</span></code></pre>
<p>When you create a tensor from data, Morok allocates device memory and copies your bytes. The UOp graph contains a <code>BUFFER</code> node pointing to this allocation.</p>
<p><strong>2. Lazy operations</strong> â€” no buffer, only graph:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let b = a.try_add(&amp;a)?;   // b.buffer = None
let c = b.try_mul(&amp;a)?;   // c.buffer = None
<span class="boring">}</span></code></pre>
<p>Arithmetic operations donâ€™t compute anything. They build a UOp graph: <code>Binary(Add, a.uop, a.uop)</code>. The tensor exists purely as a description of future work.</p>
<p><strong>3. Movement operations</strong> â€” shares the original buffer:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let d = a.try_reshape(&amp;[1, 3])?;  // d.buffer = same as a.buffer
<span class="boring">}</span></code></pre>
<p>Reshape, permute, and similar operations create new <em>views</em> of existing data. The buffer is shared; only the UOp graph changes to describe the new indexing.</p>
<h3 id="the-global-registry"><a class="header" href="#the-global-registry">The Global Registry</a></h3>
<p>Morok maintains three global maps (lock-free, thread-safe):</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Map</th><th>Key â†’ Value</th><th>Purpose</th></tr>
</thead>
<tbody>
<tr><td><code>TENSORS</code></td><td>tensor_id â†’ <code>Weak&lt;TensorEntry&gt;</code></td><td>Track all tensors for graph substitution</td></tr>
<tr><td><code>BUFFERS</code></td><td>uop_id â†’ <code>Arc&lt;Buffer&gt;</code></td><td>Find buffers during scheduling</td></tr>
<tr><td><code>UOP_TO_TENSOR</code></td><td>uop_id â†’ tensor_id</td><td>Secondary index for lookups</td></tr>
</tbody>
</table>
</div>
<p>This registry enables a critical feature: <strong>global graph substitution</strong>. When an optimization transforms a UOp, all tensors referencing that UOp automatically see the updated version. No stale references, no manual updates.</p>
<h3 id="hash-consing-in-action"><a class="header" href="#hash-consing-in-action">Hash Consing in Action</a></h3>
<p>Because UOps use hash consing (content-based deduplication), identical computations share memory:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let x = a.try_add(&amp;b)?;
let y = a.try_add(&amp;b)?;
// x.uop() and y.uop() point to the SAME Arc&lt;UOp&gt;
<span class="boring">}</span></code></pre>
<p>This matters for caching: when we compile kernels, we cache by UOp ID. Hash consing means identical computations automatically hit the cache, even if constructed separately.</p>
<hr>
<h2 id="rangeify-making-loops-explicit"><a class="header" href="#rangeify-making-loops-explicit">Rangeify: Making Loops Explicit</a></h2>
<p>When you write <code>tensor.reshape([2, 3]).expand([4, 2, 3]).sum(axis=0)</code>, those movement operations (reshape, expand) are high-level descriptions. To generate actual loops, we need explicit iteration structure.</p>
<p><strong>Rangeify</strong> transforms movement operations into <code>RANGE</code> loops and <code>INDEX</code> arithmetic. The entry point is <code>rangeify()</code> in <code>schedule/src/rangeify/transforms.rs</code>.</p>
<h3 id="the-8-pass-pipeline"><a class="header" href="#the-8-pass-pipeline">The 8-Pass Pipeline</a></h3>
<p>Rangeify isnâ€™t a single transformationâ€”itâ€™s eight coordinated passes:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Pass</th><th>Purpose</th></tr>
</thead>
<tbody>
<tr><td><strong>1. Range Assignment</strong></td><td>Create RANGE UOps for each tensor dimension</td></tr>
<tr><td><strong>2. Early Rewrites</strong></td><td>Remove DETACH, clean up trivial RESHAPE</td></tr>
<tr><td><strong>3. Split Large Reductions</strong></td><td>Two-stage reduce for huge arrays (ratio &gt; 32768)</td></tr>
<tr><td><strong>4. Core Rangeify</strong></td><td>ReduceAxis â†’ REDUCE, bufferization, movement removal</td></tr>
<tr><td><strong>5. Buffer Folding</strong></td><td>Constant propagation through buffer expressions</td></tr>
<tr><td><strong>6. Dead Axis Removal</strong></td><td>Filter ranges that donâ€™t affect the output</td></tr>
<tr><td><strong>7. Cost-Based Buffer Removal</strong></td><td>Inline buffers when profitable (PContig optimization)</td></tr>
<tr><td><strong>8. Reduction Simplification</strong></td><td>Lift range-independent code out of reductions</td></tr>
</tbody>
</table>
</div>
<p>Each pass uses pattern-based rewriting (see the <a href="#pattern-based-optimization">Pattern-Based Optimization</a> chapter). Patterns fire until no more match, then the next pass begins.</p>
<h3 id="before-and-after"><a class="header" href="#before-and-after">Before and After</a></h3>
<p>Consider this tensor expression:</p>
<pre><code class="language-text">Before: BUFFER.reshape([2, 3]).expand([4, 2, 3]).sum(axis=0)
</code></pre>
<p>After rangeify, movement ops become explicit index computations:</p>
<pre><code class="language-text">After:
STORE
â”œâ”€â”€ INDEX[RANGE(0..2), RANGE(0..3)]
â””â”€â”€ REDUCE(Add)
    â”œâ”€â”€ LOAD
    â”‚   â””â”€â”€ INDEX[RANGE(0..4), RANGE(0..2), RANGE(0..3)]
    â””â”€â”€ RANGE(0..4, Reduce)
</code></pre>
<p>The <code>EXPAND</code> became a <code>RANGE(0..4)</code> that doesnâ€™t affect the buffer indexâ€”broadcasting. The <code>RESHAPE</code> became different index arithmetic. The <code>SUM</code> became <code>REDUCE(Add)</code> with the first range marked as <code>Reduce</code> type.</p>
<h3 id="movement--index-arithmetic"><a class="header" href="#movement--index-arithmetic">Movement â†’ Index Arithmetic</a></h3>
<p>Each movement operation has a specific transformation:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Operation</th><th>Transformation</th></tr>
</thead>
<tbody>
<tr><td><strong>RESHAPE</strong></td><td>Flatten/unflatten index expressions</td></tr>
<tr><td><strong>PERMUTE</strong></td><td>Reorder dimensions in INDEX</td></tr>
<tr><td><strong>EXPAND</strong></td><td>Index becomes 0 (or range doesnâ€™t affect index)</td></tr>
<tr><td><strong>PAD</strong></td><td>WHERE(in_bounds, LOAD, pad_value)</td></tr>
<tr><td><strong>SHRINK</strong></td><td>Offset adjustment in INDEX</td></tr>
<tr><td><strong>FLIP</strong></td><td><code>size - 1 - index</code></td></tr>
</tbody>
</table>
</div>
<p>After rangeify, there are no more movement opsâ€”just arithmetic operations on indices.</p>
<hr>
<h2 id="kernel-splitting-finding-the-boundaries"><a class="header" href="#kernel-splitting-finding-the-boundaries">Kernel Splitting: Finding the Boundaries</a></h2>
<p>A computation graph might have multiple outputs, or intermediate values that need materialization. <strong>Kernel splitting</strong> identifies these boundaries and creates separate kernels.</p>
<p>The entry point is <code>run_kernel_split_pipeline()</code> in <code>schedule/src/rangeify/kernel.rs</code>.</p>
<h3 id="two-phase-transformation"><a class="header" href="#two-phase-transformation">Two-Phase Transformation</a></h3>
<p><strong>Phase 1: BUFFERIZE â†’ STORE</strong></p>
<p><code>BUFFERIZE</code> nodes mark where values should materialize. Phase 1 converts them to explicit <code>STORE</code> operations:</p>
<pre><code class="language-text">Before: BUFFERIZE(computation, ranges)
After:  END(STORE(buffer, INDEX(...), computation), ranges)
</code></pre>
<p>The <code>END</code> wrapper captures which ranges scope this store. Buffers are allocated and assigned IDs during this phase.</p>
<p><strong>Phase 2: STORE â†’ KERNEL</strong></p>
<p>Each <code>STORE</code> becomes its own kernel:</p>
<pre><code class="language-text">Before: END(STORE(...), ranges)
After:  KERNEL(SINK(STORE(...)), ranges, buffer_list)
</code></pre>
<p>The <code>KERNEL</code> node wraps everything: the computation (as a <code>SINK</code>), the iteration ranges, and the list of buffers this kernel reads and writes.</p>
<h3 id="tracking-dependencies"><a class="header" href="#tracking-dependencies">Tracking Dependencies</a></h3>
<p>When one kernelâ€™s output feeds another kernelâ€™s input, we need dependency tracking:</p>
<ol>
<li><code>fix_assign()</code> maps each buffer_id to the kernel that writes it</li>
<li>When kernel B reads a buffer written by kernel A, B depends on A</li>
<li><code>resolve_kernel_dependencies()</code> builds the dependency graph</li>
</ol>
<p>Dependencies appear as <code>AFTER</code> nodes in the IR, ensuring kernels execute in valid order.</p>
<h3 id="buffer-renumbering"><a class="header" href="#buffer-renumbering">Buffer Renumbering</a></h3>
<p>Each kernel sees buffers in a specific order (outputs first, then inputs). <code>renumber_define_globals()</code> remaps buffer IDs to match this ordering:</p>
<pre><code class="language-text">Original: buffer_3, buffer_1, buffer_7
Kernel view: buffer_0 (output), buffer_1, buffer_2 (inputs)
</code></pre>
<p>This simplifies code generationâ€”buffer <code>N</code> is always argument <code>N</code>.</p>
<hr>
<h2 id="schedule-creation-preparing-for-execution"><a class="header" href="#schedule-creation-preparing-for-execution">Schedule Creation: Preparing for Execution</a></h2>
<p>Once kernels are split, we need to <strong>schedule</strong> them: determine execution order, allocate buffers, and prepare for compilation.</p>
<p><code>create_schedule()</code> in <code>tensor/src/schedule.rs</code> produces a <code>Vec&lt;ScheduleItem&gt;</code>:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct ScheduleItem {
    pub kernel: Arc&lt;UOp&gt;,              // KERNEL wrapper
    pub ast: Arc&lt;UOp&gt;,                 // Inner computation (for codegen)
    pub buffers: Vec&lt;Buffer&gt;,          // Device buffers
    pub dependencies: Vec&lt;u64&gt;,        // Producer kernel IDs
    pub fixedvars: HashMap&lt;String, i64&gt;,  // Bound iteration variables
}
<span class="boring">}</span></code></pre>
<h3 id="buffer-allocation-strategy"><a class="header" href="#buffer-allocation-strategy">Buffer Allocation Strategy</a></h3>
<ul>
<li><strong>Input buffers</strong>: Already allocated (from <code>Tensor::from_slice</code>)</li>
<li><strong>Intermediate buffers</strong>: Allocated during scheduling (for kernel outputs that feed other kernels)</li>
<li><strong>Output buffer</strong>: Allocated and registered with the final tensor</li>
</ul>
<h3 id="parallel-group-analysis"><a class="header" href="#parallel-group-analysis">Parallel Group Analysis</a></h3>
<p>Not all kernels need sequential execution. Independent kernels can run in parallel:</p>
<pre><code class="language-text">Kernel A (writes buf0)
Kernel B (writes buf1)  â”€â”€â”€ no dependency â”€â”€â”€ can run in parallel
Kernel C (reads buf0, buf1)  â”€â”€â”€ depends on A and B
</code></pre>
<p>The scheduler uses <strong>Kahnâ€™s algorithm</strong> to find parallel groups:</p>
<ol>
<li>Build the kernel dependency DAG</li>
<li>Find all kernels with no incoming edges â†’ Group 1</li>
<li>Remove Group 1, repeat â†’ Group 2, etc.</li>
</ol>
<p>Each groupâ€™s kernels execute in parallel, then the next group starts.</p>
<hr>
<h2 id="code-generation-from-uop-to-llvm-ir"><a class="header" href="#code-generation-from-uop-to-llvm-ir">Code Generation: From UOp to LLVM IR</a></h2>
<p>With kernels scheduled, we generate actual code. Morok currently supports the LLVM backend:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Backend</th><th>Compile Speed</th><th>Output Quality</th><th>Use Case</th></tr>
</thead>
<tbody>
<tr><td><strong>LLVM</strong></td><td>Slower</td><td>Highly optimized</td><td>Production</td></tr>
</tbody>
</table>
</div>
<p>The <code>Renderer</code> trait abstracts code generation:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub trait Renderer {
    fn render(&amp;self, uop: &amp;Arc&lt;UOp&gt;, name: Option&lt;&amp;str&gt;) -&gt; Result&lt;RenderedKernel&gt;;
}
<span class="boring">}</span></code></pre>
<h3 id="llvm-cpu-renderer"><a class="header" href="#llvm-cpu-renderer">LLVM CPU Renderer</a></h3>
<p>The LLVM renderer (<code>codegen/src/llvm/cpu/</code>) traverses the UOp graph and emits LLVM IR:</p>
<pre><code class="language-llvm">define void @kernel_0(ptr %args, ptr %vars) {
entry:
  %buf0 = load ptr, ptr %args
  %buf1 = load ptr, ptr getelementptr(ptr, ptr %args, i64 1)
  ; ... loop nest ...
  br label %loop_0

loop_0:
  %i = phi i64 [ 0, %entry ], [ %i.next, %loop_0 ]
  ; ... computation ...
  %i.next = add i64 %i, 1
  %cond = icmp slt i64 %i.next, 128
  br i1 %cond, label %loop_0, label %exit

exit:
  ret void
}
</code></pre>
<p>The generated kernel takes two arguments:</p>
<ul>
<li><code>args</code>: Array of buffer pointers</li>
<li><code>vars</code>: Array of symbolic variable values (for dynamic shapes)</li>
</ul>
<h3 id="post-optimization-passes"><a class="header" href="#post-optimization-passes">Post-Optimization Passes</a></h3>
<p>Before code generation, 13+ pattern-based passes clean up the IR:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Pass</th><th>Purpose</th></tr>
</thead>
<tbody>
<tr><td><code>pm_add_loads</code></td><td>Wrap INDEX operations in LOAD</td></tr>
<tr><td><code>pre_expand</code></td><td>Convert UNROLL/UPCAST ranges to explicit operations</td></tr>
<tr><td><code>devectorize</code></td><td>Group contiguous memory accesses</td></tr>
<tr><td><code>pm_reduce_devectorize</code></td><td>Handle vector reductions (K-vec, bool, horizontal)</td></tr>
<tr><td><code>pm_fma_decomposition</code></td><td>Convert <code>a*b+c</code> to fused multiply-add</td></tr>
<tr><td><code>bool_storage_patterns</code></td><td>Convert bool â†” uint8 for memory operations</td></tr>
</tbody>
</table>
</div>
<p>These passes transform the optimized AST into a form suitable for code generation. The result is clean, vectorized code with proper memory access patterns.</p>
<hr>
<h2 id="execution-running-the-kernels"><a class="header" href="#execution-running-the-kernels">Execution: Running the Kernels</a></h2>
<p>Code generation produces LLVM IR strings. Execution involves JIT compilation and kernel launch.</p>
<h3 id="the-executionplan"><a class="header" href="#the-executionplan">The ExecutionPlan</a></h3>
<p><code>prepare_execution_plan()</code> builds an <code>ExecutionPlan</code>:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct ExecutionPlan {
    kernels: Vec&lt;PreparedKernel&gt;,       // Compiled kernels
    parallel_groups: Vec&lt;ParallelGroup&gt;,
    buffers: Vec&lt;Buffer&gt;,
    output_buffer_idx: usize,
}
<span class="boring">}</span></code></pre>
<p>The plan is <strong>reusable</strong>: compile once, execute many times with different data.</p>
<h3 id="jit-compilation"><a class="header" href="#jit-compilation">JIT Compilation</a></h3>
<p>The LLVM runtime (<code>runtime/src/llvm.rs</code>) compiles IR to machine code:</p>
<ol>
<li><strong>Parse</strong> the LLVM IR string into a module</li>
<li><strong>Verify</strong> the module is well-formed</li>
<li><strong>Optimize</strong> with LLVMâ€™s O3 pass pipeline</li>
<li><strong>JIT compile</strong> to native machine code</li>
<li><strong>Cache</strong> by (AST ID, device) for reuse</li>
</ol>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Simplified JIT flow
let module = Module::parse_ir(context, ir_string)?;
module.verify()?;
pass_manager.run(&amp;module);  // O3 optimization
let function = execution_engine.get_function::&lt;KernelFn&gt;(&amp;name)?;
// Cache: (ast_id, device) â†’ function
<span class="boring">}</span></code></pre>
<h3 id="parallel-execution"><a class="header" href="#parallel-execution">Parallel Execution</a></h3>
<p>With kernels compiled, execution follows the parallel groups:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>for group in &amp;plan.parallel_groups {
    if group.kernel_indices.len() == 1 {
        // Single kernel: direct call
        execute_kernel(&amp;kernels[group.kernel_indices[0]]);
    } else {
        // Multiple kernels: parallel execution
        rayon::scope(|s| {
            for &amp;idx in &amp;group.kernel_indices {
                s.spawn(|_| execute_kernel(&amp;kernels[idx]));
            }
        });
    }
}
<span class="boring">}</span></code></pre>
<p>Independent kernels run in parallel using Rayonâ€™s work-stealing scheduler.</p>
<h3 id="kernel-caching"><a class="header" href="#kernel-caching">Kernel Caching</a></h3>
<p>Hash consing makes kernel caching highly effective:</p>
<ul>
<li><strong>Key</strong>: <code>(UOp ID, device string)</code></li>
<li><strong>Storage</strong>: Lock-free HashMap (papaya crate)</li>
<li><strong>Hit rate</strong>: High, because identical computations share UOp IDs</li>
</ul>
<p>When you compute the same expression twice, the second call hits the cacheâ€”no recompilation.</p>
<hr>
<h2 id="worked-example-matrix-multiply"><a class="header" href="#worked-example-matrix-multiply">Worked Example: Matrix Multiply</a></h2>
<p>Letâ€™s trace <code>C = A @ B</code> through the entire pipeline. Assume 4Ã—4 matrices.</p>
<h3 id="stage-1-lazy-graph-construction"><a class="header" href="#stage-1-lazy-graph-construction">Stage 1: Lazy Graph Construction</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let a = Tensor::from_slice(&amp;a_data, &amp;[4, 4])?;  // Input buffer allocated
let b = Tensor::from_slice(&amp;b_data, &amp;[4, 4])?;  // Input buffer allocated
let c = a.matmul(&amp;b)?;                           // Graph built, no computation
<span class="boring">}</span></code></pre>
<p>At this point, <code>c</code> is a lazy tensor with this UOp graph:</p>
<pre><code class="language-text">REDUCE_AXIS(Add, axis=2)
â””â”€â”€ MUL
    â”œâ”€â”€ EXPAND(A, [4, 4, 4])    â€” A: [4, 4] â†’ [4, 1, 4] â†’ [4, 4, 4]
    â””â”€â”€ EXPAND(B, [4, 4, 4])    â€” B: [4, 4] â†’ [1, 4, 4] â†’ [4, 4, 4]
</code></pre>
<h3 id="stage-2-rangeify"><a class="header" href="#stage-2-rangeify">Stage 2: Rangeify</a></h3>
<p>Movement ops become explicit loops:</p>
<pre><code class="language-text">STORE
â”œâ”€â”€ BUFFER(C)
â”œâ”€â”€ INDEX[RANGE(i, 0..4), RANGE(j, 0..4)]
â””â”€â”€ REDUCE(Add)
    â”œâ”€â”€ MUL
    â”‚   â”œâ”€â”€ LOAD(A)
    â”‚   â”‚   â””â”€â”€ INDEX[RANGE(i), RANGE(k, 0..4, Reduce)]
    â”‚   â””â”€â”€ LOAD(B)
    â”‚       â””â”€â”€ INDEX[RANGE(k), RANGE(j)]
    â””â”€â”€ RANGE(k, Reduce)
</code></pre>
<p>The <code>i</code> and <code>j</code> ranges are output dimensions. The <code>k</code> range is the reduction (contracted) dimension.</p>
<h3 id="stage-3-kernel-splitting"><a class="header" href="#stage-3-kernel-splitting">Stage 3: Kernel Splitting</a></h3>
<p>Single STORE â†’ single KERNEL:</p>
<pre><code class="language-text">KERNEL
â”œâ”€â”€ SINK(STORE(...))
â”œâ”€â”€ ranges: [i: 0..4, j: 0..4]
â””â”€â”€ buffers: [C (output), A (input), B (input)]
</code></pre>
<h3 id="stage-4-schedule"><a class="header" href="#stage-4-schedule">Stage 4: Schedule</a></h3>
<p>One <code>ScheduleItem</code> with:</p>
<ul>
<li><code>kernel</code>: The KERNEL UOp</li>
<li><code>ast</code>: The inner SINK/STORE</li>
<li><code>buffers</code>: [C, A, B]</li>
<li><code>dependencies</code>: [] (no prior kernels)</li>
</ul>
<h3 id="stage-5-optimization"><a class="header" href="#stage-5-optimization">Stage 5: Optimization</a></h3>
<p>Heuristic optimizer applies:</p>
<ul>
<li>Vectorization: UPCAST j dimension by 4</li>
<li>Loop ordering: Ensure good cache behavior</li>
</ul>
<h3 id="stage-6-code-generation"><a class="header" href="#stage-6-code-generation">Stage 6: Code Generation</a></h3>
<p>Generated LLVM IR (simplified):</p>
<pre><code class="language-llvm">define void @matmul(ptr %args, ptr %vars) {
entry:
  %C = load ptr, ptr %args
  %A = load ptr, ptr getelementptr(ptr, ptr %args, i64 1)
  %B = load ptr, ptr getelementptr(ptr, ptr %args, i64 2)
  br label %loop_i

loop_i:
  %i = phi i64 [ 0, %entry ], [ %i.next, %loop_i.end ]
  br label %loop_j

loop_j:
  %j = phi i64 [ 0, %loop_i ], [ %j.next, %loop_k.end ]
  %acc = ... ; initialize accumulator
  br label %loop_k

loop_k:
  %k = phi i64 [ 0, %loop_j ], [ %k.next, %loop_k ]
  %a_val = load float, ptr ...  ; A[i, k]
  %b_val = load float, ptr ...  ; B[k, j]
  %prod = fmul float %a_val, %b_val
  %acc.new = fadd float %acc, %prod
  %k.next = add i64 %k, 1
  %k.cond = icmp slt i64 %k.next, 4
  br i1 %k.cond, label %loop_k, label %loop_k.end

loop_k.end:
  store float %acc.new, ptr ...  ; C[i, j]
  ; ... continue j, i loops
}
</code></pre>
<h3 id="stage-7-execution"><a class="header" href="#stage-7-execution">Stage 7: Execution</a></h3>
<ol>
<li>JIT compile the LLVM IR</li>
<li>Execute: <code>kernel([C_ptr, A_ptr, B_ptr], [])</code></li>
<li>Result is in C buffer</li>
</ol>
<p>Total: one function call, result ready.</p>
<hr>
<h2 id="comparison-how-other-frameworks-execute"><a class="header" href="#comparison-how-other-frameworks-execute">Comparison: How Other Frameworks Execute</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Aspect</th><th>PyTorch</th><th>JAX</th><th>TVM</th><th><strong>Morok</strong></th></tr>
</thead>
<tbody>
<tr><td><strong>Evaluation</strong></td><td>Eager (immediate)</td><td>Traced (jit decorator)</td><td>Lazy (te.compute)</td><td>Lazy (realize)</td></tr>
<tr><td><strong>Graph capture</strong></td><td>torch.compile</td><td>jax.jit trace</td><td>Explicit schedule</td><td>Implicit via ops</td></tr>
<tr><td><strong>Compilation</strong></td><td>TorchInductor</td><td>XLA backend</td><td>Auto-scheduler</td><td>Pattern + beam</td></tr>
<tr><td><strong>Caching</strong></td><td>Per-graph hash</td><td>Per-trace</td><td>Per-schedule</td><td>Per-AST (hash consing)</td></tr>
<tr><td><strong>Parallelism</strong></td><td>DataParallel/DDP</td><td>pmap/pjit</td><td>Parallel schedule</td><td>Parallel groups</td></tr>
</tbody>
</table>
</div>
<p><strong>PyTorch</strong>: Eager by default, torch.compile for optimization. TorchInductor generates Triton or C++ code.</p>
<p><strong>JAX</strong>: Functional transformations (jit, grad, vmap) trace computations. XLA compiles to optimized kernels.</p>
<p><strong>TVM</strong>: Explicit separation of computation and schedule. Auto-scheduler searches for good schedules.</p>
<p><strong>Morok</strong>: Fully lazyâ€”nothing executes until <code>realize()</code>. Hash consing provides automatic caching. Pattern-based optimization with optional beam search for production quality.</p>
<hr>
<h2 id="the-deeper-insight"><a class="header" href="#the-deeper-insight">The Deeper Insight</a></h2>
<p>The pipeline embodies several design principles:</p>
<p><strong>Lazy evaluation enables global optimization.</strong> By deferring computation, we see the entire graph before generating code. No local decision limits global optimization.</p>
<p><strong>Explicit loops enable hardware-specific scheduling.</strong> Movement ops are convenient abstractions, but GPUs need loops. Rangeify bridges the gap.</p>
<p><strong>Hash consing makes caching automatic.</strong> Identical computations share pointers, so cache keys are trivial. No complex graph hashing needed.</p>
<p><strong>Separation of concerns keeps each stage simple.</strong> Rangeify doesnâ€™t know about LLVM. Code generation doesnâ€™t know about tensor semantics. Each stage does one thing well.</p>
<p>The result: a compilation pipeline thatâ€™s both powerful and maintainable. From <code>tensor.realize()</code> to machine code, every step is visible, debuggable, and extensible.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="path-of-the-uop-the-22-stage-codegen-pipeline"><a class="header" href="#path-of-the-uop-the-22-stage-codegen-pipeline">Path of the UOp: The 22-Stage Codegen Pipeline</a></h1>
<p>A UOp starts as a high-level tensor expression. By the time it reaches the hardware, it has been transformed through 22 distinct stagesâ€”each with a specific purpose, each building on the last. This chapter traces that journey.</p>
<p>The pipeline is a proven design for tensor compilation. Understanding it means understanding how tensor expressions become machine code.</p>
<hr>
<h2 id="how-to-read-this-chapter"><a class="header" href="#how-to-read-this-chapter">How to Read This Chapter</a></h2>
<p>If youâ€™re not a compiler engineer, this chapter might seem intimidating. Hereâ€™s what you need to understand before diving in.</p>
<h3 id="key-concepts"><a class="header" href="#key-concepts">Key Concepts</a></h3>
<p><strong>UOp (Micro-Operation)</strong></p>
<ul>
<li>Think of it as a node in a flowchart representing one computation</li>
<li>Example: <code>ADD(a, b)</code> means â€œadd a and bâ€</li>
</ul>
<p><strong>Pattern</strong></p>
<ul>
<li>A find-and-replace rule for code structures (not text)</li>
<li>Example: â€œIf you see ADD(x, 0), replace with xâ€</li>
<li>Patterns fire repeatedly until no more matches (fixpoint)</li>
</ul>
<p><strong>Range</strong></p>
<ul>
<li>A loop iteration: <code>RANGE(0..10)</code> means â€œfor i from 0 to 10â€</li>
</ul>
<p><strong>AxisType</strong></p>
<ul>
<li>What kind of loop is this?
<ul>
<li>Global: Parallel across GPU blocks / CPU threads</li>
<li>Local: Parallel within a workgroup</li>
<li>Reduce: Accumulator (sum, max, etc.)</li>
<li>Loop: Sequential iteration</li>
</ul>
</li>
</ul>
<p><strong>Stage</strong></p>
<ul>
<li>One transformation pass through the code</li>
<li>Patterns fire until fixpoint, then move to the next stage</li>
</ul>
<h3 id="reading-strategy"><a class="header" href="#reading-strategy">Reading Strategy</a></h3>
<ol>
<li><strong>First pass</strong>: Read just the â€œWhat This Doesâ€ and â€œWhy This Mattersâ€ sections</li>
<li><strong>Second pass</strong>: Look at the diagrams and examples</li>
<li><strong>Third pass</strong> (if you want details): Read the pattern descriptions</li>
</ol>
<h3 id="questions-to-ask"><a class="header" href="#questions-to-ask">Questions to Ask</a></h3>
<p>For each stage, ask:</p>
<ul>
<li>What does this stage accomplish? (High-level goal)</li>
<li>Why do we need this stage? (Motivation)</li>
<li>What would go wrong without it? (Consequences)</li>
</ul>
<hr>
<h2 id="overview"><a class="header" href="#overview">Overview</a></h2>
<p>The 22 stages fall into four phases:</p>
<pre><code class="language-text">Tensor Expression
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RANGEIFY (Stages 1-7)               â”‚
â”‚ Movement ops â†’ Explicit loops       â”‚
â”‚                                     â”‚
â”‚ [Make iteration explicit,           â”‚
â”‚  optimize ranges]                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ EXPANDER (Stages 8-10)              â”‚
â”‚ UNROLL/UPCAST â†’ Explicit vectors    â”‚
â”‚                                     â”‚
â”‚ [Expand optimization primitives]    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DEVECTORIZER (Stages 11-15)         â”‚
â”‚ Vector ops â†’ Scalar code            â”‚
â”‚                                     â”‚
â”‚ [Lower to hardware-specific ops]    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LINEARIZER (Stages 16-22)           â”‚
â”‚ IR â†’ Linear instruction sequence    â”‚
â”‚                                     â”‚
â”‚ [Serialize to executable code]      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
  Machine Code
</code></pre>
<p>Each stage applies pattern-based rewrites. Patterns fire until fixpoint, then the next stage begins.</p>
<hr>
<h2 id="phase-1-rangeify"><a class="header" href="#phase-1-rangeify">Phase 1: Rangeify</a></h2>
<p><strong>Goal</strong>: Transform high-level movement operations into explicit loop structures and optimize ranges.</p>
<hr>
<h3 id="stage-1-early-movement-ops"><a class="header" href="#stage-1-early-movement-ops">Stage 1: Early Movement Ops</a></h3>
<blockquote>
<p><strong>Stage at a Glance</strong></p>
<p><strong>Goal</strong>: Clean up movement operations before range assignment
<strong>Key Patterns</strong>: Movement on INDEX, movement through wrappers, nested INDEX simplification
<strong>Impact</strong>: Prevents missed optimizations later in the pipeline</p>
</blockquote>
<p><strong>What This Does</strong>: This stage cleans up movement operations by pushing index manipulations into places where theyâ€™re actually needed. Think of it as organizing your desk before filing papersâ€”move instructions closer to where the data is used.</p>
<p><strong>Why This Matters</strong>: Movement operations (RESHAPE, PERMUTE, etc.) are convenient abstractions, but the hardware needs concrete index calculations. By cleaning them up early, we ensure patterns in later stages can match correctly.</p>
<p><strong>Pattern</strong>: <code>pm_mops + pm_syntactic_sugar</code> (bottom-up)</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Pattern</th><th>Transformation</th><th>Visual</th><th>Location</th></tr>
</thead>
<tbody>
<tr><td>Movement on INDEX</td><td>Apply movement to index expressions</td><td><code>INDEX(PERMUTE(arr), [i, j]) â†’ INDEX(arr, [j, i])</code></td><td><code>movement_op_patterns()</code></td></tr>
<tr><td>Movement through AFTER</td><td>Move RESHAPE through timing wrapper (Tinygrad-specific)</td><td><code>AFTER(RESHAPE(x, arg), [dep1, dep2]) â†’ RESHAPE(AFTER(x, [dep2]), arg)</code></td><td>Tinygrad only</td></tr>
<tr><td>Movement through END</td><td>Unwrap movement from END wrapper (Tinygrad-specific)</td><td><code>END(RESHAPE(x), ranges) â†’ END(x, ranges)</code></td><td>Tinygrad only</td></tr>
<tr><td>Nested INDEX simplification</td><td>Remove redundant nested INDEX (Morok)</td><td><code>INDEX(INDEX(ptr, [i]), [i]) â†’ INDEX(ptr, [i])</code></td><td><code>movement_op_patterns()</code></td></tr>
<tr><td>Nested INDEX concat</td><td>Flatten nested INDEX for PtrDType</td><td><code>INDEX(INDEX(ptr, i), j) â†’ INDEX(ptr, i, j)</code></td><td><code>pm_syntactic_sugar</code></td></tr>
</tbody>
</table>
</div>
<p><strong>Why bottom-up?</strong> Child nodes must be clean before parents can match. Movement ops nest deeply; cleaning from bottom prevents missed patterns.</p>
<p><strong>Note</strong>: Tinygrad and Morok have different approaches here. Tinygrad moves movement ops through wrappers (AFTER, END) because it re-applies movement ops during bufferization. Morok removes movement ops entirely by transforming indices during bufferization, so AFTER/END patterns are not needed.</p>
<p><strong>Morok</strong>: <code>movement_op_patterns()</code> in <code>rangeify/patterns.rs</code></p>
<hr>
<h3 id="stage-2-load-collapse"><a class="header" href="#stage-2-load-collapse">Stage 2: Load Collapse</a></h3>
<blockquote>
<p><strong>Stage at a Glance</strong></p>
<p><strong>Goal</strong>: Eliminate REDUCE operations by detecting range-independent computation
<strong>Key Patterns</strong>: Bounded sum, gated load collapse, general reduce elimination
<strong>Impact</strong>: Converts loop iterations to arithmetic operations</p>
</blockquote>
<p><strong>What This Does</strong>: Eliminates REDUCE operations by recognizing when the computation can be done without iteration. Uses range-independent computation detection and symbolic simplification.</p>
<p><strong>Why This Matters</strong>: Reducing iterations to arithmetic operations eliminates loop overhead. Instead of running a loop 1000 times, compute the answer directly.</p>
<p><strong>Pattern</strong>: <code>pm_load_collapse</code></p>
<pre><code class="language-text">// Before: Sum with bounds check
sum(1 for k in 0..64 if k &gt;= length)

// After: Compute count directly (NO LOOP!)
count = clamp(64 - length, 0, 64)
</code></pre>
<p>The mechanism works by:</p>
<ol>
<li>Identifying subexpressions that donâ€™t depend on the REDUCE range</li>
<li>Creating DEFINE_VAR for those subexpressions (treats as loop-invariant)</li>
<li>Substituting the range with DEFINE_VAR and running symbolic simplification</li>
<li>If the simplified expression has no more ranges, the REDUCE is eliminated</li>
</ol>
<p><strong>Note</strong>: WHERE movement through INDEX (<code>pm_move_where_on_load</code> in Stage 8) is a separate optimization that places conditionals before loads to skip memory accesses, but it doesnâ€™t eliminate REDUCE operations.</p>
<p><strong>Morok</strong>: <code>pm_load_collapse()</code> in <code>rangeify/patterns.rs</code></p>
<hr>
<h3 id="stage-3-split-ranges"><a class="header" href="#stage-3-split-ranges">Stage 3: Split Ranges</a></h3>
<blockquote>
<p><strong>Stage at a Glance</strong></p>
<p><strong>Goal</strong>: Enable better optimization through divmod decomposition
<strong>Key Patterns</strong>: Split ranges with modulo, flatten ranges
<strong>Impact</strong>: Inner ranges can vectorize, outer can parallelize</p>
</blockquote>
<p><strong>What This Does</strong>: Handles modulo patterns by splitting a range into outer and inner components.</p>
<p><strong>Why This Matters</strong>: Splitting ranges is like dividing a large task among team members. If you have 12 items and each person does 4, you get 3 people Ã— 4 items. Inner loops (one personâ€™s 4 items) can be fast; outer loops (3 people) can run in parallel.</p>
<p><strong>Pattern</strong>: <code>pm_split_ranges + pm_flatten_range</code></p>
<pre><code class="language-text">Before:  RANGE(end=12) % 4  // One loop with modulo (slow)
             â†“ [Split into outer Ã— inner]
After:   RANGE(end=3) * 4 + RANGE(end=4)
            â†‘outer        â†‘inner
            Parallel      Sequential
</code></pre>
<p>This enables:</p>
<ul>
<li>Inner ranges can vectorize (SIMD)</li>
<li>Outer ranges can parallelize (GPU blocks / CPU threads)</li>
</ul>
<p><code>pm_flatten_range</code> merges nested ranges on REDUCE/STORE/END when beneficial.</p>
<p><strong>Context</strong>: Requires dictionary context (<code>ctx={}</code>) to track substitutions at SINK.</p>
<p><strong>Note</strong>: The split only applies when <code>end % mod == 0</code> (divisibility check).</p>
<p><strong>Morok</strong>: <code>pm_split_ranges()</code> + <code>pm_flatten_range()</code> in <code>rangeify/transforms.rs</code></p>
<hr>
<h3 id="stage-4-initial-symbolic"><a class="header" href="#stage-4-initial-symbolic">Stage 4: Initial Symbolic</a></h3>
<blockquote>
<p><strong>Stage at a Glance</strong></p>
<p><strong>Goal</strong>: Simplify expressions using algebra rules
<strong>Key Patterns</strong>: Constant folding, identity removal, div-mod recombine
<strong>Impact</strong>: Eliminates expensive operations, reduces code size</p>
</blockquote>
<p><strong>What This Does</strong>: Applies 100+ constant folding and algebraic simplification rules.</p>
<p><strong>Why This Matters</strong>: Computers are fast at simple math. Dividing and taking remainders are slow operations. This stage uses algebra rules to eliminate slow operations whenever possible.</p>
<p><strong>Pattern</strong>: <code>sym + pm_flatten_range</code></p>
<p><strong>Constant folding</strong>:</p>
<pre><code class="language-text">ADD(CONST(2), CONST(3)) â†’ CONST(5)
MUL(x, CONST(1)) â†’ x
ADD(x, CONST(0)) â†’ x
</code></pre>
<p><strong>Div-mod recombination</strong>:</p>
<pre><code class="language-text">(x / c) * c + (x % c) â†’ x
</code></pre>
<p><em>Why?</em> Computes the same value as <code>x</code> but with 3 operations instead of 1. This pattern finds and removes the redundancy (common in stride calculations).</p>
<p><strong>Boolean algebra</strong>:</p>
<pre><code class="language-text">x AND x â†’ x
x OR FALSE â†’ x
NOT(NOT(x)) â†’ x
</code></pre>
<p><strong>Additional categories</strong>:</p>
<ul>
<li>Identity removal (self-folding, redundant operations)</li>
<li>Comparison simplification</li>
<li>Cast optimization</li>
<li>GEP pushing (move address calculations through ALUs)</li>
<li>Where folding (combine WHERE with same conditions)</li>
<li>Reduce mul chain (move multiplications outside reduce)</li>
</ul>
<p><strong>Morok</strong>: <code>symbolic_patterns()</code> in <code>symbolic/patterns.rs</code></p>
<hr>
<h3 id="stage-5-simplify-ranges"><a class="header" href="#stage-5-simplify-ranges">Stage 5: Simplify Ranges</a></h3>
<blockquote>
<p><strong>Stage at a Glance</strong></p>
<p><strong>Goal</strong>: Merge adjacent ranges to reduce loop overhead
<strong>Key Patterns</strong>: Range merging with cost analysis
<strong>Impact</strong>: Fewer loops = less overhead</p>
</blockquote>
<p><strong>What This Does</strong>: Merges adjacent ranges when profitable.</p>
<p><strong>Why This Matters</strong>: Merging ranges is like combining multiple small trips into one big one. Instead of going to the store 4 times for 4 items, go once for all 4 items. Saves the overhead of starting and stopping.</p>
<p><strong>Pattern</strong>: <code>pm_simplify_ranges</code></p>
<pre><code class="language-text">// Before: two separate ranges
RANGE(0..4), RANGE(0..8)

// After: merged (if compatible)
RANGE(0..32)
</code></pre>
<p>Merge criteria:</p>
<ol>
<li>Axis types must be compatible (both output, both reduce, etc.)</li>
<li>REDUCE scope must remain consistent</li>
<li><strong>Cost-based</strong>: Accept only if divmod operation count does not increase</li>
</ol>
<p>The compiler only merges if it saves operations. Merging might require division/modulo to recalculate indices. If that costs more than it saves, merge is skipped.</p>
<p><strong>Morok</strong>: <code>simplify_merge_adjacent()</code> in <code>rangeify/transforms.rs</code></p>
<hr>
<h3 id="stage-6-split-store-cpu-only"><a class="header" href="#stage-6-split-store-cpu-only">Stage 6: Split Store (CPU-only)</a></h3>
<blockquote>
<p><strong>Stage at a Glance</strong></p>
<p><strong>Goal</strong>: Avoid branch misprediction by splitting conditional stores
<strong>Key Patterns</strong>: Split store ranges at comparison boundaries
<strong>Impact</strong>: More predictable CPU execution</p>
</blockquote>
<p><strong>What This Does</strong>: Splits store ranges at conditional boundaries when there are <code>CMPLT(range, const)</code> comparisons in the storeâ€™s consumer map.</p>
<p><strong>Why This Matters</strong>: Branch misprediction slows down CPUs. Instead of one loop with an <code>if</code> statement that the CPU canâ€™t predict, we create two loops without conditionals. Each loop does predictable work, so the CPU stays fast.</p>
<p><strong>Pattern</strong>: <code>pm_split_store</code></p>
<pre><code class="language-text">// Before: Store with conditional (branch misprediction risk)
for i in 0..100:
    if i &lt; 50:
        output[i] = data[i]

// After: Two unconditional stores (predictable)
for i in 0..50:   // First loop
    output[i] = data[i]
for i in 50..100: // Second loop
    output[i] = data[i]
</code></pre>
<p>The transformation finds constant comparison points in the storeâ€™s consumer map and creates disjoint ranges for each segment.</p>
<p>Skipped for GPU devicesâ€”they handle conditionals differently.</p>
<p><strong>Morok</strong>: <code>pm_split_store()</code> in <code>rangeify/transforms.rs</code></p>
<hr>
<h3 id="stage-7-apply-opts"><a class="header" href="#stage-7-apply-opts">Stage 7: Apply Opts</a></h3>
<blockquote>
<p><strong>Stage at a Glance</strong></p>
<p><strong>Goal</strong>: Find optimal combination of vectorization, unrolling, memory usage
<strong>Key Algorithm</strong>: Beam search or heuristics
<strong>Impact</strong>: Can significantly improve performance</p>
</blockquote>
<p><strong>What This Does</strong>: The optimization searchâ€”either beam search or heuristicâ€”explores different combinations of optimization actions.</p>
<p><strong>Why This Matters</strong>: The compiler tries different combinations of optimizations (vectorize here? unroll there?) and picks the fastest. Finding the right combination can make code 10x faster.</p>
<p><strong>Function</strong>: <code>apply_opts(sink, renderer)</code></p>
<p><strong>Optimization actions</strong>:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Action</th><th>Effect</th><th>Hardware Target</th></tr>
</thead>
<tbody>
<tr><td></td><td>TC</td><td>Enable tensor core usage</td></tr>
<tr><td></td><td>UPCAST</td><td>Vectorize a dimension</td></tr>
<tr><td></td><td>LOCAL</td><td>Use local/shared memory</td></tr>
<tr><td></td><td>UNROLL</td><td>Unroll a loop dimension</td></tr>
<tr><td></td><td>GROUP</td><td>Group operations for cache</td></tr>
<tr><td></td><td>GROUPTOP</td><td>Group for reduce ops</td></tr>
<tr><td></td><td>THREAD</td><td>Thread-based parallelism</td></tr>
<tr><td></td><td>NOLOCALS</td><td>Disable local memory usage</td></tr>
<tr><td></td><td>SWAP</td><td>Swap range assignments</td></tr>
<tr><td></td><td>PADTO</td><td>Pad for alignment</td></tr>
</tbody>
</table>
</div>
<p><strong>Optimization Search Explained</strong>:</p>
<p>The compiler searches for the best combination:</p>
<ul>
<li><strong>Heuristic mode</strong> (BEAM=0): Fast hand-coded optimization patterns, no compilation</li>
<li><strong>Beam search</strong> (BEAMâ‰¥1): Compiles and runs candidates to measure actual performance</li>
</ul>
<pre><code class="language-text">Optimization Search:
â”œâ”€â”€ Heuristic mode (BEAM=0): Hand-coded optimizations
â””â”€â”€ Beam search (BEAMâ‰¥1):
    â”œâ”€â”€ Generate all possible actions (193 combinations)
    â”œâ”€â”€ Apply to all top-K candidates in parallel
    â”œâ”€â”€ Filter based on constraints
    â”œâ”€â”€ Compile and run each candidate â†’ Measure actual time
    â””â”€â”€ Pick fastest
</code></pre>
<p><strong>Note</strong>: NOLOCALS is a constraint that sets <code>dont_use_locals = True</code>, preventing further LOCAL actions and affecting shared memory usage decisions.</p>
<p><strong>Morok</strong>: <code>optimizer/mod.rs</code>, <code>optimizer/opts.rs</code></p>
<hr>
<h2 id="phase-2-expander"><a class="header" href="#phase-2-expander">Phase 2: Expander</a></h2>
<p><strong>Goal</strong>: Transform optimization primitives (UNROLL/UPCAST) into explicit operations.</p>
<hr>
<h3 id="stage-8-post-opt-symbolic"><a class="header" href="#stage-8-post-opt-symbolic">Stage 8: Post-Opt Symbolic</a></h3>
<blockquote>
<p><strong>Stage at a Glance</strong></p>
<p><strong>Goal</strong>: Symbolic simplification after optimization
<strong>Key Patterns</strong>: WHERE movement, constant folding
<strong>Impact</strong>: Enables better load combining and vectorization</p>
</blockquote>
<p><strong>What This Does</strong>: Symbolic simplification after optimization, plus WHERE movement.</p>
<p><strong>Why This Matters</strong>: WHERE operations are like <code>if</code> statements. This stage moves <code>if</code> checks from after a load to before the load. Hardware can skip loading when the condition is false, saving memory bandwidth.</p>
<p><strong>Pattern</strong>: <code>sym + pm_move_where_on_load</code></p>
<pre><code class="language-text">// Before: WHERE guards a load
WHERE(valid, LOAD(index), alt)

// After: validity moved to INDEX
LOAD(INDEX(ptr, idx, valid=valid), alt)
</code></pre>
<p>Moving validity into INDEX enables better load combining and vectorization.</p>
<p><strong>Note</strong>: This pattern only matches when the alternative value is <code>0</code>. The transformation involves complex clause analysis: duplicate detection, range dependency checks, and data-dependent load verification.</p>
<p><strong>Note</strong>: The Morok implementation uses <code>gate=</code> instead of <code>valid=</code> (the Index struct has a <code>gate</code> field). The concept is identical.</p>
<p><strong>Morok</strong>: <code>pm_move_where_on_load()</code> in <code>symbolic/patterns.rs</code></p>
<hr>
<h3 id="stage-9-expander"><a class="header" href="#stage-9-expander">Stage 9: Expander</a></h3>
<blockquote>
<p><strong>Stage at a Glance</strong></p>
<p><strong>Goal</strong>: Convert UNROLL/UPCAST to explicit operations
<strong>Key Concepts</strong>: UNROLL, CONTRACT, pattern order
<strong>Impact</strong>: Makes vectorization explicit and ready for hardware</p>
</blockquote>
<p><strong>What This Does</strong>: Transforms UNROLL/UPCAST optimization primitives into explicit operations.</p>
<p><strong>Why This Matters</strong>: UPCAST and UNROLL mark intentâ€”what we want to do. This stage makes that intent explicit so the hardware can actually do it.</p>
<p><strong>Pattern</strong>: <code>sym + pm_pre_expander + pm_group_for_reduce + expander</code></p>
<p>âš ï¸ <strong>Important: Pattern Precedence</strong></p>
<p>The patterns are combined and run to fixpoint. The order affects which pattern is tried first when multiple could match:</p>
<ol>
<li><code>sym</code> first (symbolic simplification)</li>
<li><code>pm_pre_expander</code> second (converts UPCAST/UNROLL ranges)</li>
<li><code>pm_group_for_reduce</code> third (handles GROUP_REDUCE axis)</li>
<li><code>expander</code> last (main expansion)</li>
</ol>
<p>Wrong precedence can cause incorrect vectorization or reduction scoping.</p>
<p><strong>UNROLL and CONTRACT</strong>:</p>
<p>UNROLL and CONTRACT work together:</p>
<pre><code class="language-text">UNROLL: "Take this one thing and make N copies for different positions"
Example:  x â†’ [x_0, x_1, x_2, x_3]

CONTRACT: "Take these N things and combine them back"
Example:  [a, b, c, d] â†’ one vector containing all four
</code></pre>
<p>Together: UPCAST marks intent to vectorize â†’ UNROLL expands â†’ CONTRACT combines.</p>
<p><strong>UPCAST range â†’ VECTORIZE</strong>:</p>
<pre><code class="language-text">// Before: UPCAST marks vectorization intent
RANGE(end=4, UPCAST)
      â†“ [pm_pre_expander]
// Step 1: Convert to UNROLL with constant indices
UNROLL(VCONST([0, 1, 2, 3]))
      â†“ [expander]
// Step 2: Expand operations with UNROLL sources
// Operations now have unrolled sources
      â†“ [CONTRACT or implicit]
// After: explicit VECTORIZE
VECTORIZE(op[0], op[1], op[2], op[3])
</code></pre>
<p><strong>UNROLL range â†’ repeated operations</strong>:</p>
<p>When we say â€œoperations duplicated,â€ it sounds like copy-paste. But thatâ€™s not what happens. The compiler creates a single SIMD instruction that processes all N elements together. Think of a SIMD register as a box holding 4 numbers; adding two boxes adds all 8 numbers at once.</p>
<pre><code class="language-text">// Before: UPCAST marks vectorization intent
RANGE(end=3, UPCAST)
      â†“ [pm_pre_expander]
// Step 1: Convert to UNROLL
UNROLL(VCONST([0, 1, 2]))
      â†“ [expander]
// Step 2: Operations expand to handle all positions
// After: operations processed together (not duplicated)
UNROLL([op_at_0, op_at_1, op_at_2])
</code></pre>
<p><strong>UNROLL/END/CONTRACT interaction</strong>:</p>
<pre><code class="language-text">Before: END(STORE(...), [RANGE(UPCAST)])
             â†“ [pm_pre_expander]
Step 1: END(STORE(...), [UNROLL(VCONST([0,1,2,3]))])
             â†“ [expander]
Step 2: END(CONTRACT(STORE(...Ã—4)), [])
</code></pre>
<p><strong>Broadcast through AFTER/END</strong>:</p>
<pre><code class="language-text">// Broadcast VECTORIZE (all elements identical)
AFTER(VECTORIZE([x, x, x, x]), deps) â†’ VECTORIZE([AFTER(x, deps), AFTER(x, deps), ...])
</code></pre>
<p><strong>GROUP_REDUCE Handling</strong> (<code>pm_group_for_reduce</code>):</p>
<p>GROUP_REDUCE is a special axis type for tensor core reductions:</p>
<pre><code class="language-text">// Before: REDUCE with GROUP_REDUCE ranges
REDUCE(src, [range(GROUP_REDUCE)])
           â†“ [pm_group_for_reduce]
// After: Shared memory reduction pattern
1. Track upstream LOCAL ranges
2. BUFFERIZE result with group ranges (AddrSpace.LOCAL)
3. INDEX into buffer with transformed ranges
4. Final REDUCE with axes (range_id+100, AxisType.REDUCE)
</code></pre>
<p>This enables efficient tensor core accumulation via shared memory.</p>
<p><strong>Morok</strong>: <code>expand.rs</code></p>
<hr>
<h3 id="stage-10-add-local-buffers"><a class="header" href="#stage-10-add-local-buffers">Stage 10: Add Local Buffers</a></h3>
<blockquote>
<p><strong>Stage at a Glance</strong></p>
<p><strong>Goal</strong>: Prepare buffers for fast memory (shared / L1)
<strong>Key Patterns</strong>: Bufferize with locals, extract hints
<strong>Impact</strong>: Frequently-accessed data stays in fast memory</p>
</blockquote>
<p><strong>What This Does</strong>: Prepares buffers for local memory usage and applies codegen-specific cleanups.</p>
<p><strong>Why This Matters</strong>: <strong>Local buffers</strong> = fast memory close to the compute unit:</p>
<ul>
<li>GPU: Shared memory (LDS) â€” 100x faster than global memory</li>
<li>CPU: L1 cache â€” 10x faster than main memory</li>
</ul>
<p>The compiler moves frequently-accessed data to local buffers, similar to keeping important files on your desktop instead of a network drive.</p>
<p><strong>Pattern</strong>: <code>pm_add_buffers_local + rangeify_codegen</code></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Transform</th><th>Purpose</th></tr>
</thead>
<tbody>
<tr><td><code>bufferize_to_store</code></td><td>Convert BUFFERIZE with <code>allow_locals=true</code></td></tr>
<tr><td><code>get_contiguous</code></td><td>Extract optimization hints from CONTIGUOUS</td></tr>
<tr><td>NOOP removal</td><td>Clean up no-op operations</td></tr>
<tr><td>Strip arg from STORE</td><td>Remove redundant arguments</td></tr>
<tr><td>Fix broadcast dtype</td><td>Ensure consistent types in broadcasts</td></tr>
</tbody>
</table>
</div>
<p><strong>Morok</strong>: <code>rangeify/kernel.rs</code></p>
<hr>
<h2 id="phase-3-devectorizer"><a class="header" href="#phase-3-devectorizer">Phase 3: Devectorizer</a></h2>
<p><strong>Goal</strong>: Lower from hardware-agnostic vectors to hardware-specific instructions.</p>
<hr>
<h3 id="stage-11-remove-reduce"><a class="header" href="#stage-11-remove-reduce">Stage 11: Remove Reduce</a></h3>
<blockquote>
<p><strong>Stage at a Glance</strong></p>
<p><strong>Goal</strong>: Convert declarative REDUCE to imperative accumulation
<strong>Key Patterns</strong>: Reduce to accumulator, horizontal reduction
<strong>Impact</strong>: Maps to hardware reduction instructions</p>
</blockquote>
<p><strong>What This Does</strong>: Converts high-level REDUCE to accumulator pattern.</p>
<p><strong>Why This Matters</strong>: A declarative â€œsum these valuesâ€ needs to become imperative instructions: initialize accumulator, loop, add each value.</p>
<p><strong>Pattern</strong>: <code>pm_reduce + gep_pushing</code></p>
<pre><code class="language-text">// Before: declarative reduction
REDUCE(Add, values, range)

// After: imperative accumulation
acc = DEFINE_REG(0.0)
for i in range:
    acc = ADD(acc, values[i])
</code></pre>
<p><strong>Horizontal reduction</strong>:</p>
<p>Before we loop through a reduction dimension, we first combine neighboring values. This creates larger reductions that map better to hardware instructions.</p>
<pre><code class="language-text">Before:  [a, b, c, d, e, f, g, h]  // 8 values
             â†“ [Horizontal reduction]
Step 1:  [a+e, b+f, c+g, d+h]      // 4 partial sums
             â†“ [Accumulator pattern]
After:   acc = acc + (a+e) + (b+f) + (c+g) + (d+h)
</code></pre>
<p><strong>GEP pushing</strong> pushes GEP (get element pointer) operations through ALUs for better vectorization:</p>
<pre><code class="language-text">GEP(ADD(ptr_a, ptr_b), idx) â†’ ADD(GEP(ptr_a, idx), GEP(ptr_b, idx))
</code></pre>
<p><em>Why?</em> Enables SIMD on the two GEPs (can be computed in parallel).</p>
<p><strong>WMMA Tensor Core Fusion</strong>:</p>
<pre><code class="language-text">// Fuse tensor core accumulation inline
WMMA(a, b, c) + add â†’ WMMA(a, b, c + add)
</code></pre>
<p>This pattern enables efficient FMA-style accumulation on NVIDIA tensor cores.</p>
<p><strong>Morok</strong>: <code>devectorize.rs</code></p>
<hr>
<h3 id="stage-12-add-gpu-dims"><a class="header" href="#stage-12-add-gpu-dims">Stage 12: Add GPU Dims</a></h3>
<blockquote>
<p><strong>Stage at a Glance</strong></p>
<p><strong>Goal</strong>: Map abstract ranges to GPU thread indices
<strong>Key Patterns</strong>: Range to SPECIAL replacement
<strong>Impact</strong>: Enables parallel execution on GPU</p>
</blockquote>
<p><strong>What This Does</strong>: Replaces ranges with GPU thread indices.</p>
<p><strong>Why This Matters</strong>: GPUs have hard limits: max 1024 threads per block, max 48KB shared memory. If your computation needs 2000 threads, the compiler must split it into multiple blocks. Dimension limiting handles this automatically.</p>
<p><strong>Pattern</strong>: <code>pm_add_gpudims</code></p>
<pre><code class="language-text">// Before: abstract range
RANGE(end=256, Global)

// After: GPU-specific
SPECIAL(gidx0)  // global thread index
</code></pre>
<p><strong>Mapping</strong>:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Range Type</th><th>GPU Equivalent</th></tr>
</thead>
<tbody>
<tr><td>Global, THREAD</td><td><code>gidx</code> (global index)</td></tr>
<tr><td>Local, WARP, GROUP_REDUCE</td><td><code>lidx</code> (local/workgroup index)</td></tr>
<tr><td>Reduce</td><td>Loop (no mapping)</td></tr>
</tbody>
</table>
</div>
<p><strong>Dimension Limiting</strong>:</p>
<p>GPUs have hardware limits (e.g., max 1024 threads per block). When ranges exceed these limits, the compiler:</p>
<ol>
<li><strong>Groups</strong> adjacent dimensions: <code>[256, 256, 256]</code> with max <code>[256, 256]</code> â†’ <code>[65536, 256]</code></li>
<li><strong>Splits</strong> large dimensions: <code>[2048]</code> with max <code>[1024]</code> â†’ <code>[2, 1024]</code></li>
<li><strong>Reconstructs</strong> indices via divmod</li>
</ol>
<p><strong>Store Masking</strong>:</p>
<p>Global stores that donâ€™t use all local dimensions are masked:</p>
<pre><code class="language-text">// If STORE doesn't use lidx1, mask it:
STORE(INDEX(...), value) â†’ STORE(INDEX(..., gate=(lidx1 == 0)), value)
</code></pre>
<p>This ensures stores only execute when unused local indices are 0.</p>
<p><strong>Morok</strong>: <code>gpudims.rs</code></p>
<hr>
<h3 id="stage-13-add-loads"><a class="header" href="#stage-13-add-loads">Stage 13: Add Loads</a></h3>
<blockquote>
<p><strong>Stage at a Glance</strong></p>
<p><strong>Goal</strong>: Wrap INDEX operations in explicit LOAD
<strong>Key Patterns</strong>: Add LOAD, remove redundant loads
<strong>Impact</strong>: Makes memory operations explicit for codegen</p>
</blockquote>
<p><strong>What This Does</strong>: Wraps INDEX operations in explicit LOAD.</p>
<p><strong>Why This Matters</strong>: Index operations compute addresses. LOAD actually reads memory. Making this explicit helps the code generator understand what memory accesses are needed.</p>
<p><strong>Pattern</strong>: <code>pm_add_loads</code></p>
<pre><code class="language-text">// Before: bare index
INDEX(ptr, i)

// After: explicit load
LOAD(INDEX(ptr, i))
</code></pre>
<p>Also removes redundant loads from stores (write-only access).</p>
<p>Note: Not all INDEX operations get wrapped in LOAD. Pointer types (already addresses) and image textures (special hardware) use different access methods.</p>
<p><strong>Morok</strong>: <code>devectorize.rs</code></p>
<hr>
<h3 id="stage-14-devectorize"><a class="header" href="#stage-14-devectorize">Stage 14: Devectorize</a></h3>
<blockquote>
<p><strong>Stage at a Glance</strong></p>
<p><strong>Goal</strong>: Convert abstract vectors to match hardware capabilities
<strong>Key Phases</strong>: 4 coordinated passes
<strong>Impact</strong>: Vectors work with actual hardware width</p>
</blockquote>
<p><strong>What This Does</strong>: Handles the transition from abstract vectors to hardware operations.</p>
<p><strong>Why This Matters</strong>: Devectorize uses 4 conceptual phases within a single <code>graph_rewrite</code>:</p>
<ol>
<li><strong>Phase 1</strong>: Create PTRCAT to group consecutive pointer accesses, devectorize ALU/WMMA/buffers, expand vector INDEX â†’ GEP(PTRCAT)</li>
<li><strong>Phase 2</strong>: Move GEP through LOAD/STORE</li>
<li><strong>Phase 3</strong>: Distribute PTRCAT through LOAD/STORE, creating CAT(LOADs), fix image buffers</li>
<li><strong>Phase 4</strong>: Split CAT(LOADs) into smaller chunks matching hardware width</li>
</ol>
<p><strong>PTRCAT Construction</strong>:</p>
<p>PTRCAT groups consecutive pointer accesses:</p>
<ol>
<li>Generate individual indexes for each vector element</li>
<li>Extract (valid, root_src) â†’ [offsets] mapping</li>
<li>Group consecutive offsets by validity and source</li>
<li>Create PTRCAT from grouped pointers</li>
<li>Return with GEP permutation for correct element order</li>
</ol>
<p>This reduces memory bus transactions.</p>
<p><strong>Device-Specific Fold Lengths</strong>:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Device</th><th>Fold Lengths</th><th>Notes</th></tr>
</thead>
<tbody>
<tr><td>DSP</td><td>128, 64, 32, 16, 8, 4</td><td>Large vectors for DSP SIMD</td></tr>
<tr><td>GPU (float4)</td><td>4, 2</td><td>Standard GPU vectorization</td></tr>
<tr><td>GPU (half + ALLOW_HALF8)</td><td>8, 4, 2</td><td>Half precision with env var</td></tr>
<tr><td>GPU (AMX)</td><td>16, 8, 4, 2</td><td>Apple AMX support</td></tr>
<tr><td>Image</td><td>4</td><td>Fixed for image textures</td></tr>
<tr><td>Default</td><td>1</td><td>Scalar fallback</td></tr>
</tbody>
</table>
</div>
<p><strong>Environment Variable</strong>: <code>DEVECTORIZE</code></p>
<ul>
<li><code>0</code>: Skip <code>devectorize</code> only (keeps <code>correct_load_store</code>)</li>
<li><code>1</code>: Full devectorization (default)</li>
<li><code>â‰¥2</code>: Skip both <code>devectorize</code> and <code>correct_load_store</code></li>
</ul>
<p><strong>Pattern</strong>: <code>devectorize + load_store_folding + correct_load_store + load_store_indexing</code></p>
<p><strong>Split vectorized ALUs</strong>:</p>
<pre><code class="language-text">// If hardware doesn't support vec4 add
ADD(vec4_a, vec4_b) â†’ [ADD(a[0], b[0]), ADD(a[1], b[1]), ...]
</code></pre>
<p><strong>Load/store chunk splitting</strong>: Match hardware memory width.</p>
<p><strong>Image fixup</strong>: Special handling for image tensor buffers.</p>
<p><strong>Morok</strong>: <code>devectorize.rs</code></p>
<hr>
<h3 id="stage-15-lower-index-dtype"><a class="header" href="#stage-15-lower-index-dtype">Stage 15: Lower Index Dtype</a></h3>
<blockquote>
<p><strong>Stage at a Glance</strong></p>
<p><strong>Goal</strong>: Convert abstract Index type to concrete integers
<strong>Key Patterns</strong>: Operation-specific lowering based on value bounds
<strong>Impact</strong>: Indices use hardware-native integer types (i32 or i64)</p>
</blockquote>
<p><strong>What This Does</strong>: Converts abstract <code>Index</code> type to concrete integers.</p>
<p><strong>Why This Matters</strong>: The Index type is abstractâ€”hardware doesnâ€™t have it. We need to convert to i32 or i64, which the hardware actually supports.</p>
<p><strong>Pattern</strong>: <code>pm_lower_index_dtype</code></p>
<pre><code class="language-text">// Before: abstract index type
idx: Index

// After: concrete type
idx: i32  // or i64, based on bounds
</code></pre>
<p><strong>Operation-Specific Lowering</strong>:</p>
<p>Index type lowering is NOT a single castâ€”each operation type has specific patterns:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Operation</th><th>Before</th><th>After</th></tr>
</thead>
<tbody>
<tr><td>Binary ops</td><td><code>ADD(Index, Index)</code></td><td><code>ADD(i32, i32)</code> with casts</td></tr>
<tr><td>CONST</td><td><code>CONST(5): Index</code></td><td><code>CONST(5): i32</code></td></tr>
<tr><td>WHERE</td><td><code>WHERE(c, Index, Index)</code></td><td><code>WHERE(c, i32, i32)</code></td></tr>
<tr><td>RANGE</td><td><code>RANGE(end: Index)</code></td><td><code>RANGE(end: i32)</code> with cast</td></tr>
<tr><td>SPECIAL</td><td><code>SPECIAL(gidx)</code></td><td>Always i32 (GPU indices are 32-bit)</td></tr>
<tr><td>DEFINE_VAR</td><td><code>DEFINE_VAR: Index</code></td><td>i32 if bounds fit, else i64</td></tr>
<tr><td>VECTORIZE</td><td><code>VECTORIZE(Index...)</code></td><td>Cast each to concrete scalar</td></tr>
<tr><td>CAST cleanup</td><td><code>CAST(i32, Index)</code></td><td>Just <code>i32</code> (remove redundant cast)</td></tr>
</tbody>
</table>
</div>
<p>The <code>select_concrete_dtype()</code> function determines i32 vs i64 using vmin/vmax bounds analysis:</p>
<pre><code class="language-text">dtype = i32 if bounds fit in [-2^31, 2^31-1] else i64
</code></pre>
<p><strong>Morok</strong>: <code>symbolic/index_lowering.rs</code></p>
<hr>
<h2 id="phase-4-linearizer"><a class="header" href="#phase-4-linearizer">Phase 4: Linearizer</a></h2>
<p><strong>Goal</strong>: Convert the DAG to a linear instruction sequence.</p>
<hr>
<h3 id="stage-16-post-index-symbolic"><a class="header" href="#stage-16-post-index-symbolic">Stage 16: Post-Index Symbolic</a></h3>
<blockquote>
<p><strong>Stage at a Glance</strong></p>
<p><strong>Goal</strong>: Full symbolic simplification after index lowering
<strong>Key Patterns</strong>: All symbolic rules (140+)
<strong>Impact</strong>: Final cleanup before serialization</p>
</blockquote>
<p><strong>What This Does</strong>: Full symbolic simplification after index lowering.</p>
<p><strong>Why This Matters</strong>: Now that indices are concrete integers (i32/i64), arithmetic can fully simplify. This is the last chance to clean up expressions before linearization.</p>
<p><strong>Pattern</strong>: <code>symbolic</code></p>
<p>Includes GEP pushing patternsâ€”move address calculations through arithmetic:</p>
<pre><code class="language-text">Before:  GEP(ADD(arr_a, arr_b), idx)
              â†“ [Push GEP through ADD]
After:   ADD(GEP(arr_a, idx), GEP(arr_b, idx))
</code></pre>
<p><em>Why?</em> Enables parallel computation of GEPs and may enable downstream vectorization. (Note: The pattern only applies when GEPâ€™s dtype and ALUâ€™s dtype are NOT pointers.)</p>
<hr>
<h3 id="stage-17-pre-matcher-optional"><a class="header" href="#stage-17-pre-matcher-optional">Stage 17: Pre-Matcher (Optional)</a></h3>
<blockquote>
<p><strong>Stage at a Glance</strong></p>
<p><strong>Goal</strong>: Backend-specific patterns before decomposition
<strong>Key Patterns</strong>: Renderer-specific
<strong>Impact</strong>: Hardware-specific optimizations</p>
</blockquote>
<p><strong>What This Does</strong>: Renderer-specific patterns applied before decomposition.</p>
<p><strong>Why This Matters</strong>: Each backend can add its own patterns. For example, DSP backends use this to replace generic patterns with DSP-specific SIMD intrinsics. This allows hardware-specific optimizations without changing the generic pipeline.</p>
<p><strong>Pattern</strong>: <code>renderer.pre_matcher</code></p>
<p>Most backends (CPU, GPU) donâ€™t need this. Only specialized hardware uses it.</p>
<p><strong>Note</strong>: Morok does not currently implement this stage. The <code>Renderer</code> trait has only a <code>decompositor()</code> method. This is a future enhancement for DSP and other specialized backends.</p>
<hr>
<h3 id="stage-18-decompositions"><a class="header" href="#stage-18-decompositions">Stage 18: Decompositions</a></h3>
<blockquote>
<p><strong>Stage at a Glance</strong></p>
<p><strong>Goal</strong>: Rewrite operations the target doesnâ€™t support
<strong>Key Patterns</strong>: Power-of-2, transcendental approximations
<strong>Impact</strong>: Maps high-level ops to hardware instructions</p>
</blockquote>
<p><strong>What This Does</strong>: Late rewrites for operations the target doesnâ€™t support.</p>
<p><strong>Why This Matters</strong>: Hardware doesnâ€™t have every operation. For example, most CPUs donâ€™t have a direct <code>sin</code> instruction. We approximate it with operations that do exist (addition, multiplication, etc.).</p>
<p><strong>Pattern</strong>: <code>symbolic_simple + get_late_rewrite_patterns</code></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Pattern</th><th>Example</th><th>When Used</th></tr>
</thead>
<tbody>
<tr><td><code>MOD â†’ AND</code></td><td><code>x % 8 â†’ x &amp; 7</code></td><td>Power-of-2 divisor</td></tr>
<tr><td><code>MUL â†’ SHL</code></td><td><code>x * 16 â†’ x &lt;&lt; 4</code></td><td>Power-of-2 multiplier</td></tr>
<tr><td><code>DIV â†’ SHR</code></td><td><code>x // 8 â†’ x &gt;&gt; 3</code></td><td>Power-of-2 divisor</td></tr>
<tr><td><code>FDIV â†’ MUL</code></td><td><code>x / 2.0 â†’ x * 0.5</code></td><td>Float constant divisor</td></tr>
<tr><td><code>NEG</code></td><td><code>x * -1 â†’ NEG(x)</code></td><td>When NEG supported</td></tr>
<tr><td><code>MULACC</code></td><td><code>a * b + c â†’ MULACC(a, b, c)</code></td><td>When FMA supported</td></tr>
<tr><td>Fast integer division</td><td><code>x // 7 â†’ (x * M) &gt;&gt; S</code></td><td>Non-power-of-2 divisor</td></tr>
<tr><td>De Morganâ€™s laws</td><td><code>(!x) &amp; (!y) â†’ !(x | y)</code></td><td>Boolean simplification</td></tr>
<tr><td>Comparison negations</td><td><code>!(x &lt; c) â†’ (c-1) &lt; x</code></td><td>Integer comparisons</td></tr>
</tbody>
</table>
</div>
<p>Transcendental function approximations (SIN, EXP, LOG, etc.) are implemented via the <code>decompositor()</code> pathway (see <code>ir/src/decompositions/transcendentals.rs</code>).</p>
<p><strong>Morok</strong>: <code>optimizer/mod.rs</code></p>
<hr>
<h3 id="stage-19-final-rewrite"><a class="header" href="#stage-19-final-rewrite">Stage 19: Final Rewrite</a></h3>
<blockquote>
<p><strong>Stage at a Glance</strong></p>
<p><strong>Goal</strong>: Prepare for linearization
<strong>Key Patterns</strong>: CONST vectorization, GEP resolution, END splitting
<strong>Impact</strong>: Clean representation ready for linearization</p>
</blockquote>
<p><strong>What This Does</strong>: Prepare for linearization.</p>
<p><strong>Why This Matters</strong>: Some patterns are easier to apply after decomposition. This stage does final cleanup before converting to a linear sequence.</p>
<p><strong>Pattern</strong>: <code>pm_decomp + pm_render + extra_matcher + pm_split_ends</code></p>
<p><strong>CONST vectorization</strong>:</p>
<pre><code class="language-text">// Make vector constants explicit
CONST(1.0) used as vec4 â†’ VECTORIZE(1.0, 1.0, 1.0, 1.0)
</code></pre>
<p><strong>CAT to VECTORIZE</strong> (via <code>gep_pushing</code> in <code>symbolic</code>):</p>
<pre><code class="language-text">CAT(a, b, c, d) â†’ VECTORIZE(a, b, c, d)
</code></pre>
<p>CAT cannot be rendered directly; explicit VECTORIZE is required for codegen.</p>
<p><strong>GEP resolution</strong>: Convert remaining GEP operations.</p>
<p><strong>Split multi-range ENDs</strong>:</p>
<pre><code class="language-text">// Before: END closing multiple ranges
END(op, [range_a, range_b])

// After: nested single ENDs
END(END(op, range_a), range_b)
</code></pre>
<p><strong>extra_matcher</strong>: Each backend can add its own final patterns. This allows hardware-specific optimizations without changing the generic pipeline.</p>
<p><strong>Morok</strong>: <code>devectorize.rs</code>, <code>linearize/mod.rs</code>, <code>optimizer/mod.rs</code></p>
<hr>
<h3 id="stage-20-add-control-flow"><a class="header" href="#stage-20-add-control-flow">Stage 20: Add Control Flow</a></h3>
<blockquote>
<p><strong>Stage at a Glance</strong></p>
<p><strong>Goal</strong>: Build control flow graph and add range dependencies
<strong>Key Concept</strong>: Three relationship types (nested, dependent, independent)
<strong>Impact</strong>: Correct instruction ordering</p>
</blockquote>
<p><strong>What This Does</strong>: Builds the control flow graph and adds range dependencies.</p>
<p><strong>Why This Matters</strong>: Operations must execute in a valid order. If a load uses a RANGEâ€™s value, the RANGE must come first. This stage tracks and enforces these dependencies.</p>
<p><strong>Pattern</strong>: <code>pm_add_control_flow</code> (bottom-up)</p>
<pre><code class="language-text">// Analyze which END operations depend on which
END(computation, [RANGE_A]) and END(other_computation, [RANGE_B]) are siblings
â†’ Creates edge: RANGE_B.src += END(computation)

// Add explicit dependency
RANGE_B waits for RANGE_A to complete
</code></pre>
<p><strong>Three relationship types</strong>:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Relationship</th><th>Example</th><th>Meaning</th></tr>
</thead>
<tbody>
<tr><td>Nested</td><td>RANGE_A inside RANGE_B</td><td>A must complete before B starts</td></tr>
<tr><td>Dependent</td><td>LOAD_A uses RANGE_A</td><td>RANGE_A must precede LOAD_A</td></tr>
<tr><td>Independent</td><td>RANGE_X and RANGE_Y donâ€™t interact</td><td>Can run in parallel</td></tr>
</tbody>
</table>
</div>
<p>Bottom-up traversal ensures dependencies flow correctly from leaves to roots.</p>
<p><strong>Morok</strong>: <code>schedule/src/linearize/mod.rs</code></p>
<hr>
<h3 id="stage-21-linearize"><a class="header" href="#stage-21-linearize">Stage 21: Linearize</a></h3>
<blockquote>
<p><strong>Stage at a Glance</strong></p>
<p><strong>Goal</strong>: Convert DAG to linear instruction sequence
<strong>Key Algorithm</strong>: Priority-aware topological sort
<strong>Impact</strong>: Valid execution order</p>
</blockquote>
<p><strong>What This Does</strong>: Converts the DAG to a linear instruction sequence via priority-aware topological sort.</p>
<p><strong>Why This Matters</strong>: The graph structure doesnâ€™t specify execution order. We need to flatten it while respecting dependencies. Priorities ensure sensible ordering (definitions before uses, loads before computation, stores after).</p>
<p><strong>Function</strong>: <code>linearize(sink)</code></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Operation</th><th>Priority</th><th>Why</th></tr>
</thead>
<tbody>
<tr><td>DEFINE_GLOBAL</td><td>-20</td><td>Arguments must be defined first</td></tr>
<tr><td>DEFINE_VAR</td><td>-19</td><td>Variables must be defined first</td></tr>
<tr><td>DEFINE_LOCAL</td><td>-18</td><td>Allocations first</td></tr>
<tr><td>DEFINE_REG</td><td>-17</td><td>Registers first</td></tr>
<tr><td>CONST</td><td>-10</td><td>Constants early for reuse</td></tr>
<tr><td>LOAD</td><td>-1</td><td>Loads before use</td></tr>
<tr><td>END</td><td>-5</td><td>Closes ranges</td></tr>
<tr><td>STORE</td><td>+1</td><td>Stores after computation</td></tr>
<tr><td>RANGE</td><td>+5</td><td>Ranges open before use</td></tr>
</tbody>
</table>
</div>
<p>Lower priority = earlier in sequence. This ensures:</p>
<ul>
<li>Definitions come first</li>
<li>Loads happen before computation</li>
<li>Stores happen last</li>
<li>Ranges open before their contents, close after</li>
</ul>
<p><strong>Run_count ordering</strong>: Operations are sorted primarily by execution frequency (run_count), then by priority. Operations with lower execution frequency (outside inner loops) are scheduled first, while operations in inner loops (higher run_count) are scheduled later. Example: A CONST executed 100 times appears before a CONST executed 1M times.</p>
<p><strong>run_count Calculation</strong>:</p>
<pre><code class="language-text">run_count = prod(int(r.vmax) + 1 for r in u.ranges)
</code></pre>
<p>This computes how many times an operation executes based on its enclosing ranges.</p>
<p><strong>Morok</strong>: <code>schedule/src/linearize/mod.rs</code></p>
<hr>
<h3 id="stage-22-cleanup-ifendif"><a class="header" href="#stage-22-cleanup-ifendif">Stage 22: Cleanup IF/ENDIF</a></h3>
<blockquote>
<p><strong>Stage at a Glance</strong></p>
<p><strong>Goal</strong>: Final cleanup of linear instruction list
<strong>Key Transformation</strong>: Gated INDEX â†’ IF/STORE/ENDIF
<strong>Impact</strong>: Handles hardware without predicated stores</p>
</blockquote>
<p><strong>What This Does</strong>: Final cleanup of the linear instruction list.</p>
<p><strong>Why This Matters</strong>: Some hardware (modern GPUs) supports â€œpredicated storesâ€â€”write to memory only if condition is true. Older hardware doesnâ€™t. For those, we wrap store in an IF statement. This stage ONLY runs when hardware lacks predicated store support.</p>
<p><strong>Pattern</strong>: <code>pm_linearize_cleanups</code> (via <code>line_rewrite</code>, not <code>graph_rewrite</code>)</p>
<pre><code class="language-text">// Gated INDEX in STORE becomes conditional store
STORE(INDEX(ptr, idx, valid=cond), value)
â†’ IF(cond) { STORE(INDEX(ptr, idx), value) } ENDIF
</code></pre>
<p><strong>Note</strong>: This stage uses <code>line_rewrite</code> instead of <code>graph_rewrite</code> because it operates on the already-linearized instruction list rather than a DAG.</p>
<p>At this point, the instruction list is ready for code generation.</p>
<p><strong>Morok</strong>: <code>schedule/src/linearize/mod.rs</code> (predicated stores path)</p>
<hr>
<h2 id="worked-example-tracing-through-all-22-stages"><a class="header" href="#worked-example-tracing-through-all-22-stages">Worked Example: Tracing Through All 22 Stages</a></h2>
<p>Letâ€™s trace <code>c = a + b</code> (where a, b are [100, 100] tensors) through the pipeline.</p>
<h3 id="initial-tensor-graph"><a class="header" href="#initial-tensor-graph">Initial Tensor Graph</a></h3>
<pre><code>[ADD]
â”œâ”€â”€ [BUFFER(a)] : Float32
â””â”€â”€ [BUFFER(b)] : Float32
</code></pre>
<h3 id="after-stage-1-early-movement-ops"><a class="header" href="#after-stage-1-early-movement-ops">After Stage 1: Early Movement Ops</a></h3>
<p>(No changeâ€”no movement ops in this example)</p>
<h3 id="after-stage-2-load-collapse"><a class="header" href="#after-stage-2-load-collapse">After Stage 2: Load Collapse</a></h3>
<p>(No changeâ€”no reductions in this example)</p>
<h3 id="after-stage-3-split-ranges"><a class="header" href="#after-stage-3-split-ranges">After Stage 3: Split Ranges</a></h3>
<p>(No changeâ€”no modulo operations)</p>
<h3 id="after-stage-4-initial-symbolic"><a class="header" href="#after-stage-4-initial-symbolic">After Stage 4: Initial Symbolic</a></h3>
<p>(No changeâ€”no simplification needed)</p>
<h3 id="after-stage-5-simplify-ranges"><a class="header" href="#after-stage-5-simplify-ranges">After Stage 5: Simplify Ranges</a></h3>
<p>(No changeâ€”no adjacent ranges yet)</p>
<h3 id="after-stage-6-split-store"><a class="header" href="#after-stage-6-split-store">After Stage 6: Split Store</a></h3>
<p>(Not applicableâ€”GPU backend)</p>
<h3 id="after-stage-7-apply-opts"><a class="header" href="#after-stage-7-apply-opts">After Stage 7: Apply Opts</a></h3>
<p>Optimization actions applied:</p>
<ul>
<li>UPCAST j dimension by 4 (vectorization)</li>
<li>LOCAL for input buffers (if beneficial)</li>
</ul>
<h3 id="after-stage-8-post-opt-symbolic"><a class="header" href="#after-stage-8-post-opt-symbolic">After Stage 8: Post-Opt Symbolic</a></h3>
<p>No changesâ€”symbolic already clean.</p>
<h3 id="after-stage-9-expander"><a class="header" href="#after-stage-9-expander">After Stage 9: Expander</a></h3>
<p>UPCAST â†’ UNROLL â†’ CONTRACT:</p>
<pre><code>[VECTORIZE]
â”œâ”€â”€ [ADD]
â”‚   â”œâ”€â”€ [LOAD(a)]
â”‚   â”‚   â””â”€â”€ [INDEX]
â”‚   â”‚       â”œâ”€â”€ [BUFFER(a)]
â”‚   â”‚       â”œâ”€â”€ [RANGE(i, Global, 0..100)]
â”‚   â”‚       â””â”€â”€ [UNROLL(VCONST([0,1,2,3]))]  // Converted from RANGE(j, UPCAST)
â”‚   â””â”€â”€ [LOAD(b)]
â”‚       â””â”€â”€ [INDEX]
â”‚           â”œâ”€â”€ [BUFFER(b)]
â”‚           â”œâ”€â”€ [RANGE(i)]  // Same RANGE via hash consing
â”‚           â””â”€â”€ [UNROLL(VCONST([0,1,2,3]))]  // Same UNROLL via hash consing
</code></pre>
<h3 id="after-stage-10-add-local-buffers"><a class="header" href="#after-stage-10-add-local-buffers">After Stage 10: Add Local Buffers</a></h3>
<p>(If LOCAL opt was chosen)</p>
<h3 id="after-stage-11-remove-reduce"><a class="header" href="#after-stage-11-remove-reduce">After Stage 11: Remove Reduce</a></h3>
<p>(No changeâ€”no reductions)</p>
<h3 id="after-stage-12-add-gpu-dims"><a class="header" href="#after-stage-12-add-gpu-dims">After Stage 12: Add GPU Dims</a></h3>
<pre><code>[SPECIAL(gidx0)] : Index  // replaces RANGE(i)
</code></pre>
<h3 id="after-stage-13-add-loads"><a class="header" href="#after-stage-13-add-loads">After Stage 13: Add Loads</a></h3>
<p>(No changeâ€”loads already present)</p>
<h3 id="after-stage-14-devectorize"><a class="header" href="#after-stage-14-devectorize">After Stage 14: Devectorize</a></h3>
<p>Vector split to match hardware width:</p>
<pre><code>[VECTORIZE] : &lt;4 x Float32&gt;
â”œâ”€â”€ [ADD(a[0], b[0])]
â”œâ”€â”€ [ADD(a[1], b[1])]
â”œâ”€â”€ [ADD(a[2], b[2])]
â””â”€â”€ [ADD(a[3], b[3])]
</code></pre>
<h3 id="after-stage-15-lower-index-dtype"><a class="header" href="#after-stage-15-lower-index-dtype">After Stage 15: Lower Index Dtype</a></h3>
<pre><code>[SPECIAL(gidx0)] : i32  // concrete type
</code></pre>
<h3 id="after-stage-16-post-index-symbolic"><a class="header" href="#after-stage-16-post-index-symbolic">After Stage 16: Post-Index Symbolic</a></h3>
<p>No changes needed.</p>
<h3 id="after-stage-17-pre-matcher"><a class="header" href="#after-stage-17-pre-matcher">After Stage 17: Pre-Matcher</a></h3>
<p>(No patterns for standard backends)</p>
<h3 id="after-stage-18-decompositions"><a class="header" href="#after-stage-18-decompositions">After Stage 18: Decompositions</a></h3>
<p>No decompositions neededâ€”all ops supported.</p>
<h3 id="after-stage-19-final-rewrite"><a class="header" href="#after-stage-19-final-rewrite">After Stage 19: Final Rewrite</a></h3>
<p>No changes needed.</p>
<h3 id="after-stage-20-add-control-flow"><a class="header" href="#after-stage-20-add-control-flow">After Stage 20: Add Control Flow</a></h3>
<p>Dependencies trackedâ€”no issues.</p>
<h3 id="after-stage-21-linearize"><a class="header" href="#after-stage-21-linearize">After Stage 21: Linearize</a></h3>
<p>Linear instruction sequence (simplified):</p>
<pre><code>1. DEFINE_GLOBAL(0)  // Output buffer c
2. DEFINE_GLOBAL(1)  // Input buffer a
3. DEFINE_GLOBAL(2)  // Input buffer b
4. RANGE(i, 0..100, Global)  // gidx0
5. RANGE(j, 0..25, Loop)  // Unrolled /4
6. LOAD(a, i, j*4+0)  // Vector load
7. LOAD(b, i, j*4+0)  // Vector load
8. ADD(vec_a, vec_b)  // Vector add
9. STORE(c, i, j*4+0, result)
10. END(RANGE(j))
11. END(RANGE(i))
</code></pre>
<h3 id="after-stage-22-cleanup-ifendif"><a class="header" href="#after-stage-22-cleanup-ifendif">After Stage 22: Cleanup IF/ENDIF</a></h3>
<p>No changes neededâ€”no gated stores.</p>
<p><strong>Result</strong>: Ready for code generation! The LLVM/CUDA/other backend will compile this to actual machine code.</p>
<hr>
<h2 id="pattern-application-strategy"><a class="header" href="#pattern-application-strategy">Pattern Application Strategy</a></h2>
<p>Each stage uses one of two rewrite strategies:</p>
<p><strong>Top-down</strong> (default): Process parents before children. Use when transformations create new matchable subterms.</p>
<p><strong>Bottom-up</strong>: Process children before parents. Use when child state affects parent matching (stages 1, 20).</p>
<p>Both iterate to fixpointâ€”patterns fire until no more match.</p>
<hr>
<h2 id="debugging-the-pipeline"><a class="header" href="#debugging-the-pipeline">Debugging the Pipeline</a></h2>
<p>When a kernel produces wrong results, the bug lives in one of these 22 stages. Use environment variables to extract IR at each stage:</p>
<pre><code class="language-bash"># See IR after each transformation
MOROK_DEBUG=ir cargo test failing_test
</code></pre>
<h3 id="quick-reference"><a class="header" href="#quick-reference">Quick Reference</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Symptom</th><th>Likely Stages</th><th>What to Check</th></tr>
</thead>
<tbody>
<tr><td>Wrong values in output</td><td>4, 9, 11, 18</td><td>Symbolic simplification, expansion, devectorization</td></tr>
<tr><td>Slow performance</td><td>7, 9, 14, 21</td><td>Optimization, expansion, devectorization, linearization</td></tr>
<tr><td>Crashes/panics</td><td>11, 12</td><td>Reduce, GPU dims</td></tr>
<tr><td>Wrong loop count</td><td>3, 5, 12</td><td>Split ranges, simplify ranges, GPU dims</td></tr>
<tr><td>Missing vectorization</td><td>9, 14</td><td>Expander, devectorize</td></tr>
</tbody>
</table>
</div>
<h3 id="common-issues"><a class="header" href="#common-issues">Common Issues</a></h3>
<ol>
<li><strong>Stage 3-4</strong>: Range splitting/symbolic may lose constraints</li>
<li><strong>Stage 9</strong>: Expansion order affects vectorization correctness</li>
<li><strong>Stage 11</strong>: Accumulator initialization must match reduction identity</li>
<li><strong>Stage 14</strong>: Hardware width mismatchâ€”check vector fold length</li>
<li><strong>Stage 18</strong>: Missing decompositionâ€”check supported_ops list for backend</li>
<li><strong>Stage 21</strong>: Priority bugs cause data racesâ€”verify dependencies</li>
</ol>
<hr>
<h2 id="summary-1"><a class="header" href="#summary-1">Summary</a></h2>
<p>The 22-stage pipeline transforms tensor expressions into machine code through systematic refinement:</p>
<ol>
<li><strong>Stages 1-7</strong>: Make iteration explicit, optimize ranges</li>
<li><strong>Stages 8-10</strong>: Expand optimization primitives</li>
<li><strong>Stages 11-15</strong>: Lower to hardware-specific operations</li>
<li><strong>Stages 16-22</strong>: Serialize to executable instructions</li>
</ol>
<p>Each stage has a single responsibility. Each builds on the last. The result: high-level tensor code runs at near-optimal speed on diverse hardware.</p>
<hr>
<h2 id="tinygrad-vs-morok-architectural-differences"><a class="header" href="#tinygrad-vs-morok-architectural-differences">Tinygrad vs Morok: Architectural Differences</a></h2>
<p>This chapter describes the â€œidealâ€ 22-stage pipeline based on Tinygradâ€™s implementation. Morok now closely follows this design with minimal differences.</p>
<h3 id="remaining-architectural-differences"><a class="header" href="#remaining-architectural-differences">Remaining Architectural Differences</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Stage</th><th>Tinygrad</th><th>Morok</th><th>Notes</th></tr>
</thead>
<tbody>
<tr><td>1: Early Movement Ops</td><td>Moves movement ops through AFTER/END wrappers</td><td>Removes movement ops during bufferization</td><td>Both approaches achieve functional equivalence; Morokâ€™s is cleaner</td></tr>
</tbody>
</table>
</div>
<h3 id="aligned-stages-previously-different"><a class="header" href="#aligned-stages-previously-different">Aligned Stages (Previously Different)</a></h3>
<p>The following stages were aligned with Tinygrad as of this implementation:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Stage</th><th>What Changed</th></tr>
</thead>
<tbody>
<tr><td>15: Index Dtype Lowering</td><td>Morok now has <code>pm_lower_index_dtype()</code> with full pattern coverage: Binary ops, CONST, WHERE, VECTORIZE, SPECIAL, DEFINE_VAR, RANGE, CAST cleanup</td></tr>
<tr><td>18: Decompositions</td><td>Added: <code>fast_division_patterns()</code>, <code>pm_div_to_shr()</code>, <code>pm_fdiv_to_mul()</code>, <code>pm_comparison_negations()</code>, De Morganâ€™s laws</td></tr>
<tr><td>19: Final Rewrite</td><td><code>pm_render()</code> moved from codegen to Stage 19 in schedule pipeline</td></tr>
</tbody>
</table>
</div>
<h3 id="tinygrad-only-patterns"><a class="header" href="#tinygrad-only-patterns">Tinygrad-Only Patterns</a></h3>
<p>Morok intentionally does not implement these Tinygrad-specific patterns:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Pattern</th><th>Purpose</th><th>Why Morok Doesnâ€™t Need It</th></tr>
</thead>
<tbody>
<tr><td><code>to_bufferview</code></td><td>Avoid disk buffer copies for DISK/TINYFS devices</td><td>Morok doesnâ€™t support DISK/TINYFS; in-memory backends donâ€™t need this</td></tr>
<tr><td>AFTER/END movement patterns</td><td>Move movement ops through timing wrappers</td><td>Morok removes movement ops during bufferization instead</td></tr>
</tbody>
</table>
</div>
<h3 id="morok-enhancements"><a class="header" href="#morok-enhancements">Morok Enhancements</a></h3>
<p>Morok has some patterns/enhancements not in Tinygrad:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Enhancement</th><th>Location</th><th>Purpose</th></tr>
</thead>
<tbody>
<tr><td>Nested INDEX flattening with identical indices</td><td><code>movement_op_patterns()</code></td><td>Removes redundant <code>INDEX(INDEX(ptr, [i]), [i])</code></td></tr>
<tr><td>CAT â†’ VECTORIZE</td><td><code>pm_render</code></td><td>Converts CAT to explicit VECTORIZE (canâ€™t render CAT directly)</td></tr>
<tr><td>PTRCAT([x]) unwrap</td><td><code>pm_render</code></td><td>Removes single-element PTRCAT wrappers</td></tr>
<tr><td>GEP through CAST/BITCAST</td><td><code>gep_pushing_patterns()</code></td><td>Pushes GEP through type casts for better optimization</td></tr>
<tr><td>Image dtype guard</td><td><code>pm_add_loads()</code></td><td>Skips LOAD wrapping for Image dtype (handled in codegen)</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="glossary"><a class="header" href="#glossary">Glossary</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Term</th><th>Simple Definition</th><th>Example</th></tr>
</thead>
<tbody>
<tr><td><strong>Accumulator</strong></td><td>Variable holding running total</td><td><code>acc = acc + value</code> (in reduction)</td></tr>
<tr><td><strong>Axis</strong></td><td>One dimension of a tensor</td><td>Shape [100, 200] has 2 axes</td></tr>
<tr><td><strong>AxisType</strong></td><td>How a loop executes</td><td>Global=parallel, Reduce=accumulate</td></tr>
<tr><td><strong>Buffer</strong></td><td>Allocated memory holding data</td><td>A tensorâ€™s data lives in a buffer</td></tr>
<tr><td><strong>Bufferize</strong></td><td>Store result in memory instead of computing on-demand</td><td>Materialize intermediate value</td></tr>
<tr><td><strong>CONTRACT</strong></td><td>Combine multiple values into one vector</td><td><code>[a, b, c, d] â†’ vec4(a,b,c,d)</code></td></tr>
<tr><td><strong>Devectorize</strong></td><td>Split vectors to match hardware</td><td><code>vec8 â†’ vec4, vec4</code></td></tr>
<tr><td><strong>Divmod</strong></td><td>Division and remainder operations</td><td><code>x // 7, x % 7</code></td></tr>
<tr><td><strong>Fixpoint</strong></td><td>When applying patterns no longer changes anything</td><td>Patterns fire until fixpoint</td></tr>
<tr><td><strong>GEP</strong></td><td>Get Element Pointerâ€”compute address from indices</td><td><code>arr[i][j] â†’ base + i*stride + j</code></td></tr>
<tr><td><strong>Hash consing</strong></td><td>Reuse identical expressions</td><td><code>ADD(x, 0) + ADD(x, 0)</code> shares memory</td></tr>
<tr><td><strong>Index</strong></td><td>Integer type for array indices</td><td>i32 or i64, depending on device</td></tr>
<tr><td><strong>Load</strong></td><td>Read from memory</td><td><code>value = arr[i]</code></td></tr>
<tr><td><strong>Pattern</strong></td><td>Find-and-replace rule for code</td><td><code>ADD(x, 0) â†’ x</code></td></tr>
<tr><td><strong>Predicated store</strong></td><td>Write to memory conditionally</td><td>Write if valid else skip</td></tr>
<tr><td><strong>Range</strong></td><td>Loop iteration specification</td><td><code>for i in 0..100</code></td></tr>
<tr><td><strong>Reduction</strong></td><td>Combine many values into one</td><td>Sum, max, min</td></tr>
<tr><td><strong>Store</strong></td><td>Write to memory</td><td><code>arr[i] = value</code></td></tr>
<tr><td><strong>Symbolic</strong></td><td>Simplify using algebra rules</td><td><code>(x/4)*4 â†’ x</code> (when <code>x%4=0</code>)</td></tr>
<tr><td><strong>Tensor core</strong></td><td>Hardware for fast matrix multiply</td><td>NVIDIA GPUs only</td></tr>
<tr><td><strong>Topological sort</strong></td><td>Order nodes respecting dependencies</td><td>A before B if B uses Aâ€™s result</td></tr>
<tr><td><strong>UNROLL</strong></td><td>Expand one op into multiple positions</td><td><code>x â†’ [x_0, x_1, x_2, x_3]</code></td></tr>
<tr><td><strong>UPCAST</strong></td><td>Mark intent to vectorize</td><td><code>RANGE(0..4, UPCAST)</code></td></tr>
<tr><td><strong>Vectorize</strong></td><td>Process multiple values together</td><td>SIMD: add 4 numbers at once</td></tr>
<tr><td><strong>WHERE</strong></td><td>Conditional selection</td><td><code>WHERE(cond, x, y) = x if cond else y</code></td></tr>
</tbody>
</table>
</div>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="one-ir-to-rule-them-all"><a class="header" href="#one-ir-to-rule-them-all">One IR to Rule Them All</a></h1>
<p>Youâ€™re debugging a slow model. The profiler says â€œkernel X takes 200msâ€ but you have no idea what kernel X actually <em>does</em>. You trace through PyTorchâ€™s dispatcher, then ATen, then TorchInductor, then Triton IR, and finally land in LLVM IR. Five different representations, five different mental models, five different debugging tools.</p>
<p>This is the reality of modern ML compilation. TensorFlowâ€™s XLA has a similar story: Python â†’ Graph â†’ XLA HLO â†’ MLIR â†’ LLVM IR. Each layer was added to solve a real problem, but the accumulated complexity is staggering.</p>
<p>Morok takes a different approach, borrowed from <a href="https://github.com/tinygrad/tinygrad">Tinygrad</a>: <strong>one IR from tensors to machine code</strong>.</p>
<pre><code class="language-text">â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    TensorFlow    â”‚   â”‚     PyTorch     â”‚   â”‚     Morok     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Python API     â”‚   â”‚   Python API    â”‚   â”‚  Rust/Python  â”‚
â”‚   TF Graph       â”‚   â”‚   FX Graph      â”‚   â”‚       â†“       â”‚
â”‚   XLA HLO        â”‚   â”‚   Inductor IR   â”‚   â”‚    UOp IR     â”‚
â”‚   MLIR dialects  â”‚   â”‚   Triton IR     â”‚   â”‚       â†“       â”‚
â”‚   LLVM IR        â”‚   â”‚   LLVM/PTX      â”‚   â”‚  Machine code â”‚
â”‚   Machine code   â”‚   â”‚   Machine code  â”‚   â”‚               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚      5 IRs       â”‚   â”‚      4 IRs      â”‚   â”‚     1 IR      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<p>The simplest architecture often wins. This chapter explains how one carefully designed IR can replace an entire compiler stack.</p>
<hr>
<h2 id="uop-the-universal-node"><a class="header" href="#uop-the-universal-node">UOp: The Universal Node</a></h2>
<p>A <strong>UOp</strong> (micro-operation) is a node in a computation graph. But unlike nodes in other IRs, a UOp can represent operations at <em>any</em> abstraction levelâ€”from high-level tensor reshapes down to individual CPU instructions.</p>
<p>Hereâ€™s the key insight: instead of having separate IRs for â€œtensor operationsâ€ and â€œloop structuresâ€ and â€œmemory accessesâ€, we put them all in one enum:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub enum Op {
    // High-level tensor operations
    Reshape { src: Arc&lt;UOp&gt;, new_shape: Arc&lt;UOp&gt; },
    Permute { src: Arc&lt;UOp&gt;, axes: Vec&lt;usize&gt; },
    ReduceAxis { src: Arc&lt;UOp&gt;, reduce_op: ReduceOp, axes: Vec&lt;usize&gt; },

    // Loop-level control flow
    Range { end: Arc&lt;UOp&gt;, axis_id: AxisId, axis_type: AxisType },
    End { computation: Arc&lt;UOp&gt;, ranges: SmallVec&lt;[Arc&lt;UOp&gt;; 4]&gt; },

    // Memory operations
    Load { buffer: Arc&lt;UOp&gt;, index: Arc&lt;UOp&gt; },
    Store { buffer: Arc&lt;UOp&gt;, index: Arc&lt;UOp&gt;, value: Arc&lt;UOp&gt;, ... },

    // ALU operations (same as hardware)
    Binary(BinaryOp, Arc&lt;UOp&gt;, Arc&lt;UOp&gt;),  // Add, Mul, etc.
    Unary(UnaryOp, Arc&lt;UOp&gt;),              // Sqrt, Exp, etc.
}
<span class="boring">}</span></code></pre>
<p>The enum has ~80 variants organized by abstraction level:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Category</th><th>Examples</th><th>What It Represents</th></tr>
</thead>
<tbody>
<tr><td><strong>Movement</strong></td><td><code>RESHAPE</code>, <code>PERMUTE</code>, <code>EXPAND</code>, <code>PAD</code></td><td>Tensor shape transformations</td></tr>
<tr><td><strong>Reduction</strong></td><td><code>REDUCE_AXIS</code>, <code>REDUCE</code></td><td>Mathematical aggregations</td></tr>
<tr><td><strong>Control</strong></td><td><code>RANGE</code>, <code>END</code>, <code>IF</code>, <code>BARRIER</code></td><td>Loop and branch structure</td></tr>
<tr><td><strong>Memory</strong></td><td><code>LOAD</code>, <code>STORE</code>, <code>INDEX</code>, <code>BUFFER</code></td><td>Hardware memory access</td></tr>
<tr><td><strong>ALU</strong></td><td><code>ADD</code>, <code>MUL</code>, <code>SQRT</code>, <code>EXP</code>, <code>WHERE</code></td><td>CPU/GPU instructions</td></tr>
<tr><td><strong>Advanced</strong></td><td><code>WMMA</code>, <code>CONTRACT</code>, <code>UNROLL</code></td><td>Tensor cores, vectorization</td></tr>
</tbody>
</table>
</div>
<p>When you print a UOp graph, you see its tree structure:</p>
<pre><code class="language-text">[42] STORE : Void
â”œâ”€â”€ [10] DEFINE_GLOBAL(0) : Ptr&lt;Float32&gt;
â”œâ”€â”€ [35] INDEX : Ptr&lt;Float32&gt;
â”‚   â”œâ”€â”€ [10] â†’ (same as above)
â”‚   â””â”€â”€ [30] RANGE(axis=0, Reduce) : Index
â”‚       â””â”€â”€ [5] CONST(4) : Index
â””â”€â”€ [40] REDUCE(Add) : Float32
    â”œâ”€â”€ [38] MUL : Float32
    â”‚   â”œâ”€â”€ [36] LOAD : Float32
    â”‚   â””â”€â”€ [37] LOAD : Float32
    â””â”€â”€ [30] â†’ (same RANGE as above)
</code></pre>
<p>Notice the arrows pointing to â€œsame as aboveâ€? Thatâ€™s not just pretty-printingâ€”itâ€™s a fundamental property called <strong>hash consing</strong>.</p>
<hr>
<h2 id="hash-consing-structural-sharing"><a class="header" href="#hash-consing-structural-sharing">Hash Consing: Structural Sharing</a></h2>
<p>When you create the same expression twice in Morok, you get the <em>same pointer</em>. Not equal valuesâ€”the same memory address.</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let a = UOp::binary(Add, x.clone(), y.clone());
let b = UOp::binary(Add, x.clone(), y.clone());

assert!(Arc::ptr_eq(&amp;a, &amp;b));  // Same pointer!
<span class="boring">}</span></code></pre>
<p>This works through a global cache. When constructing a UOp, we first check if an identical one exists:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn new(op: Op, dtype: DType) -&gt; Arc&lt;Self&gt; {
    let key = UOpKey::new(&amp;op, dtype);

    // Check cache first
    if let Some(existing) = CACHE.get(&amp;key) {
        return existing;
    }

    // Create new and cache it
    let uop = Arc::new(UOp { op, dtype, ... });
    CACHE.insert(key, uop.clone());
    uop
}
<span class="boring">}</span></code></pre>
<p>Why does this matter for ML engineers?</p>
<ul>
<li>
<p><strong>Pointer equality is semantic equality.</strong> To check if two subexpressions are identical, just compare pointers: <code>Arc::ptr_eq(&amp;a, &amp;b)</code>. No tree traversal needed.</p>
</li>
<li>
<p><strong>Pattern matching is O(1).</strong> When the optimizer asks â€œhave I seen this pattern before?â€, pointer comparison gives an instant answer.</p>
</li>
<li>
<p><strong>Memory efficiency.</strong> Common subexpressions (think: shared computations in attention, gradient graphs) are stored once, not duplicated.</p>
</li>
<li>
<p><strong>Thread safety.</strong> The same computation from different threads produces the same objectâ€”no synchronization bugs.</p>
</li>
</ul>
<p>The tree printout shows this: when you see <code>[10] â†’ (same as above)</code>, thatâ€™s not a copyâ€”itâ€™s the <em>same node</em> referenced from multiple places.</p>
<hr>
<h2 id="explicit-loops-the-range-operation"><a class="header" href="#explicit-loops-the-range-operation">Explicit Loops: The <code>RANGE</code> Operation</a></h2>
<p>Most ML IRs hide loops inside operations. In ONNX, a reduction looks like:</p>
<pre><code class="language-python">ReduceSum(data, axes=[1], keepdims=0)
</code></pre>
<p>Whereâ€™s the loop? Itâ€™s implicitâ€”somewhere inside the runtimeâ€™s implementation of <code>ReduceSum</code>. You canâ€™t see it, canâ€™t modify it, canâ€™t reason about it.</p>
<p>Morok makes loops <em>explicit</em> using <code>RANGE</code> operations. The same reduction becomes:</p>
<pre><code class="language-text">[REDUCE(Add)]
â”œâ”€â”€ [LOAD]
â”‚   â””â”€â”€ [INDEX]
â”‚       â”œâ”€â”€ [BUFFER]
â”‚       â”œâ”€â”€ [RANGE(axis=0, Global)]   # outer loop (parallelized)
â”‚       â”‚   â””â”€â”€ [CONST(128)]
â”‚       â””â”€â”€ [RANGE(axis=1, Reduce)]   # reduction loop
â”‚           â””â”€â”€ [CONST(64)]
â””â”€â”€ [RANGE(axis=1, Reduce)]           # same RANGE via hash consing
</code></pre>
<p>Each <code>RANGE</code> has an <strong>AxisType</strong> that tells the code generator how to compile it:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>AxisType</th><th>CPU</th><th>CUDA</th><th>Meaning</th></tr>
</thead>
<tbody>
<tr><td><strong>Global</strong></td><td>Thread pool</td><td><code>blockIdx</code></td><td>Outer parallel dimension</td></tr>
<tr><td><strong>Local</strong></td><td>(N/A)</td><td><code>threadIdx</code></td><td>Workgroup parallelism</td></tr>
<tr><td><strong>Loop</strong></td><td><code>for</code> loop</td><td><code>for</code> loop</td><td>Sequential iteration</td></tr>
<tr><td><strong>Reduce</strong></td><td>Accumulator</td><td>Warp reduce</td><td>Reduction dimension</td></tr>
<tr><td><strong>Upcast</strong></td><td>SIMD vector</td><td>Register tile</td><td>Vectorization</td></tr>
<tr><td><strong>Unroll</strong></td><td>Unrolled</td><td>Unrolled</td><td>Loop unrolling</td></tr>
</tbody>
</table>
</div>
<p>The AxisType hierarchy (Global â†’ Local â†’ Loop â†’ Reduce â†’ Upcast â†’ Unroll) maps directly to GPU programming models. A <code>RANGE</code> with <code>AxisType::Global</code> becomes <code>blockIdx.x</code> in CUDA. A <code>RANGE</code> with <code>AxisType::Local</code> becomes <code>threadIdx.x</code>.</p>
<p>Why explicit loops matter:</p>
<ul>
<li>
<p><strong>Optimization is visible.</strong> You can <em>see</em> which loops will be parallelized, which will be unrolled, which will use SIMD.</p>
</li>
<li>
<p><strong>Scheduling is graph rewriting.</strong> Changing loop order, tiling, or unrolling is just a pattern transformationâ€”no special â€œscheduling passâ€.</p>
</li>
<li>
<p><strong>Same IR at every stage.</strong> The <code>RANGE</code> that represents â€œiterate over batch dimensionâ€ at the tensor level is the <em>same</em> <code>RANGE</code> that becomes <code>for (int i = 0; i &lt; N; i++)</code> in generated code.</p>
</li>
</ul>
<hr>
<h2 id="graph-rewriting-one-transformation-mechanism"><a class="header" href="#graph-rewriting-one-transformation-mechanism">Graph Rewriting: One Transformation Mechanism</a></h2>
<p>Traditional compilers have dozens of specialized passes: constant folding, dead code elimination, loop unrolling, operator fusion. Each pass has custom logic, custom data structures, custom bugs.</p>
<p>Morok uses one mechanism: <strong>pattern-based graph rewriting</strong>.</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>patterns! {
    // Identity folding: x + 0 â†’ x
    Add[x, @zero] ~&gt; x,

    // Constant folding: 3 + 4 â†’ 7
    Add(a @const(a_val), b @const(b_val))
        =&gt; eval_add(a_val, b_val).map(|r| UOp::const_(a.dtype(), r)),

    // Self-folding: x / x â†’ 1
    Idiv(x, x) ~&gt; UOp::one(x.dtype()),

    // Dead code: if(true) { x } else { y } â†’ x
    Where(@true, t, _f) ~&gt; t,
}
<span class="boring">}</span></code></pre>
<p>The DSL is expressive:</p>
<ul>
<li><strong><code>[x, y]</code> â€” commutative.</strong> Try both orderings (for <code>ADD</code>, <code>MUL</code>, etc.)</li>
<li><strong><code>(x, y)</code> â€” ordered.</strong> Match exactly this order.</li>
<li><strong><code>@zero</code>, <code>@one</code>, <code>@true</code> â€” semantic constants.</strong> Works for any dtype.</li>
<li><strong><code>@const(val)</code> â€” extract value.</strong> For compile-time computation.</li>
<li><strong><code>x, x</code> â€” same operand.</strong> Detects pointer equality.</li>
<li><strong><code>~&gt;</code> vs <code>=&gt;</code></strong> â€” infallible vs fallible rewrite.</li>
</ul>
<p>The rewrite engine applies patterns bottom-up until no more matches:</p>
<pre><code class="language-text">Original:       Add(Mul(x, 1), 0)
After Mul:      Add(x, 0)         # Mul(x, 1) â†’ x
After Add:      x                 # Add(x, 0) â†’ x
</code></pre>
<p>This single mechanism handles:</p>
<ul>
<li><strong>Algebraic simplification</strong> â€” constant folding, identity removal</li>
<li><strong>Rangeify transformation</strong> â€” movement ops â†’ explicit loops</li>
<li><strong>Kernel optimization</strong> â€” vectorization, unrolling, tensor cores</li>
<li><strong>Code generation</strong> â€” lowering to hardware primitives</li>
</ul>
<p>Same patterns, same engine, different pattern sets for each stage.</p>
<hr>
<h2 id="worked-example-matmul-journey"><a class="header" href="#worked-example-matmul-journey">Worked Example: Matmul Journey</a></h2>
<p>Letâ€™s trace <code>C = A @ B</code> (a 4Ã—4 matrix multiply) through the entire pipeline.</p>
<h3 id="stage-1-tensor-construction"><a class="header" href="#stage-1-tensor-construction">Stage 1: Tensor Construction</a></h3>
<p>When you write <code>A.matmul(&amp;B)</code>, Morok builds a high-level UOp graph:</p>
<pre><code class="language-text">[REDUCE_AXIS(Add, axes=[2])]
â”œâ”€â”€ [MUL]
â”‚   â”œâ”€â”€ [EXPAND]           # A: [4,4] â†’ [4,4,4]
â”‚   â”‚   â””â”€â”€ [BUFFER(A)]
â”‚   â””â”€â”€ [EXPAND]           # B: [4,4] â†’ [4,4,4]
â”‚       â””â”€â”€ [PERMUTE]      # transpose for broadcasting
â”‚           â””â”€â”€ [BUFFER(B)]
</code></pre>
<p>This is pure math: â€œexpand A and B to align dimensions, multiply elementwise, sum along the contracted axis.â€</p>
<h3 id="stage-2-rangeify-1"><a class="header" href="#stage-2-rangeify-1">Stage 2: Rangeify</a></h3>
<p>The rangeify pass converts movement ops (<code>EXPAND</code>, <code>PERMUTE</code>) into explicit index computations with <code>RANGE</code> loops:</p>
<pre><code class="language-text">[STORE]
â”œâ”€â”€ [DEFINE_GLOBAL(C)]
â”œâ”€â”€ [INDEX]
â”‚   â”œâ”€â”€ [DEFINE_GLOBAL(C)]
â”‚   â”œâ”€â”€ [RANGE(i, Global)]     # i âˆˆ [0, 4)
â”‚   â”‚   â””â”€â”€ [CONST(4)]
â”‚   â””â”€â”€ [RANGE(j, Global)]     # j âˆˆ [0, 4)
â”‚       â””â”€â”€ [CONST(4)]
â””â”€â”€ [REDUCE(Add)]
    â”œâ”€â”€ [MUL]
    â”‚   â”œâ”€â”€ [LOAD(A)]
    â”‚   â”‚   â””â”€â”€ [INDEX]
    â”‚   â”‚       â”œâ”€â”€ [RANGE(i)]     # same i (hash consing)
    â”‚   â”‚       â””â”€â”€ [RANGE(k, Reduce)]
    â”‚   â””â”€â”€ [LOAD(B)]
    â”‚       â””â”€â”€ [INDEX]
    â”‚           â”œâ”€â”€ [RANGE(k)]     # same k
    â”‚           â””â”€â”€ [RANGE(j)]     # same j
    â””â”€â”€ [RANGE(k, Reduce)]         # k âˆˆ [0, 4)
        â””â”€â”€ [CONST(4)]
</code></pre>
<p>Now we see the loop structure: <code>i</code> and <code>j</code> are <code>Global</code> (parallelized), <code>k</code> is <code>Reduce</code> (accumulated).</p>
<h3 id="stage-3-symbolic-simplification"><a class="header" href="#stage-3-symbolic-simplification">Stage 3: Symbolic Simplification</a></h3>
<p>Pattern rewrites clean up redundant operations, fold constants, and simplify index arithmetic.</p>
<h3 id="stage-4-code-generation"><a class="header" href="#stage-4-code-generation">Stage 4: Code Generation</a></h3>
<p>The final IR translates directly to loops:</p>
<pre><code class="language-c">// GPU kernel (conceptual)
__global__ void matmul(float* C, float* A, float* B) {
    int i = blockIdx.x;   // from RANGE(i, Global)
    int j = blockIdx.y;   // from RANGE(j, Global)
    float acc = 0.0f;
    for (int k = 0; k &lt; 4; k++) {  // from RANGE(k, Reduce)
        acc += A[i*4 + k] * B[k*4 + j];
    }
    C[i*4 + j] = acc;
}
</code></pre>
<p>The key observation: <strong>structure is visible at every stage</strong>. Thereâ€™s no magic fusion pass that turns three nested loops into something unrecognizable. The <code>RANGE</code> structure you see in Stage 2 is exactly what becomes loops in Stage 4.</p>
<hr>
<h2 id="comparison-how-other-irs-differ"><a class="header" href="#comparison-how-other-irs-differ">Comparison: How Other IRs Differ</a></h2>
<p>Different IRs make different tradeoffs. Hereâ€™s how they stack up:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Aspect</th><th>ONNX</th><th>XLA HLO</th><th>Triton</th><th><strong>Morok</strong></th></tr>
</thead>
<tbody>
<tr><td><strong>Purpose</strong></td><td>Model interchange</td><td>Backend optimization</td><td>GPU kernel DSL</td><td>Full compilation</td></tr>
<tr><td><strong>Operators</strong></td><td>~200 high-level</td><td>~100â€“150 high-level</td><td>Tile operations</td><td>~80 multi-level</td></tr>
<tr><td><strong>Loop model</strong></td><td>Implicit</td><td>Implicit</td><td>Tile-based</td><td><strong>Explicit <code>RANGE</code></strong></td></tr>
<tr><td><strong>Memory</strong></td><td>Pure values</td><td>Pure values â†’ buffers</td><td>Explicit pointers</td><td><strong>Explicit <code>LOAD</code>/<code>STORE</code></strong></td></tr>
<tr><td><strong>Optimization</strong></td><td>None</td><td>Specialized passes</td><td>MLIR patterns</td><td><strong>Unified rewriting</strong></td></tr>
<tr><td><strong>Targets</strong></td><td>Runtime engines</td><td>CPU/GPU/TPU</td><td>GPU only</td><td>CPU/GPU</td></tr>
</tbody>
</table>
</div>
<p><strong>ONNX</strong> maximizes portability. Operations like <code>Conv</code> and <code>MatMul</code> hide all implementation details. Great for model exchange, but you canâ€™t optimize what you canâ€™t see.</p>
<p><strong>XLA HLO</strong> is functional and pureâ€”no side effects, immutable tensors. This enables algebraic optimization but requires a separate â€œbuffer assignmentâ€ phase before code generation. The transition from HLO to LMHLO (buffer-based) is a fundamental boundary.</p>
<p><strong>Triton</strong> exposes more than ONNX but less than Morok. You write â€œtile-levelâ€ codeâ€”operations on blocks of dataâ€”and the compiler handles thread-level details. Explicit memory (<code>tl.load</code>, <code>tl.store</code>) but implicit parallelization within tiles.</p>
<p><strong>Morok</strong> exposes everything: loops are explicit (<code>RANGE</code>), memory is explicit (<code>LOAD</code>/<code>STORE</code>), parallelization is explicit (<code>AxisType</code>). This means more to learn, but nothing is hidden.</p>
<hr>
<h2 id="why-this-matters-practical-benefits"><a class="header" href="#why-this-matters-practical-benefits">Why This Matters: Practical Benefits</a></h2>
<p>Morokâ€™s transparent IR has practical benefits for ML engineers:</p>
<p><strong>Debugging is direct.</strong> Print the graph at any stage:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>println!("{}", tensor.uop().tree());
<span class="boring">}</span></code></pre>
<p>Youâ€™ll see exactly what operations exist, how they connect, and where the computation happens. No â€œkernel Xâ€ mysteries.</p>
<p><strong>Performance tuning is informed.</strong> See which loops are parallelized:</p>
<pre><code class="language-text">[RANGE(batch, Global)]    # parallelized across GPU blocks
[RANGE(channel, Local)]   # parallelized within blocks
[RANGE(pixel, Loop)]      # sequential â€” might be slow!
</code></pre>
<p>If something should be parallel but isnâ€™t, you can see it.</p>
<p><strong>The mental model is simple.</strong> Thereâ€™s one IR, one transformation mechanism, one set of operations. You donâ€™t need to learn XLA HLO <em>and</em> MLIR <em>and</em> Triton <em>and</em> LLVM. Just UOps.</p>
<p><strong>Optimization is composable.</strong> Want a custom rewrite? Add a pattern:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>patterns! {
    // Your custom optimization
    MyPattern(x, y) ~&gt; better_version(x, y),
}
<span class="boring">}</span></code></pre>
<p>It works with the same engine as constant folding, fusion, and everything else.</p>
<hr>
<h2 id="the-deeper-insight-1"><a class="header" href="#the-deeper-insight-1">The Deeper Insight</a></h2>
<p>Morok/Tinygrad proves that compiler complexity is often <em>accidental</em>, not essential. The multi-layer IR stacks in TensorFlow and PyTorch accumulated organicallyâ€”each layer solved a real problem, but the combined system is harder to understand than any individual part.</p>
<p>One well-designed IR, one transformation mechanism, and principled composition can replace thousands of lines of specialized passes. Itâ€™s the Unix philosophy applied to compilers: do one thing well, and compose.</p>
<p>The cost is explicitnessâ€”you see loops, memory accesses, and parallelization hints that other IRs hide. But visibility is a feature, not a bug. When your model is slow, you want to see <em>why</em>, not hope the compiler figures it out.</p>
<p>Thatâ€™s the bet Morok makes: transparent complexity beats hidden complexity.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="pattern-based-optimization"><a class="header" href="#pattern-based-optimization">Pattern-Based Optimization</a></h1>
<p>Open any production ML compiler and youâ€™ll find dozens of optimization passes: constant folding, dead code elimination, operator fusion, loop tiling, vectorization, memory layout optimization. Each pass has its own data structures, its own traversal logic, its own bugs.</p>
<p>Morok takes a different approach: <strong>one mechanism for everything</strong>.</p>
<pre><code class="language-text">Traditional Compiler:              Morok:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Constant Folding       â”‚       â”‚                         â”‚
â”‚  Dead Code Elimination  â”‚       â”‚   patterns! {           â”‚
â”‚  Loop Unrolling         â”‚       â”‚       Add[x, @zero] ~&gt; xâ”‚
â”‚  Operator Fusion        â”‚       â”‚       Mul[x, @zero] ~&gt; 0â”‚
â”‚  Vectorization          â”‚       â”‚       // ...more        â”‚
â”‚  Memory Planning        â”‚       â”‚   }                     â”‚
â”‚  ...20 more passes      â”‚       â”‚                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚   graph_rewrite(...)    â”‚
     Custom logic each            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       One mechanism
</code></pre>
<p>Every optimization in Morok is expressed as a <strong>pattern</strong>: â€œwhen you see this structure, replace it with that structure.â€ The same <code>graph_rewrite()</code> function applies constant folding, converts movement ops to loops, optimizes memory access patterns, and lowers to hardware primitives.</p>
<p>This chapter explains how pattern-based optimization works and why itâ€™s powerful.</p>
<hr>
<h2 id="the-patterns-dsl"><a class="header" href="#the-patterns-dsl">The <code>patterns!</code> DSL</a></h2>
<p>Morok provides a domain-specific language for writing optimization patterns. Hereâ€™s what it looks like:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>patterns! {
    // Identity folding: x + 0 â†’ x
    Add[x, @zero] ~&gt; |x| x.clone(),

    // Constant folding: 3 + 4 â†’ 7
    Add(a @const(a_val), b @const(b_val))
        =&gt; |a, a_val, b_val| eval_add(a_val, b_val).map(|r| UOp::const_(a.dtype(), r)),

    // Self-folding: x / x â†’ 1
    Idiv(x, x) ~&gt; |x| UOp::one(x.dtype()),

    // Dead code elimination: if(true) { t } else { f } â†’ t
    Where(@true, t, _f) ~&gt; |t| t.clone(),
}
<span class="boring">}</span></code></pre>
<p>The macro compiles these patterns into efficient Rust code. Letâ€™s break down the syntax:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Syntax</th><th>Meaning</th><th>Example</th></tr>
</thead>
<tbody>
<tr><td><code>(x, y)</code></td><td><strong>Ordered.</strong> Match in exact order.</td><td><code>Sub(x, @zero) ~&gt; x</code></td></tr>
<tr><td><code>[x, y]</code></td><td><strong>Commutative.</strong> Try both orderings.</td><td><code>Add[x, @zero] ~&gt; x</code></td></tr>
<tr><td><code>@zero</code></td><td><strong>Zero constant.</strong> Matches 0 or 0.0.</td><td><code>Mul[_, z @ @zero] ~&gt; z</code></td></tr>
<tr><td><code>@one</code></td><td><strong>One constant.</strong> Matches 1 or 1.0.</td><td><code>Mul[x, @one] ~&gt; x</code></td></tr>
<tr><td><code>@const(val)</code></td><td><strong>Extract constant.</strong> Binds the value.</td><td><code>Add(@const(a), @const(b))</code></td></tr>
<tr><td><code>x, x</code></td><td><strong>Same operand.</strong> Auto-generates ptr_eq check.</td><td><code>Idiv(x, x) ~&gt; UOp::one(...)</code></td></tr>
<tr><td><code>~&gt;</code></td><td><strong>Infallible.</strong> Always succeeds, returns <code>Arc&lt;UOp&gt;</code>.</td><td><code>Add[x, @zero] ~&gt; x</code></td></tr>
<tr><td><code>=&gt;</code></td><td><strong>Fallible.</strong> May fail, returns <code>Option&lt;Arc&lt;UOp&gt;&gt;</code>.</td><td><code>=&gt; eval(...).map(...)</code></td></tr>
<tr><td><code>for op in binary [...]</code></td><td><strong>Template.</strong> Generate patterns for multiple ops.</td><td>See below</td></tr>
<tr><td><code>@context Type</code></td><td><strong>Stateful.</strong> Access mutable context in patterns.</td><td>See below</td></tr>
</tbody>
</table>
</div>
<h3 id="template-expansion"><a class="header" href="#template-expansion">Template Expansion</a></h3>
<p>Instead of writing the same pattern for every binary operation, use a for-loop:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>patterns! {
    for op in binary [Add, Mul, Sub, Idiv, Fdiv, Max] {
        op(a @const(a_val), b @const(b_val))
            =&gt; |a, a_val, b_val| eval_binary(op, a_val, b_val)
                .map(|r| UOp::const_(a.dtype(), r))
    }
}
<span class="boring">}</span></code></pre>
<p>This expands to six separate patterns at compile timeâ€”one for each operation.</p>
<h3 id="stateful-patterns"><a class="header" href="#stateful-patterns">Stateful Patterns</a></h3>
<p>Some optimizations need context (e.g., which kernel weâ€™re in, what ranges are active). Declare a context type:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>patterns! {
    @context KernelContext;

    ReduceAxis { src } =&gt; |reduce, src, ctx| {
        ctx.record_reduction(reduce);
        transform_reduce(reduce, src, ctx)
    }
}
<span class="boring">}</span></code></pre>
<p>The context is passed as the last argument to pattern closures.</p>
<hr>
<h2 id="how-pattern-matching-works"><a class="header" href="#how-pattern-matching-works">How Pattern Matching Works</a></h2>
<p>The <code>patterns!</code> macro generates a <code>SimplifiedPatternMatcher</code> that dispatches patterns in <strong>O(1)</strong> time.</p>
<h3 id="the-opkey-index"><a class="header" href="#the-opkey-index">The OpKey Index</a></h3>
<p>Every UOp has an operation type (Add, Mul, Load, etc.). The <code>#[derive(PatternEnum)]</code> macro generates an <code>OpKey</code> enum that maps operations to hashable keys:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub enum OpKey {
    Binary(BinaryOp),    // Add, Mul, Sub, ...
    Unary(UnaryOp),      // Neg, Sqrt, Exp, ...
    Ternary(TernaryOp),  // Where, MulAcc
    Const,
    Load,
    Store,
    // ... one variant per operation category
}
<span class="boring">}</span></code></pre>
<h3 id="the-matcher-structure"><a class="header" href="#the-matcher-structure">The Matcher Structure</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct SimplifiedPatternMatcher&lt;C = ()&gt; {
    indexed: HashMap&lt;OpKey, Vec&lt;PatternClosure&lt;C&gt;&gt;&gt;,  // O(1) lookup
    wildcards: Vec&lt;PatternClosure&lt;C&gt;&gt;,                 // patterns matching any op
}
<span class="boring">}</span></code></pre>
<p>When matching a UOp:</p>
<ol>
<li><strong>Extract OpKey</strong> from the UOpâ€™s operation</li>
<li><strong>Lookup</strong> in the HashMapâ€”O(1)</li>
<li><strong>Try each closure</strong> until one matches</li>
<li><strong>Fall back</strong> to wildcards if no indexed pattern matches</li>
</ol>
<p>This is 5-10x faster than scanning all patterns linearly.</p>
<h3 id="commutative-handling"><a class="header" href="#commutative-handling">Commutative Handling</a></h3>
<p>For patterns like <code>Add[x, @zero]</code>, the macro generates code that tries both orderings:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Try (x, @zero)
if let Some(result) = try_match_ordered(&amp;children[0], &amp;children[1]) {
    return result;
}
// Try (@zero, x)
if let Some(result) = try_match_ordered(&amp;children[1], &amp;children[0]) {
    return result;
}
<span class="boring">}</span></code></pre>
<h3 id="duplicate-detection"><a class="header" href="#duplicate-detection">Duplicate Detection</a></h3>
<p>When you write <code>Idiv(x, x)</code>, the pattern should only match if both operands are the <em>same</em> UOp (pointer equality, not structural equality). The macro automatically generates this check:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Generated code for Idiv(x, x)
let x = &amp;children[0];
let x_dup = &amp;children[1];
if !Arc::ptr_eq(x, x_dup) {
    return NoMatch;
}
// ... rest of pattern
<span class="boring">}</span></code></pre>
<p>This leverages hash consingâ€”identical subexpressions share the same pointer.</p>
<hr>
<h2 id="the-rewrite-engine-two-stage-algorithm"><a class="header" href="#the-rewrite-engine-two-stage-algorithm">The Rewrite Engine: Two-Stage Algorithm</a></h2>
<p>Pattern matching alone isnâ€™t enough. Consider this expression:</p>
<pre><code class="language-text">WHERE(Lt(3, 5), t, f)
</code></pre>
<p>To simplify it, we need two steps:</p>
<ol>
<li><code>Lt(3, 5)</code> â†’ <code>true</code> (constant folding)</li>
<li><code>WHERE(true, t, f)</code> â†’ <code>t</code> (dead code elimination)</li>
</ol>
<p>But the <code>WHERE</code> pattern wonâ€™t match until its child is simplified. The rewrite engine solves this with a <strong>two-stage algorithm</strong>.</p>
<h3 id="stage-0-pattern-application"><a class="header" href="#stage-0-pattern-application">Stage 0: Pattern Application</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn rewrite_stage0(&amp;mut self, uop: &amp;Arc&lt;UOp&gt;) -&gt; RewriteResult {
    match self.matcher.try_match(uop) {
        Some(replacement) =&gt; RewriteResult::Rewritten(replacement),
        None =&gt; RewriteResult::Gate(uop.clone()),  // process children
    }
}
<span class="boring">}</span></code></pre>
<p>If no pattern matches, return <code>Gate</code>â€”a signal to process children first.</p>
<h3 id="stage-1-source-reconstruction"><a class="header" href="#stage-1-source-reconstruction">Stage 1: Source Reconstruction</a></h3>
<p>After children are rewritten, rebuild the node with new children and try patterns again:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn rewrite_stage1(&amp;mut self, uop: &amp;Arc&lt;UOp&gt;, new_children: Vec&lt;Arc&lt;UOp&gt;&gt;) {
    // Rebuild with optimized children
    let rebuilt = uop.with_sources(new_children);

    // Try patterns againâ€”might match now!
    match self.matcher.try_match(&amp;rebuilt) {
        Some(replacement) =&gt; replacement,
        None =&gt; rebuilt,
    }
}
<span class="boring">}</span></code></pre>
<h3 id="the-magic-cascading-optimizations"><a class="header" href="#the-magic-cascading-optimizations">The Magic: Cascading Optimizations</a></h3>
<pre><code class="language-text">Stage 0: WHERE(Lt(3, 5), t, f)     â†’ Gate (no match, process children)
         â””â”€â”€ Lt(3, 5)              â†’ true (constant folding matches!)

Stage 1: WHERE(true, t, f)         â†’ t (dead code elimination matches!)
</code></pre>
<p>The reconstruction stage re-applies patterns, enabling multi-step optimizations in a single traversal.</p>
<h3 id="safety-limits"><a class="header" href="#safety-limits">Safety Limits</a></h3>
<p>To prevent infinite loops, the engine has limits:</p>
<ul>
<li><strong>1000 iterations</strong> per node maximum</li>
<li><strong>100,000 iterations</strong> total maximum</li>
<li>Panics with diagnostic info if limits exceeded</li>
</ul>
<p>In practice, well-formed patterns converge quickly.</p>
<hr>
<h2 id="the-full-optimization-pipeline"><a class="header" href="#the-full-optimization-pipeline">The Full Optimization Pipeline</a></h2>
<p>Pattern matching is one part of a larger pipeline. When you call <code>tensor.realize()</code>, hereâ€™s what happens:</p>
<pre><code class="language-text">Tensor.realize()
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RANGEIFY                                               â”‚
â”‚  Convert movement ops (RESHAPE, PERMUTE, EXPAND)        â”‚
â”‚  into explicit RANGE loops with INDEX operations        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  KERNEL SPLITTING                                       â”‚
â”‚  Split computation graph at STORE boundaries            â”‚
â”‚  Each STORE becomes a separate kernel                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FOR EACH KERNEL:                                       â”‚
â”‚                                                         â”‚
â”‚  1. Symbolic Simplification (algebraic patterns)        â”‚
â”‚                                                         â”‚
â”‚  2. Scheduler Creation                                  â”‚
â”‚     â””â”€â”€ Convert LOOP â†’ GLOBAL for GPU parallelization   â”‚
â”‚                                                         â”‚
â”‚  3. Kernel Optimization (heuristic OR beam search)      â”‚
â”‚     â”œâ”€â”€ Tensor Cores (WMMA) for matmul                  â”‚
â”‚     â”œâ”€â”€ Vectorization (UPCAST)                          â”‚
â”‚     â”œâ”€â”€ Loop Unrolling (UNROLL)                         â”‚
â”‚     â”œâ”€â”€ GPU Local Memory (LOCAL)                        â”‚
â”‚     â”œâ”€â”€ Grouped Reductions (GROUP)                      â”‚
â”‚     â””â”€â”€ Threading (THREAD) for CPU                      â”‚
â”‚                                                         â”‚
â”‚  4. Post-Optimization Passes                            â”‚
â”‚     â”œâ”€â”€ Devectorize (memory coalescing)                 â”‚
â”‚     â”œâ”€â”€ Expand (UNROLL â†’ vector operations)             â”‚
â”‚     â”œâ”€â”€ FMA Decomposition (a*b+c â†’ MulAcc)              â”‚
â”‚     â””â”€â”€ Bool Storage (cast boolâ†”uint8 for memory)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CODE GENERATION                                        â”‚
â”‚  Render optimized AST to LLVM IR, compile, execute      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<p>Each box uses pattern-based rewriting. The difference is which patterns are applied:</p>
<ul>
<li><strong>Rangeify</strong>: Movement op â†’ BUFFERIZE + INDEX patterns</li>
<li><strong>Symbolic</strong>: Algebraic simplification patterns</li>
<li><strong>Post-opt</strong>: Memory access optimization patterns</li>
</ul>
<hr>
<h2 id="kernel-optimization-heuristics-vs-beam-search"><a class="header" href="#kernel-optimization-heuristics-vs-beam-search">Kernel Optimization: Heuristics vs Beam Search</a></h2>
<p>After symbolic simplification, each kernel needs <em>scheduling decisions</em>: how to tile loops, where to parallelize, whether to use tensor cores. Morok offers two strategies.</p>
<h3 id="heuristics-default"><a class="header" href="#heuristics-default">Heuristics (Default)</a></h3>
<p>The heuristic optimizer applies optimizations in a fixed order:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn hand_coded_optimizations(scheduler: &amp;mut Scheduler) {
    // 1. Tensor cores (if matmul pattern detected)
    if let Some(tc) = detect_tensor_core_pattern(scheduler) {
        apply_tensor_core(scheduler, tc);
        return;  // TC handles everything
    }

    // 2. Grouped reductions (two-stage for large reductions)
    apply_grouped_reduction_if_needed(scheduler);

    // 3. Vectorization (UPCAST output dimensions)
    apply_upcast(scheduler, 4);

    // 4. GPU local memory (workgroup dimensions)
    apply_local_dims(scheduler);

    // 5. CPU threading
    apply_threading(scheduler);
}
<span class="boring">}</span></code></pre>
<p><strong>Pros</strong>: Fast (~50ms per kernel), predictable, no hardware measurement needed.</p>
<p><strong>Cons</strong>: May miss optimization opportunities, fixed heuristics donâ€™t adapt to workload.</p>
<h3 id="beam-search-optional"><a class="header" href="#beam-search-optional">Beam Search (Optional)</a></h3>
<p>For production workloads, beam search finds better schedules:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn beam_search(scheduler: Scheduler, config: BeamConfig) -&gt; Scheduler {
    let mut beam = vec![scheduler];

    for iteration in 0..config.max_iterations {
        let mut candidates = vec![];

        for state in &amp;beam {
            // Generate all valid next actions
            for action in generate_actions(state) {
                if let Ok(next) = state.apply(action) {
                    candidates.push(next);
                }
            }
        }

        // Compile and time each candidate
        let timed: Vec&lt;_&gt; = candidates.par_iter()
            .map(|c| (c, measure_kernel_time(c)))
            .collect();

        // Keep top K by execution time
        beam = timed.into_iter()
            .sorted_by_key(|(_, time)| *time)
            .take(config.beam_width)
            .map(|(c, _)| c)
            .collect();
    }

    beam.into_iter().next().unwrap()
}
<span class="boring">}</span></code></pre>
<p>The action space includes ~500 predefined actions:</p>
<ul>
<li><code>UPCAST(axis, amount)</code> â€” vectorize output dimension</li>
<li><code>UNROLL(axis, amount)</code> â€” unroll reduction loop</li>
<li><code>LOCAL(axis, amount)</code> â€” use GPU shared memory</li>
<li><code>GROUP(axis, amount)</code> â€” two-stage reduction</li>
<li><code>THREAD(axis, amount)</code> â€” CPU parallelization</li>
<li><code>SWAP(axis1, axis2)</code> â€” reorder global dimensions</li>
</ul>
<p><strong>Pros</strong>: Finds near-optimal schedules, adapts to hardware.</p>
<p><strong>Cons</strong>: Minutes per kernel (but results are cached by AST hash).</p>
<h3 id="configuration"><a class="header" href="#configuration">Configuration</a></h3>
<pre><code class="language-bash"># Disable optimization (debugging)
MOROK_NOOPT=1 cargo run

# Enable beam search with width 8
MOROK_BEAM=8 cargo run
</code></pre>
<p>Or programmatically:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let config = OptimizerConfig::builder()
    .strategy(OptStrategy::Beam { width: 8 })
    .build();

tensor.realize_with(config)?;
<span class="boring">}</span></code></pre>
<hr>
<h2 id="comparison-how-other-compilers-optimize"><a class="header" href="#comparison-how-other-compilers-optimize">Comparison: How Other Compilers Optimize</a></h2>
<p>Different ML compilers take different approaches to optimization:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Aspect</th><th>XLA</th><th>TVM/Ansor</th><th>Triton</th><th><strong>Morok</strong></th></tr>
</thead>
<tbody>
<tr><td><strong>Philosophy</strong></td><td>Fixed heuristics</td><td>Search-based</td><td>Programmer-guided</td><td>Pattern-based</td></tr>
<tr><td><strong>Fusion</strong></td><td>Conservative rules</td><td>Tile-and-fuse</td><td>Block-level</td><td>Graph rewriting</td></tr>
<tr><td><strong>Auto-tuning</strong></td><td>None</td><td>Evolutionary + cost model</td><td>Grid search</td><td>Beam search</td></tr>
<tr><td><strong>Tuning cost</strong></td><td>0</td><td>Hours</td><td>Minutes</td><td>Minutes (cached)</td></tr>
<tr><td><strong>Flexibility</strong></td><td>Low</td><td>High</td><td>Medium</td><td>High</td></tr>
<tr><td><strong>Transparency</strong></td><td>Low (C++ passes)</td><td>Medium (Python)</td><td>Medium (DSL)</td><td>High (patterns!)</td></tr>
</tbody>
</table>
</div>
<h3 id="xla--production-conservative"><a class="header" href="#xla--production-conservative">XLA â€” Production Conservative</a></h3>
<p>XLA uses fixed heuristics for fusion decisions. Safe and predictable, but leaves performance on the table. The fusion rules are hard-coded in C++â€”extending them requires deep compiler knowledge.</p>
<h3 id="tvmansor--maximum-auto-tuning"><a class="header" href="#tvmansor--maximum-auto-tuning">TVM/Ansor â€” Maximum Auto-Tuning</a></h3>
<p>TVM separates <em>what</em> to compute from <em>how</em> to compute it. Ansor uses evolutionary search with a learned cost model to explore the schedule space. Can achieve best-in-class performance, but tuning takes hours per model.</p>
<h3 id="triton--programmer-guided"><a class="header" href="#triton--programmer-guided">Triton â€” Programmer-Guided</a></h3>
<p>Triton exposes a Python-like DSL where you write blocked algorithms explicitly. The compiler handles register allocation and memory management. Good balance of control and automation, but requires GPU programming expertise.</p>
<h3 id="morok--pattern-composition"><a class="header" href="#morok--pattern-composition">Morok â€” Pattern Composition</a></h3>
<p>Morokâ€™s insight: express optimizations as composable patterns. Each pattern is local and verifiable. Complex optimizations emerge from composition. Beam search adds auto-tuning when needed, with results cached for reuse.</p>
<hr>
<h2 id="why-this-matters-practical-benefits-1"><a class="header" href="#why-this-matters-practical-benefits-1">Why This Matters: Practical Benefits</a></h2>
<p>Pattern-based optimization has concrete advantages for developers:</p>
<p><strong>Debugging is direct.</strong> Patterns are readable code. Add a <code>println!</code> to any pattern to trace when it fires:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>patterns! {
    Add[x, @zero] ~&gt; |x| {
        println!("Folding add-zero: {:?}", x);
        x.clone()
    }
}
<span class="boring">}</span></code></pre>
<p><strong>Extensibility is easy.</strong> Adding a custom optimization is two lines:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>patterns! {
    // Your domain-specific optimization
    MyOp(x, y) if is_special_case(x, y) ~&gt; transform(x, y)
}
<span class="boring">}</span></code></pre>
<p>No need to understand compiler internals, write visitors, or modify pass managers.</p>
<p><strong>Correctness is local.</strong> Each pattern is a small theorem: â€œif this structure appears, replacing it with that structure preserves semantics.â€ Verify each pattern independently. Composition of correct patterns yields correct programs.</p>
<p><strong>Performance is tunable.</strong> O(1) pattern dispatch is fast by default. Enable beam search for production workloads. Cache results by AST hashâ€”tune once, benefit forever.</p>
<hr>
<h2 id="the-deeper-insight-2"><a class="header" href="#the-deeper-insight-2">The Deeper Insight</a></h2>
<p>Pattern matching trades generality for composability.</p>
<p>A general-purpose optimization pass can do anythingâ€”but thatâ€™s exactly the problem. Itâ€™s hard to verify, hard to extend, hard to compose with other passes. Ordering matters. Interactions are subtle.</p>
<p>A pattern is constrained: it matches a specific structure and produces a specific replacement. But constraints enable composition. Run patterns in any orderâ€”the result converges to the same fixed point. Add new patterns without breaking existing ones. Delete patterns without cascading failures.</p>
<p>Each pattern is a theorem about semantic equivalence. The rewrite engine is a theorem prover, finding derivations from input to optimized output. Correctness follows from the correctness of individual steps.</p>
<p>This is the Unix philosophy applied to compilers: small, focused tools that compose. Pattern-based optimization wonâ€™t solve every problemâ€”but for the problems it solves, it solves them elegantly.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="op-bestiary-a-field-guide-to-uop-operations"><a class="header" href="#op-bestiary-a-field-guide-to-uop-operations">Op Bestiary: A Field Guide to UOp Operations</a></h1>
<p>When debugging Morok IR dumps, youâ€™ll encounter operations that arenâ€™t obvious from their names. This chapter documents non-trivial operations with signatures, field explanations, and examples.</p>
<p><strong>Whatâ€™s covered:</strong> Operations that require explanationâ€”loop control, reductions, memory operations, kernel structure, vectorization, tensor cores.</p>
<p><strong>Whatâ€™s NOT covered:</strong> Trivial ALU operations (<code>Add</code>, <code>Mul</code>, <code>Sqrt</code>, etc.) that work exactly as youâ€™d expect.</p>
<hr>
<h2 id="loop-control-range-and-end"><a class="header" href="#loop-control-range-and-end">Loop Control: RANGE and END</a></h2>
<h3 id="range--loop-scope-opener"><a class="header" href="#range--loop-scope-opener">RANGE â€” Loop Scope Opener</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>Range {
    end: Arc&lt;UOp&gt;,           // loop bound (exclusive)
    axis_id: AxisId,         // identifier for deduplication
    axis_type: AxisType,     // scheduling behavior
}
<span class="boring">}</span></code></pre>
<p><strong>Fields:</strong></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Field</th><th>Type</th><th>Purpose</th></tr>
</thead>
<tbody>
<tr><td><code>end</code></td><td><code>Arc&lt;UOp&gt;</code></td><td>Upper bound (exclusive), typically a <code>CONST</code></td></tr>
<tr><td><code>axis_id</code></td><td><code>AxisId</code></td><td><code>Unrenumbered(n)</code> before kernel splitting, <code>Renumbered(n)</code> after</td></tr>
<tr><td><code>axis_type</code></td><td><code>AxisType</code></td><td>Determines how the loop is scheduled (see below)</td></tr>
</tbody>
</table>
</div>
<p><strong>AxisType Hierarchy:</strong></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Type</th><th>Priority</th><th>GPU Mapping</th><th>Purpose</th></tr>
</thead>
<tbody>
<tr><td><code>Outer</code></td><td>-2</td><td>â€”</td><td>Kernel boundary marker</td></tr>
<tr><td><code>Loop</code></td><td>-1</td><td><code>for</code> loop</td><td>Sequential iteration</td></tr>
<tr><td><code>Global</code></td><td>0</td><td><code>blockIdx</code></td><td>Grid parallelism</td></tr>
<tr><td><code>Thread</code></td><td>0</td><td>thread pool</td><td>CPU parallelism</td></tr>
<tr><td><code>Warp</code></td><td>1</td><td>warp/wavefront</td><td>Sub-group parallelism</td></tr>
<tr><td><code>Local</code></td><td>2</td><td><code>threadIdx</code></td><td>Workgroup parallelism</td></tr>
<tr><td><code>GroupReduce</code></td><td>2</td><td>shared memory</td><td>Two-stage reduction</td></tr>
<tr><td><code>Upcast</code></td><td>3</td><td>SIMD</td><td>Vectorization</td></tr>
<tr><td><code>Reduce</code></td><td>4</td><td>accumulator</td><td>Reduction dimension</td></tr>
<tr><td><code>Unroll</code></td><td>5</td><td>unrolled</td><td>Loop unrolling</td></tr>
</tbody>
</table>
</div>
<p>Priority determines loop nesting orderâ€”lower values are outer loops.</p>
<p><strong>Example:</strong></p>
<pre><code class="language-text">RANGE(end=128, axis_id=R0, type=Global)
â””â”€â”€ CONST(128) : Index
</code></pre>
<h3 id="end--loop-scope-closer"><a class="header" href="#end--loop-scope-closer">END â€” Loop Scope Closer</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>End {
    computation: Arc&lt;UOp&gt;,              // value computed inside loop
    ranges: SmallVec&lt;[Arc&lt;UOp&gt;; 4]&gt;,    // ranges being closed
}
<span class="boring">}</span></code></pre>
<p>END closes one or more RANGE scopes and removes them from the active set. Multiple ranges can be closed simultaneously.</p>
<p><strong>Example:</strong></p>
<pre><code class="language-text">END
â”œâ”€â”€ STORE(...)           â€” computation
â”œâ”€â”€ RANGE(R0, Global)    â€” first range closed
â””â”€â”€ RANGE(R1, Local)     â€” second range closed
</code></pre>
<hr>
<h2 id="reduction-reduce-vs-reduce_axis"><a class="header" href="#reduction-reduce-vs-reduce_axis">Reduction: REDUCE vs REDUCE_AXIS</a></h2>
<p>Two operations with similar names serve different purposes.</p>
<h3 id="reduce_axis--tensor-dimension-reduction-high-level"><a class="header" href="#reduce_axis--tensor-dimension-reduction-high-level">REDUCE_AXIS â€” Tensor Dimension Reduction (High-Level)</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>ReduceAxis {
    src: Arc&lt;UOp&gt;,           // input tensor
    reduce_op: ReduceOp,     // Add, Mul, Max, Min
    axes: Vec&lt;usize&gt;,        // axes to reduce
}
<span class="boring">}</span></code></pre>
<p>Used <strong>before</strong> rangeify. Operates on tensor dimensions like NumPyâ€™s <code>.sum(axis=0)</code>.</p>
<p><strong>Example:</strong></p>
<pre><code class="language-text">REDUCE_AXIS(Add, axes=[1])
â””â”€â”€ BUFFER[10, 20] : Float32
</code></pre>
<p>This reduces a <code>[10, 20]</code> tensor to <code>[10]</code> by summing along axis 1.</p>
<h3 id="reduce--range-iteration-reduction-low-level"><a class="header" href="#reduce--range-iteration-reduction-low-level">REDUCE â€” Range Iteration Reduction (Low-Level)</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>Reduce {
    src: Arc&lt;UOp&gt;,                      // value to accumulate
    ranges: SmallVec&lt;[Arc&lt;UOp&gt;; 4]&gt;,    // ranges being reduced
    reduce_op: ReduceOp,                // Add, Mul, Max, Min
}
<span class="boring">}</span></code></pre>
<p>Used <strong>after</strong> rangeify. Accumulates values across RANGE iterations and closes the specified ranges.</p>
<p><strong>ReduceOp Variants:</strong></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Op</th><th>Identity</th><th>Operation</th><th>Tinygrad</th></tr>
</thead>
<tbody>
<tr><td><code>Add</code></td><td>0</td><td><code>acc + value</code></td><td>âœ“</td></tr>
<tr><td><code>Mul</code></td><td>1</td><td><code>acc * value</code></td><td>âœ“</td></tr>
<tr><td><code>Max</code></td><td>-âˆ</td><td><code>max(acc, value)</code></td><td>âœ“</td></tr>
<tr><td><code>Min</code></td><td>+âˆ</td><td><code>min(acc, value)</code></td><td>Morok-only</td></tr>
</tbody>
</table>
</div>
<blockquote>
<p><strong>Compatibility:</strong> Tinygradâ€™s spec restricts REDUCE_AXIS to <code>{Add, Mul, Max}</code>. Morok extends this with <code>Min</code>.</p>
</blockquote>
<p><strong>Example:</strong></p>
<pre><code class="language-text">REDUCE(Add)
â”œâ”€â”€ MUL                      â€” value to accumulate
â”‚   â”œâ”€â”€ LOAD(A, ...)
â”‚   â””â”€â”€ LOAD(B, ...)
â””â”€â”€ RANGE(R2, Reduce)        â€” range being reduced
    â””â”€â”€ CONST(64)
</code></pre>
<h3 id="allreduce--cross-device-reduction"><a class="header" href="#allreduce--cross-device-reduction">ALLREDUCE â€” Cross-Device Reduction</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>AllReduce {
    src: Arc&lt;UOp&gt;,           // local partial result
    device: Arc&lt;UOp&gt;,        // device specification
    reduce_op: ReduceOp,     // reduction operation
}
<span class="boring">}</span></code></pre>
<p>Performs distributed reduction across multiple devices. Used for multi-GPU training.</p>
<hr>
<h2 id="buffer-operations"><a class="header" href="#buffer-operations">Buffer Operations</a></h2>
<h3 id="buffer--buffer-declaration"><a class="header" href="#buffer--buffer-declaration">BUFFER â€” Buffer Declaration</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>Buffer {
    unique: Arc&lt;UOp&gt;,        // UNIQUE op for identity
    device: Arc&lt;UOp&gt;,        // DEVICE op
    size: usize,             // total element count
}
<span class="boring">}</span></code></pre>
<p>Declares a buffer for tensor storage. The <code>unique</code> field ensures distinct buffers even with identical size/device.</p>
<h3 id="bufferize--materialization-marker"><a class="header" href="#bufferize--materialization-marker">BUFFERIZE â€” Materialization Marker</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>Bufferize {
    compute: Arc&lt;UOp&gt;,                  // computation to materialize
    ranges: SmallVec&lt;[Arc&lt;UOp&gt;; 4]&gt;,    // output dimensions
    opts: BufferizeOpts,                // address space, device
}
<span class="boring">}</span></code></pre>
<p>Marks where computation should materialize to memory. Triggers kernel splitting.</p>
<p><strong>BufferizeOpts:</strong></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Field</th><th>Type</th><th>Purpose</th></tr>
</thead>
<tbody>
<tr><td><code>device</code></td><td><code>Option&lt;DeviceSpec&gt;</code></td><td>Target device, <code>None</code> for local</td></tr>
<tr><td><code>addrspace</code></td><td><code>AddrSpace</code></td><td><code>Global</code> (device) or <code>Local</code> (shared)</td></tr>
</tbody>
</table>
</div>
<p><strong>Example:</strong></p>
<pre><code class="language-text">BUFFERIZE(opts={addrspace=Global})
â”œâ”€â”€ REDUCE(Add, ...)         â€” computation
â”œâ”€â”€ RANGE(R0, Global)        â€” output dim 0
â””â”€â”€ RANGE(R1, Global)        â€” output dim 1
</code></pre>
<h3 id="index--multi-dimensional-buffer-access"><a class="header" href="#index--multi-dimensional-buffer-access">INDEX â€” Multi-Dimensional Buffer Access</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>Index {
    buffer: Arc&lt;UOp&gt;,                   // BUFFER or DEFINE_GLOBAL
    indices: SmallVec&lt;[Arc&lt;UOp&gt;; 4]&gt;,   // index per dimension
    gate: Option&lt;Arc&lt;UOp&gt;&gt;,             // optional predicate
}
<span class="boring">}</span></code></pre>
<p>Computes memory address from multi-dimensional indices. Returns element dtype (not pointer).</p>
<p><strong>Example:</strong></p>
<pre><code class="language-text">INDEX : Float32
â”œâ”€â”€ DEFINE_GLOBAL(0)
â”œâ”€â”€ RANGE(R0, Global)        â€” index for dim 0
â”œâ”€â”€ RANGE(R1, Loop)          â€” index for dim 1
â””â”€â”€ MUL(...)                 â€” index for dim 2
</code></pre>
<h3 id="pointer_index--low-level-pointer-arithmetic"><a class="header" href="#pointer_index--low-level-pointer-arithmetic">POINTER_INDEX â€” Low-Level Pointer Arithmetic</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>PointerIndex {
    ptr: Arc&lt;UOp&gt;,           // base pointer
    offset: Arc&lt;UOp&gt;,        // byte offset
}
<span class="boring">}</span></code></pre>
<p>Direct pointer arithmetic. Used after linearization when indices are flattened.</p>
<blockquote>
<p><strong>Compatibility:</strong> Tinygrad uses <code>INDEX</code> with a <code>ptr=True</code> flag instead of a separate operation.</p>
</blockquote>
<h3 id="load--memory-read"><a class="header" href="#load--memory-read">LOAD â€” Memory Read</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>Load {
    buffer: Arc&lt;UOp&gt;,        // buffer or pointer
    index: Arc&lt;UOp&gt;,         // INDEX op
}
<span class="boring">}</span></code></pre>
<p>Read value from buffer at index. For gated loads, use an INDEX with a gate (INDEX has an optional <code>gate</code> field).</p>
<p><strong>Example:</strong></p>
<pre><code class="language-text">LOAD : Float32
â”œâ”€â”€ DEFINE_GLOBAL(1)
â””â”€â”€ INDEX
    â”œâ”€â”€ DEFINE_GLOBAL(1)
    â”œâ”€â”€ RANGE(R0)
    â””â”€â”€ RANGE(R2)
</code></pre>
<h3 id="store--memory-write"><a class="header" href="#store--memory-write">STORE â€” Memory Write</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>Store {
    buffer: Arc&lt;UOp&gt;,                   // output buffer
    index: Arc&lt;UOp&gt;,                    // INDEX op
    value: Arc&lt;UOp&gt;,                    // value to write
    ranges: SmallVec&lt;[Arc&lt;UOp&gt;; 4]&gt;,    // ranges being closed
}
<span class="boring">}</span></code></pre>
<p>Write value to buffer. STORE closes the specified ranges, which represent output iteration dimensions. The ranges field is used for output upcasting: when a <code>Range(Upcast)</code> is included, it becomes <code>UNROLL</code> during expansion, then contracted via <code>CONTRACT</code>.</p>
<p>For gated stores, use an INDEX with a gate (INDEX has an optional <code>gate</code> field).</p>
<blockquote>
<p><strong>Compatibility:</strong> Morokâ€™s STORE has an explicit <code>index</code> field (sources: buffer=0, index=1, value=2, ranges=3+). Tinygradâ€™s STORE combines buffer and value differently (range_start=2).</p>
</blockquote>
<p><strong>Example:</strong></p>
<pre><code class="language-text">STORE
â”œâ”€â”€ DEFINE_GLOBAL(0)         â€” output buffer
â”œâ”€â”€ INDEX[R0, R1]            â€” write address
â”œâ”€â”€ REDUCE(Add, ...)         â€” value
â”œâ”€â”€ RANGE(R0, Global)        â€” output dim 0 (closed)
â””â”€â”€ RANGE(R1, Global)        â€” output dim 1 (closed)
</code></pre>
<hr>
<h2 id="kernel-structure"><a class="header" href="#kernel-structure">Kernel Structure</a></h2>
<h3 id="kernel--kernel-wrapper"><a class="header" href="#kernel--kernel-wrapper">KERNEL â€” Kernel Wrapper</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>Kernel {
    sources: SmallVec&lt;[Arc&lt;UOp&gt;; 4]&gt;,   // arguments
    ast: Arc&lt;UOp&gt;,                       // computation (usually SINK)
}
<span class="boring">}</span></code></pre>
<p>Wraps a complete kernel for code generation. Sources are kernel arguments (<code>DefineGlobal</code>, <code>DefineLocal</code>, <code>DefineVar</code>).</p>
<p><strong>Example:</strong></p>
<pre><code class="language-text">KERNEL
â”œâ”€â”€ DEFINE_GLOBAL(0)         â€” output buffer arg
â”œâ”€â”€ DEFINE_GLOBAL(1)         â€” input A arg
â”œâ”€â”€ DEFINE_GLOBAL(2)         â€” input B arg
â””â”€â”€ SINK                     â€” computation
    â””â”€â”€ STORE(...)
</code></pre>
<h3 id="sink--multiple-root-collector"><a class="header" href="#sink--multiple-root-collector">SINK â€” Multiple Root Collector</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>Sink {
    sources: SmallVec&lt;[Arc&lt;UOp&gt;; 4]&gt;,
}
<span class="boring">}</span></code></pre>
<p>Collects multiple outputs into a single root. Every kernelâ€™s <code>ast</code> is typically a SINK containing STORE operations.</p>
<p><strong>Example:</strong></p>
<pre><code class="language-text">SINK
â”œâ”€â”€ STORE(output_0, ...)
â”œâ”€â”€ STORE(output_1, ...)
â””â”€â”€ STORE(output_2, ...)
</code></pre>
<h3 id="after--dependency-marker"><a class="header" href="#after--dependency-marker">AFTER â€” Dependency Marker</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>After {
    passthrough: Arc&lt;UOp&gt;,              // value that flows through
    deps: SmallVec&lt;[Arc&lt;UOp&gt;; 4]&gt;,      // operations that must complete
}
<span class="boring">}</span></code></pre>
<p>Expresses execution dependencies between kernels without data dependency. The <code>passthrough</code> value is returned unchanged, but only after all <code>deps</code> complete.</p>
<p><strong>Example:</strong></p>
<pre><code class="language-text">SINK
â”œâ”€â”€ AFTER
â”‚   â”œâ”€â”€ DEFINE_GLOBAL(0)     â€” passthrough (buffer reference)
â”‚   â””â”€â”€ KERNEL(...)          â€” must complete first
â””â”€â”€ KERNEL(...)              â€” can use buffer after AFTER
</code></pre>
<h3 id="barrier--synchronization-fence"><a class="header" href="#barrier--synchronization-fence">BARRIER â€” Synchronization Fence</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>Barrier {
    src: Arc&lt;UOp&gt;,                      // value passing through
    deps: SmallVec&lt;[Arc&lt;UOp&gt;; 4]&gt;,      // operations to wait for
}
<span class="boring">}</span></code></pre>
<p>GPU workgroup synchronization. Ensures all threads in a workgroup reach the barrier before continuing.</p>
<hr>
<h2 id="vector-operations"><a class="header" href="#vector-operations">Vector Operations</a></h2>
<h3 id="vectorize--create-vector-from-scalars"><a class="header" href="#vectorize--create-vector-from-scalars">VECTORIZE â€” Create Vector from Scalars</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>Vectorize {
    elements: SmallVec&lt;[Arc&lt;UOp&gt;; 4]&gt;,
}
<span class="boring">}</span></code></pre>
<p>Combines N scalar values into a vector of size N. All elements must have the same base dtype.</p>
<p><strong>Example:</strong></p>
<pre><code class="language-text">VECTORIZE : &lt;4 x Float32&gt;
â”œâ”€â”€ CONST(1.0)
â”œâ”€â”€ CONST(2.0)
â”œâ”€â”€ CONST(3.0)
â””â”€â”€ CONST(4.0)
</code></pre>
<h3 id="gep--get-element-pointer-vector-extract"><a class="header" href="#gep--get-element-pointer-vector-extract">GEP â€” Get Element Pointer (Vector Extract)</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>Gep {
    vector: Arc&lt;UOp&gt;,        // source vector
    indices: Vec&lt;usize&gt;,     // positions to extract
}
<span class="boring">}</span></code></pre>
<p>Extracts elements from a vector:</p>
<ul>
<li>Single index â†’ scalar</li>
<li>Multiple indices â†’ smaller vector</li>
</ul>
<p><strong>Example:</strong></p>
<pre><code class="language-text">GEP([0, 2]) : &lt;2 x Float32&gt;
â””â”€â”€ VECTORIZE : &lt;4 x Float32&gt;
    â””â”€â”€ ...
</code></pre>
<h3 id="vconst--vector-constant"><a class="header" href="#vconst--vector-constant">VConst â€” Vector Constant</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>VConst {
    values: Vec&lt;ConstValue&gt;,
}
<span class="boring">}</span></code></pre>
<p>Vector of compile-time constants. More efficient than <code>VECTORIZE</code> of <code>CONST</code> nodes.</p>
<h3 id="cat--concatenate-vectors"><a class="header" href="#cat--concatenate-vectors">CAT â€” Concatenate Vectors</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>Cat {
    sources: SmallVec&lt;[Arc&lt;UOp&gt;; 4]&gt;,
}
<span class="boring">}</span></code></pre>
<p>Concatenates vectors into a larger vector. Output <code>vcount</code> = sum of input <code>vcount</code>s.</p>
<p><strong>Example:</strong></p>
<pre><code class="language-text">CAT : &lt;8 x Float32&gt;
â”œâ”€â”€ VECTORIZE : &lt;4 x Float32&gt;
â””â”€â”€ VECTORIZE : &lt;4 x Float32&gt;
</code></pre>
<h3 id="ptrcat--concatenate-pointers"><a class="header" href="#ptrcat--concatenate-pointers">PtrCat â€” Concatenate Pointers</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>PtrCat {
    sources: SmallVec&lt;[Arc&lt;UOp&gt;; 4]&gt;,
}
<span class="boring">}</span></code></pre>
<p>Groups memory accesses for vectorized load/store. Used by the devectorizer pass.</p>
<hr>
<h2 id="expansion-unroll-and-contract"><a class="header" href="#expansion-unroll-and-contract">Expansion: UNROLL and CONTRACT</a></h2>
<h3 id="unroll--expand-computation-across-iterations"><a class="header" href="#unroll--expand-computation-across-iterations">UNROLL â€” Expand Computation Across Iterations</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>Unroll {
    src: Arc&lt;UOp&gt;,                       // computation to expand
    unroll_axes: Vec&lt;(usize, usize)&gt;,    // (axis_index, factor) pairs
}
<span class="boring">}</span></code></pre>
<p>Creates multiple versions of computation for different iteration values. Used for loop unrolling optimization.</p>
<p><strong>Example:</strong> <code>UNROLL(unroll_axes=[(0, 4)])</code> expands computation 4 times with different index values.</p>
<h3 id="contract--collapse-unrolled-values-to-vector"><a class="header" href="#contract--collapse-unrolled-values-to-vector">CONTRACT â€” Collapse Unrolled Values to Vector</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>Contract {
    src: Arc&lt;UOp&gt;,                       // unrolled computation
    upcast_ranges: Vec&lt;(usize, usize)&gt;,  // (axis_index, factor) pairs
}
<span class="boring">}</span></code></pre>
<p>The inverse of UNROLLâ€”collects expanded scalar values into a vector. Output vector size = product of factors.</p>
<p><strong>Example:</strong></p>
<pre><code class="language-text">CONTRACT(upcast_ranges=[(0, 4)]) : &lt;4 x Float32&gt;
â””â”€â”€ UNROLL(unroll_axes=[(0, 4)])
    â””â”€â”€ LOAD(...)
</code></pre>
<p>This pattern vectorizes a load: expand 4 iterations, then pack results into a 4-element vector.</p>
<hr>
<h2 id="tensor-cores-wmma"><a class="header" href="#tensor-cores-wmma">Tensor Cores: WMMA</a></h2>
<h3 id="wmma--warp-matrix-multiply-accumulate"><a class="header" href="#wmma--warp-matrix-multiply-accumulate">WMMA â€” Warp Matrix Multiply-Accumulate</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>Wmma {
    a: Arc&lt;UOp&gt;,             // matrix A fragment
    b: Arc&lt;UOp&gt;,             // matrix B fragment
    c: Arc&lt;UOp&gt;,             // accumulator C fragment
    metadata: WmmaMetadata,  // hardware configuration
}
<span class="boring">}</span></code></pre>
<p>Hardware tensor core operation: <code>D = A Ã— B + C</code>. Requires specific matrix shapes and data layouts.</p>
<p><strong>WmmaMetadata Fields:</strong></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Field</th><th>Type</th><th>Purpose</th></tr>
</thead>
<tbody>
<tr><td><code>name</code></td><td><code>String</code></td><td>Instruction name (e.g., <code>"__hmma..."</code>)</td></tr>
<tr><td><code>dims</code></td><td><code>(N, M, K)</code></td><td>Matrix dimensions (e.g., <code>(16, 16, 16)</code>)</td></tr>
<tr><td><code>dtype_in</code></td><td><code>DType</code></td><td>Input matrix precision (e.g., <code>Float16</code>)</td></tr>
<tr><td><code>dtype_out</code></td><td><code>DType</code></td><td>Output precision (e.g., <code>Float32</code>)</td></tr>
<tr><td><code>device</code></td><td><code>String</code></td><td>Target device string</td></tr>
<tr><td><code>threads</code></td><td><code>usize</code></td><td>Threads per warp (typically 32)</td></tr>
<tr><td><code>upcast_axes</code></td><td><code>Vec&lt;(usize, usize)&gt;</code></td><td>Vectorization for output</td></tr>
<tr><td><code>reduce_axes</code></td><td><code>Vec&lt;(usize, usize)&gt;</code></td><td>Contraction axes</td></tr>
</tbody>
</table>
</div>
<p><strong>Example:</strong></p>
<pre><code class="language-text">WMMA(dims=(16, 16, 16), dtype_in=Float16, dtype_out=Float32)
â”œâ”€â”€ A fragment : &lt;8 x Float16&gt;
â”œâ”€â”€ B fragment : &lt;8 x Float16&gt;
â””â”€â”€ C accumulator : &lt;8 x Float32&gt;
</code></pre>
<hr>
<h2 id="control-flow"><a class="header" href="#control-flow">Control Flow</a></h2>
<h3 id="if--endif--conditional-execution"><a class="header" href="#if--endif--conditional-execution">IF / ENDIF â€” Conditional Execution</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>If {
    condition: Arc&lt;UOp&gt;,                // boolean predicate
    body: SmallVec&lt;[Arc&lt;UOp&gt;; 4]&gt;,      // operations to execute
}

EndIf {
    if_op: Arc&lt;UOp&gt;,         // corresponding IF op
}
<span class="boring">}</span></code></pre>
<p>Execute body only when condition is true. Used for boundary checks and sparse operations.</p>
<p><strong>Example:</strong></p>
<pre><code class="language-text">IF
â”œâ”€â”€ LT(idx, bound)           â€” condition (src[0])
â”œâ”€â”€ STORE(...)               â€” body[0]
â””â”€â”€ STORE(...)               â€” body[1]

ENDIF
â””â”€â”€ IF(...)                  â€” references IF op
</code></pre>
<hr>
<h2 id="definition-operations"><a class="header" href="#definition-operations">Definition Operations</a></h2>
<h3 id="define_global--device-memory-argument"><a class="header" href="#define_global--device-memory-argument">DEFINE_GLOBAL â€” Device Memory Argument</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>DefineGlobal(usize)          // argument index
<span class="boring">}</span></code></pre>
<p>Kernel argument for device (global) memory. Index refers to position in kernel argument list.</p>
<h3 id="define_local--shared-memory-allocation"><a class="header" href="#define_local--shared-memory-allocation">DEFINE_LOCAL â€” Shared Memory Allocation</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>DefineLocal(usize)           // local memory index
<span class="boring">}</span></code></pre>
<p>GPU shared memory (LDS) allocation. Visible within a workgroup.</p>
<h3 id="define_var--symbolic-runtime-variable"><a class="header" href="#define_var--symbolic-runtime-variable">DEFINE_VAR â€” Symbolic Runtime Variable</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>DefineVar {
    name: String,            // variable name
    min_val: i64,            // minimum bound
    max_val: i64,            // maximum bound
}
<span class="boring">}</span></code></pre>
<p>Runtime variable with known bounds. Used for dynamic shapes where bounds are known.</p>
<p><strong>Example:</strong></p>
<pre><code class="language-text">DEFINE_VAR(name="batch_size", min=1, max=128) : Index
</code></pre>
<h3 id="define_reg--register-allocation"><a class="header" href="#define_reg--register-allocation">DEFINE_REG â€” Register Allocation</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>DefineReg {
    size: usize,             // register size
}
<span class="boring">}</span></code></pre>
<p>Allocates a register for intermediate storage. Used in code generation.</p>
<h3 id="bind--variable-binding"><a class="header" href="#bind--variable-binding">BIND â€” Variable Binding</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>Bind {
    var: Arc&lt;UOp&gt;,           // DEFINE_VAR
    value: Arc&lt;UOp&gt;,         // concrete value
}
<span class="boring">}</span></code></pre>
<p>Binds a symbolic variable to a concrete value at runtime.</p>
<hr>
<h2 id="special-operations"><a class="header" href="#special-operations">Special Operations</a></h2>
<h3 id="special--hardware-provided-values"><a class="header" href="#special--hardware-provided-values">SPECIAL â€” Hardware-Provided Values</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>Special {
    end: Arc&lt;UOp&gt;,           // upper bound for this dimension
    name: String,            // e.g., "blockIdx.x", "threadIdx.y"
}
<span class="boring">}</span></code></pre>
<p>Accesses hardware-provided values (thread/block indices). Not a loopâ€”the hardware provides the value directly.</p>
<p><strong>Example:</strong></p>
<pre><code class="language-text">SPECIAL(name="blockIdx.x", end=128) : Index
â””â”€â”€ CONST(128)
</code></pre>
<h3 id="unique--identity-marker"><a class="header" href="#unique--identity-marker">UNIQUE â€” Identity Marker</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>Unique(usize)                // unique identifier
<span class="boring">}</span></code></pre>
<p>Creates a unique identity for buffer disambiguation. Two buffers with different UNIQUE values are distinct even if otherwise identical.</p>
<h3 id="device--device-specification"><a class="header" href="#device--device-specification">DEVICE â€” Device Specification</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>Device(DeviceSpec)           // device specification
<span class="boring">}</span></code></pre>
<p>Specifies target device for computation.</p>
<hr>
<h2 id="movement-operations"><a class="header" href="#movement-operations">Movement Operations</a></h2>
<p>High-level tensor shape transformations. These are converted to explicit INDEX operations during rangeify.</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Operation</th><th>Signature</th><th>Purpose</th></tr>
</thead>
<tbody>
<tr><td><code>Reshape</code></td><td><code>{ src, new_shape }</code></td><td>Change shape, same elements</td></tr>
<tr><td><code>Permute</code></td><td><code>{ src, axes: Vec&lt;usize&gt; }</code></td><td>Transpose/reorder axes</td></tr>
<tr><td><code>Expand</code></td><td><code>{ src, new_shape }</code></td><td>Broadcast to larger shape</td></tr>
<tr><td><code>Pad</code></td><td><code>{ src, begin_pads, end_pads }</code></td><td>Add padding</td></tr>
<tr><td><code>Shrink</code></td><td><code>{ src, begins, ends }</code></td><td>Extract sub-region</td></tr>
<tr><td><code>Flip</code></td><td><code>{ src, axes: Vec&lt;bool&gt; }</code></td><td>Reverse along axes</td></tr>
</tbody>
</table>
</div>
<p><strong>Example:</strong> RESHAPE</p>
<pre><code class="language-text">RESHAPE(new_shape=[6, 4]) : Shape[6, 4]
â”œâ”€â”€ BUFFER[2, 3, 4] : Float32
â””â”€â”€ CONST([6, 4]) : Shape
</code></pre>
<hr>
<h2 id="quick-reference-1"><a class="header" href="#quick-reference-1">Quick Reference</a></h2>
<h3 id="by-category"><a class="header" href="#by-category">By Category</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Category</th><th>Operations</th></tr>
</thead>
<tbody>
<tr><td><strong>Loop Control</strong></td><td><code>RANGE</code>, <code>END</code></td></tr>
<tr><td><strong>Reduction</strong></td><td><code>REDUCE_AXIS</code>, <code>REDUCE</code>, <code>ALLREDUCE</code></td></tr>
<tr><td><strong>Memory</strong></td><td><code>BUFFER</code>, <code>BUFFERIZE</code>, <code>INDEX</code>, <code>POINTER_INDEX</code>, <code>LOAD</code>, <code>STORE</code></td></tr>
<tr><td><strong>Kernel</strong></td><td><code>KERNEL</code>, <code>SINK</code>, <code>AFTER</code>, <code>BARRIER</code></td></tr>
<tr><td><strong>Vector</strong></td><td><code>VECTORIZE</code>, <code>GEP</code>, <code>VCONST</code>, <code>CAT</code>, <code>PTRCAT</code></td></tr>
<tr><td><strong>Expansion</strong></td><td><code>UNROLL</code>, <code>CONTRACT</code></td></tr>
<tr><td><strong>Hardware</strong></td><td><code>WMMA</code>, <code>SPECIAL</code></td></tr>
<tr><td><strong>Control</strong></td><td><code>IF</code>, <code>ENDIF</code></td></tr>
<tr><td><strong>Definition</strong></td><td><code>DEFINE_GLOBAL</code>, <code>DEFINE_LOCAL</code>, <code>DEFINE_VAR</code>, <code>DEFINE_REG</code>, <code>BIND</code>, <code>UNIQUE</code>, <code>DEVICE</code></td></tr>
<tr><td><strong>Movement</strong></td><td><code>RESHAPE</code>, <code>PERMUTE</code>, <code>EXPAND</code>, <code>PAD</code>, <code>SHRINK</code>, <code>FLIP</code></td></tr>
<tr><td><strong>ALU</strong></td><td><code>Unary(...)</code>, <code>Binary(...)</code>, <code>Ternary(...)</code>, <code>Cast</code>, <code>BitCast</code></td></tr>
</tbody>
</table>
</div>
<h3 id="range-ending-operations"><a class="header" href="#range-ending-operations">Range-Ending Operations</a></h3>
<p>Operations that close RANGE scopes (remove ranges from active set):</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Operation</th><th>Range Start Index</th></tr>
</thead>
<tbody>
<tr><td><code>BUFFERIZE</code></td><td>1 (compute=0, ranges=1+)</td></tr>
<tr><td><code>REDUCE</code></td><td>1 (src=0, ranges=1+)</td></tr>
<tr><td><code>STORE</code></td><td>3 (buffer=0, index=1, value=2, ranges=3+)</td></tr>
<tr><td><code>WMMA</code></td><td>3 (a=0, b=1, c=2)</td></tr>
<tr><td><code>END</code></td><td>1 (computation=0, ranges=1+)</td></tr>
</tbody>
</table>
</div>
<h3 id="expandable-operations"><a class="header" href="#expandable-operations">Expandable Operations</a></h3>
<p>Operations that propagate UNROLL through the computation graph:</p>
<ul>
<li>ALU: <code>Unary</code>, <code>Binary</code>, <code>Ternary</code></li>
<li>Type: <code>Cast</code>, <code>BitCast</code></li>
<li>Vector: <code>Gep</code>, <code>Vectorize</code></li>
<li>Memory: <code>Load</code>, <code>Store</code>, <code>Index</code>, <code>PointerIndex</code></li>
<li>Control: <code>Reduce</code>, <code>End</code>, <code>After</code></li>
<li>Buffer: <code>Bufferize</code></li>
<li>Hardware: <code>Wmma</code></li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>

        <template id=fa-eye><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M288 32c-80.8 0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7 0 24.6C17.3 304 48.6 356 95.4 399.4C142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144s64.5-144 144-144s144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64c-11.5 0-22.3-3-31.6-8.4c-.2 2.8-.4 5.5-.4 8.4c0 53 43 96 96 96s96-43 96-96s-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6z"/></svg></span></template>
        <template id=fa-eye-slash><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M38.8 5.1C28.4-3.1 13.3-1.2 5.1 9.2S-1.2 34.7 9.2 42.9l592 464c10.4 8.2 25.5 6.3 33.7-4.1s6.3-25.5-4.1-33.7L525.6 386.7c39.6-40.6 66.4-86.1 79.9-118.4c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C465.5 68.8 400.8 32 320 32c-68.2 0-125 26.3-169.3 60.8L38.8 5.1zM223.1 149.5C248.6 126.2 282.7 112 320 112c79.5 0 144 64.5 144 144c0 24.9-6.3 48.3-17.4 68.7L408 294.5c5.2-11.8 8-24.8 8-38.5c0-53-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6c0 10.2-2.4 19.8-6.6 28.3l-90.3-70.8zm223.1 298L373 389.9c-16.4 6.5-34.3 10.1-53 10.1c-79.5 0-144-64.5-144-144c0-6.9 .5-13.6 1.4-20.2L83.1 161.5C60.3 191.2 44 220.8 34.5 243.7c-3.3 7.9-3.3 16.7 0 24.6c14.9 35.7 46.2 87.7 93 131.1C174.5 443.2 239.2 480 320 480c47.8 0 89.9-12.9 126.2-32.5z"/></svg></span></template>
        <template id=fa-copy><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M502.6 70.63l-61.25-61.25C435.4 3.371 427.2 0 418.7 0H255.1c-35.35 0-64 28.66-64 64l.0195 256C192 355.4 220.7 384 256 384h192c35.2 0 64-28.8 64-64V93.25C512 84.77 508.6 76.63 502.6 70.63zM464 320c0 8.836-7.164 16-16 16H255.1c-8.838 0-16-7.164-16-16L239.1 64.13c0-8.836 7.164-16 16-16h128L384 96c0 17.67 14.33 32 32 32h47.1V320zM272 448c0 8.836-7.164 16-16 16H63.1c-8.838 0-16-7.164-16-16L47.98 192.1c0-8.836 7.164-16 16-16H160V128H63.99c-35.35 0-64 28.65-64 64l.0098 256C.002 483.3 28.66 512 64 512h192c35.2 0 64-28.8 64-64v-32h-47.1L272 448z"/></svg></span></template>
        <template id=fa-play><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M73 39c-14.8-9.1-33.4-9.4-48.5-.9S0 62.6 0 80V432c0 17.4 9.4 33.4 24.5 41.9s33.7 8.1 48.5-.9L361 297c14.3-8.7 23-24.2 23-41s-8.7-32.2-23-41L73 39z"/></svg></span></template>
        <template id=fa-clock-rotate-left><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M75 75L41 41C25.9 25.9 0 36.6 0 57.9V168c0 13.3 10.7 24 24 24H134.1c21.4 0 32.1-25.9 17-41l-30.8-30.8C155 85.5 203 64 256 64c106 0 192 86 192 192s-86 192-192 192c-40.8 0-78.6-12.7-109.7-34.4c-14.5-10.1-34.4-6.6-44.6 7.9s-6.6 34.4 7.9 44.6C151.2 495 201.7 512 256 512c141.4 0 256-114.6 256-256S397.4 0 256 0C185.3 0 121.3 28.7 75 75zm181 53c-13.3 0-24 10.7-24 24V256c0 6.4 2.5 12.5 7 17l72 72c9.4 9.4 24.6 9.4 33.9 0s9.4-24.6 0-33.9l-65-65V152c0-13.3-10.7-24-24-24z"/></svg></span></template>



        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr-ef4e11c1.min.js"></script>
        <script src="mark-09e88c2c.min.js"></script>
        <script src="searcher-c2a407aa.js"></script>

        <script src="clipboard-1626706a.min.js"></script>
        <script src="highlight-abc7f01d.js"></script>
        <script src="book-a0b12cfe.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>


    </div>
    </body>
</html>
